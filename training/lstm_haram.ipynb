{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM - Mindspore**"
      ],
      "metadata": {
        "id": "2lLjLLsrCOVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing Libraries**"
      ],
      "metadata": {
        "id": "Qw9auoiOCYue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mindspore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTF82yxz_TwP",
        "outputId": "a427dc56-86e1-45a9-f897-9ed6fc00f2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mindspore\n",
            "  Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy<2.0.0,>=1.20.0 (from mindspore)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (5.29.4)\n",
            "Collecting asttokens>=2.0.4 (from mindspore)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (24.2)\n",
            "Requirement already satisfied: psutil>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from mindspore) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.6.3)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (0.5.3)\n",
            "Collecting dill>=0.3.7 (from mindspore)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (1.17.0)\n",
            "Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl (962.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.0/962.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, dill, asttokens, mindspore\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-3.0.0 dill-0.4.0 mindspore-2.5.0 numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMXPZRvGAFUC",
        "outputId": "f76887c1-53af-49c2-d2a6-1b30cb755a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMe6kzoqAGz9",
        "outputId": "0586276e-8570-448e-9bd6-86fcb5d3d66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le-_qWHuPo2H",
        "outputId": "3cefd41f-15a4-4ffc-e601-de88787b867d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "5P2CNElwAMY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import mindspore as ms\n",
        "from mindspore import context, Tensor, dtype as mstype\n",
        "from mindspore.nn import (\n",
        "    Embedding, LSTM, Dropout, Dense, ReLU,\n",
        "    SoftmaxCrossEntropyWithLogits, Adam\n",
        ")\n",
        "from mindspore.train import Model\n",
        "from mindspore.train.callback import (\n",
        "    CheckpointConfig, ModelCheckpoint, EarlyStopping, LossMonitor\n",
        ")\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "R5NB_0OK_Nl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"irfanakbarihabibi/food-ingredients-dataset-with-halal-label\")"
      ],
      "metadata": {
        "id": "MZdbzaBO-r58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71d83e9-4d71-4671-b331-446156bf1fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/irfanakbarihabibi/food-ingredients-dataset-with-halal-label?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.98M/1.98M [00:00<00:00, 87.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MindSpore version:\", ms.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBprL7D_wM6",
        "outputId": "86e5fae7-4250-4291-ee9a-4c4e59fe649a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MindSpore version: 2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mindspore import context\n",
        "device = context.get_context(\"device_target\")\n",
        "print(\"Current device target:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFDbzeG8Aqbq",
        "outputId": "6006dd61-15f1-46e0-8409-282cb9941f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device target: CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Analysis**"
      ],
      "metadata": {
        "id": "3ltnPritCgQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = os.path.join(path, 'cleaned_dataset.csv')\n",
        "df = pd.read_csv(csv_path, encoding='latin-1')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T6yhMJMiAvzi",
        "outputId": "1f66a4e2-dad3-4051-8ca2-a8933c0b336f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0                                      vegetable oil  halal\n",
              "1  beef stock contains less than of mirepoix carr...  halal\n",
              "2  clam stock potatoes clams cream vegetable oil ...  haram\n",
              "3  water cream broccoli celery vegetable oil corn...  haram\n",
              "4  chicken stock contains less than of yeast extr...  halal"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5566241f-0270-4a7a-9f3d-7a99928ca3b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vegetable oil</td>\n",
              "      <td>halal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>beef stock contains less than of mirepoix carr...</td>\n",
              "      <td>halal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>clam stock potatoes clams cream vegetable oil ...</td>\n",
              "      <td>haram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>water cream broccoli celery vegetable oil corn...</td>\n",
              "      <td>haram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>chicken stock contains less than of yeast extr...</td>\n",
              "      <td>halal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5566241f-0270-4a7a-9f3d-7a99928ca3b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5566241f-0270-4a7a-9f3d-7a99928ca3b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5566241f-0270-4a7a-9f3d-7a99928ca3b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21a49aab-d0a4-491e-97da-36cf2edc91dc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21a49aab-d0a4-491e-97da-36cf2edc91dc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21a49aab-d0a4-491e-97da-36cf2edc91dc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 39787,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39573,\n        \"samples\": [\n          \"pepperoni stick beef pork water salt contains less than of corn syrup spices natural flavor hydrolyzed soy protein spice extractives including paprika sodium erythorbate lactic acid starter culture sodium nitrite pasteurized process american cheese food stick pasteurized process american cheese food cultured pasteurized milk water sodium phosphate natural flavoring salt sorbic acid preservative enzymes\",\n          \"cooked scallop trim meat sorbitol fish sauce sardine hot pepper paste rice wheat soybean red pepper sardine red pepper sesame seed sesame oil salt vinegar paprika garlic paste krill extract scallop extract scallop yeast extract ginger powder kelp extract momosodium glutamate glicine disodium succinate disodium ribonucleotide sodium citrate xanthan gum alcohol modified starch potato\",\n          \"fresh cucumbers fructose water salt vinegar mustard seed onion celery seed natural flavors calcium chloride sodium benzoate yellow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"haram\",\n          \"halal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1OgC--npBHr0",
        "outputId": "653faa50-f4ba-4b5c-df66-cdbcfef6217e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'water cream broccoli celery vegetable oil corn canola andor soybean modified food starch cheddar cheese cheddar cheese cultured milk salt enzymes whey salt sodium phosphate contains less than of butter parmesan and cheddar cheese milk cultures salt enzymes wheat flour salt potatoes onions soy protein concentrate roasted garlic annatto extract for color soy lecithin driedcontains wheat milk soy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = df['text'].str.contains(r'\\blard\\b', case=False, na=False)\n",
        "lard_rows = df[mask]\n",
        "print(lard_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4K5BEMUBkha",
        "outputId": "93a20c3a-46bf-418a-fec4-14a139eee91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label\n",
            "241   water pinto beans soybean oil onion iodized sa...  haram\n",
            "721   water black beans soybean oil onion salt pork ...  haram\n",
            "722   water pinto beans soybean oil onion salt pork ...  haram\n",
            "723   prepared beans water contains or less lard sal...  haram\n",
            "729   lard greens water sugar salt minced onion hydr...  haram\n",
            "...                                                 ...    ...\n",
            "5668  water runner bean pork meat smoked pork bacon ...  haram\n",
            "5698  filling gravy and sausage gravy water cream mo...  haram\n",
            "5725  enriched durum flour wheat flour niacin iron t...  haram\n",
            "8703  baked chocolate pie dough enriched wheat flour...  haram\n",
            "9529  cooked jasmine rice water white rice seasoned ...  haram\n",
            "\n",
            "[71 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contains_numbers = df['text'].astype(str).str.contains(r'\\d', na=False)\n",
        "rows_with_numbers = df[contains_numbers]\n",
        "print(rows_with_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW8EF1Qq0HzP",
        "outputId": "982c7b96-dd64-4fd1-fadb-05b6257d61ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [text, label]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "D9UNGXdWBbrF",
        "outputId": "b386b5d1-08b0-4f5f-cfb2-e2c2bea8a3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = 'text'\n",
        "label_column = 'label'\n",
        "\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "_NpAJqnzCp0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[label_column].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpWb9NGuC2m6",
        "outputId": "80db43e3-16ba-470c-acaf-89f5b3ada3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "halal    21826\n",
            "haram    17961\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[label_column].apply(lambda x: 0 if x.lower() == 'halal' else 1)\n",
        "class_counts = y.value_counts()"
      ],
      "metadata": {
        "id": "quS6-9DJC6__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "class_counts.plot(kind='bar', color=['green', 'red'])\n",
        "plt.title('Data Variance')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Data')\n",
        "plt.xticks([0, 1], ['Halal', 'Haram'], rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('class_distribution.png', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "S3m-HwkXDMEE",
        "outputId": "d0f4930a-f842-4fa6-af82-139c5bcfc6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenizer**"
      ],
      "metadata": {
        "id": "7jYRE4xrQDNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[text_column])"
      ],
      "metadata": {
        "id": "0S8Wb2jhDdWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df[text_column])\n",
        "X = pad_sequences(X).astype(np.int32)\n",
        "with open('tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "y = LabelEncoder().fit_transform(y).astype(np.int32)"
      ],
      "metadata": {
        "id": "eskyA8JDQKIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split**"
      ],
      "metadata": {
        "id": "RSEC-V_PQeRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "RRlHHC9pQTdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Make Dataset**"
      ],
      "metadata": {
        "id": "QnDOw7paQsLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(X_arr, y_arr, batch_size=32, shuffle=True):\n",
        "    ds = ms.dataset.NumpySlicesDataset(\n",
        "        {\"data\": X_arr, \"label\": y_arr}, shuffle=shuffle\n",
        "    )\n",
        "    return ds.batch(batch_size)\n",
        "\n",
        "train_dataset = make_dataset(X_train, y_train)\n",
        "val_dataset   = make_dataset(X_val,   y_val,   shuffle=False)\n",
        "test_dataset  = make_dataset(X_test,  y_test,  shuffle=False)"
      ],
      "metadata": {
        "id": "hitmLoFpQgpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(ms.nn.Cell):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden1, hidden2, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm1     = LSTM(embed_dim, hidden1, batch_first=True)\n",
        "        self.drop1     = Dropout(0.2)\n",
        "        self.lstm2     = LSTM(hidden1, hidden2, batch_first=True)\n",
        "        self.drop2     = Dropout(0.2)\n",
        "        self.fc1       = Dense(hidden2, 32)\n",
        "        self.relu      = ReLU()\n",
        "        self.fc2       = Dense(32, num_classes)\n",
        "\n",
        "    def construct(self, x):\n",
        "        x = self.embedding(x)             # (B, L, E)\n",
        "        x, _ = self.lstm1(x)              # (B, L, H1)\n",
        "        x = self.drop1(x)\n",
        "        x, _ = self.lstm2(x)              # (B, L, H2)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        batch_size=x.shape[0]\n",
        "        seq_len=x.shape[1]\n",
        "        last_time_step = x[:, seq_len-1, :]\n",
        "\n",
        "        x = self.fc1(last_time_step)      # (B, 32)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)                   # (B, num_classes)\n",
        "        return x\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model_net  = LSTMClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=50,\n",
        "    hidden1=100,\n",
        "    hidden2=50,\n",
        "    num_classes=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaO4vFw3QpjG",
        "outputId": "9cf3b0a5-f8b2-4d66-80e4-38f5b1420742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:06:48.661.000 [mindspore/nn/layer/basic.py:176] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:06:48.694.000 [mindspore/nn/layer/basic.py:176] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "optimizer = Adam(model_net.trainable_params(), learning_rate=1e-4)\n",
        "\n",
        "model = Model(model_net, loss_fn=loss_fn, optimizer=optimizer, metrics={\"accuracy\"})\n",
        "\n",
        "# 7) CALLBACKS: checkpoint + early stopping + loss monitor\n",
        "ckpt_config = CheckpointConfig(save_checkpoint_steps=100, keep_checkpoint_max=1)\n",
        "ckpt_cb     = ModelCheckpoint(prefix=\"best_model\", directory=\"./\", config=ckpt_config)\n",
        "early_cb    = EarlyStopping(monitor=\"loss\", patience=5, mode=\"max\")\n",
        "loss_cb     = LossMonitor()"
      ],
      "metadata": {
        "id": "iOr2UUyKQ2e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(\n",
        "    epoch=10,\n",
        "    train_dataset=train_dataset,\n",
        "    callbacks=[ckpt_cb, early_cb, loss_cb],\n",
        "    dataset_sink_mode=False\n",
        ")\n",
        "\n",
        "res = model.eval(test_dataset, dataset_sink_mode=False)\n",
        "print(f\"Test set accuracy: {res['accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95EeJmTbRPRW",
        "outputId": "a4dfd232-dac2-4be4-dc8d-e8a4e2dff9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1 step: 1, loss is 0.7065979242324829\n",
            "epoch: 1 step: 2, loss is 0.7237032055854797\n",
            "epoch: 1 step: 3, loss is 0.7048289775848389\n",
            "epoch: 1 step: 4, loss is 0.7035069465637207\n",
            "epoch: 1 step: 5, loss is 0.7069597840309143\n",
            "epoch: 1 step: 6, loss is 0.7051082253456116\n",
            "epoch: 1 step: 7, loss is 0.6951498985290527\n",
            "epoch: 1 step: 8, loss is 0.6971979737281799\n",
            "epoch: 1 step: 9, loss is 0.6888637542724609\n",
            "epoch: 1 step: 10, loss is 0.7102413177490234\n",
            "epoch: 1 step: 11, loss is 0.7140167355537415\n",
            "epoch: 1 step: 12, loss is 0.7358464598655701\n",
            "epoch: 1 step: 13, loss is 0.6869316101074219\n",
            "epoch: 1 step: 14, loss is 0.7150680422782898\n",
            "epoch: 1 step: 15, loss is 0.6937225461006165\n",
            "epoch: 1 step: 16, loss is 0.7101569771766663\n",
            "epoch: 1 step: 17, loss is 0.7023451924324036\n",
            "epoch: 1 step: 18, loss is 0.6817297339439392\n",
            "epoch: 1 step: 19, loss is 0.704456090927124\n",
            "epoch: 1 step: 20, loss is 0.6949878931045532\n",
            "epoch: 1 step: 21, loss is 0.715787947177887\n",
            "epoch: 1 step: 22, loss is 0.6903873085975647\n",
            "epoch: 1 step: 23, loss is 0.7054176330566406\n",
            "epoch: 1 step: 24, loss is 0.7226768136024475\n",
            "epoch: 1 step: 25, loss is 0.6949813365936279\n",
            "epoch: 1 step: 26, loss is 0.7038348913192749\n",
            "epoch: 1 step: 27, loss is 0.6947916150093079\n",
            "epoch: 1 step: 28, loss is 0.7172974348068237\n",
            "epoch: 1 step: 29, loss is 0.7072080969810486\n",
            "epoch: 1 step: 30, loss is 0.6856622695922852\n",
            "epoch: 1 step: 31, loss is 0.7053171992301941\n",
            "epoch: 1 step: 32, loss is 0.7128406167030334\n",
            "epoch: 1 step: 33, loss is 0.7047377228736877\n",
            "epoch: 1 step: 34, loss is 0.7027842998504639\n",
            "epoch: 1 step: 35, loss is 0.6880283951759338\n",
            "epoch: 1 step: 36, loss is 0.6882022619247437\n",
            "epoch: 1 step: 37, loss is 0.6849424839019775\n",
            "epoch: 1 step: 38, loss is 0.7085633873939514\n",
            "epoch: 1 step: 39, loss is 0.71185702085495\n",
            "epoch: 1 step: 40, loss is 0.7270438075065613\n",
            "epoch: 1 step: 41, loss is 0.6939972043037415\n",
            "epoch: 1 step: 42, loss is 0.7017734050750732\n",
            "epoch: 1 step: 43, loss is 0.7018464803695679\n",
            "epoch: 1 step: 44, loss is 0.7152859568595886\n",
            "epoch: 1 step: 45, loss is 0.6865748763084412\n",
            "epoch: 1 step: 46, loss is 0.6799288988113403\n",
            "epoch: 1 step: 47, loss is 0.6952402591705322\n",
            "epoch: 1 step: 48, loss is 0.7044453620910645\n",
            "epoch: 1 step: 49, loss is 0.7021780610084534\n",
            "epoch: 1 step: 50, loss is 0.698280394077301\n",
            "epoch: 1 step: 51, loss is 0.6853885650634766\n",
            "epoch: 1 step: 52, loss is 0.7034428119659424\n",
            "epoch: 1 step: 53, loss is 0.7076839208602905\n",
            "epoch: 1 step: 54, loss is 0.706964910030365\n",
            "epoch: 1 step: 55, loss is 0.691229522228241\n",
            "epoch: 1 step: 56, loss is 0.7165499925613403\n",
            "epoch: 1 step: 57, loss is 0.6971383690834045\n",
            "epoch: 1 step: 58, loss is 0.7026808261871338\n",
            "epoch: 1 step: 59, loss is 0.7065151929855347\n",
            "epoch: 1 step: 60, loss is 0.7075448632240295\n",
            "epoch: 1 step: 61, loss is 0.7046530842781067\n",
            "epoch: 1 step: 62, loss is 0.6923876404762268\n",
            "epoch: 1 step: 63, loss is 0.6978351473808289\n",
            "epoch: 1 step: 64, loss is 0.7111054062843323\n",
            "epoch: 1 step: 65, loss is 0.7054884433746338\n",
            "epoch: 1 step: 66, loss is 0.706831157207489\n",
            "epoch: 1 step: 67, loss is 0.6992374062538147\n",
            "epoch: 1 step: 68, loss is 0.698803186416626\n",
            "epoch: 1 step: 69, loss is 0.6974929571151733\n",
            "epoch: 1 step: 70, loss is 0.6882766485214233\n",
            "epoch: 1 step: 71, loss is 0.6941454410552979\n",
            "epoch: 1 step: 72, loss is 0.6817710995674133\n",
            "epoch: 1 step: 73, loss is 0.7055916786193848\n",
            "epoch: 1 step: 74, loss is 0.716719925403595\n",
            "epoch: 1 step: 75, loss is 0.6855282187461853\n",
            "epoch: 1 step: 76, loss is 0.69674152135849\n",
            "epoch: 1 step: 77, loss is 0.7183997631072998\n",
            "epoch: 1 step: 78, loss is 0.7010810375213623\n",
            "epoch: 1 step: 79, loss is 0.7077470421791077\n",
            "epoch: 1 step: 80, loss is 0.6887624263763428\n",
            "epoch: 1 step: 81, loss is 0.6853026747703552\n",
            "epoch: 1 step: 82, loss is 0.6935209035873413\n",
            "epoch: 1 step: 83, loss is 0.7157655358314514\n",
            "epoch: 1 step: 84, loss is 0.6792341470718384\n",
            "epoch: 1 step: 85, loss is 0.7095372080802917\n",
            "epoch: 1 step: 86, loss is 0.7064544558525085\n",
            "epoch: 1 step: 87, loss is 0.7041304707527161\n",
            "epoch: 1 step: 88, loss is 0.6917300820350647\n",
            "epoch: 1 step: 89, loss is 0.7039249539375305\n",
            "epoch: 1 step: 90, loss is 0.6991109251976013\n",
            "epoch: 1 step: 91, loss is 0.6901479363441467\n",
            "epoch: 1 step: 92, loss is 0.7029759287834167\n",
            "epoch: 1 step: 93, loss is 0.6894149780273438\n",
            "epoch: 1 step: 94, loss is 0.6931484937667847\n",
            "epoch: 1 step: 95, loss is 0.6943308711051941\n",
            "epoch: 1 step: 96, loss is 0.7028331160545349\n",
            "epoch: 1 step: 97, loss is 0.6976988315582275\n",
            "epoch: 1 step: 98, loss is 0.699667751789093\n",
            "epoch: 1 step: 99, loss is 0.6919576525688171\n",
            "epoch: 1 step: 100, loss is 0.6924001574516296\n",
            "epoch: 1 step: 101, loss is 0.6897251605987549\n",
            "epoch: 1 step: 102, loss is 0.7086147665977478\n",
            "epoch: 1 step: 103, loss is 0.6951354742050171\n",
            "epoch: 1 step: 104, loss is 0.6910927295684814\n",
            "epoch: 1 step: 105, loss is 0.6926224827766418\n",
            "epoch: 1 step: 106, loss is 0.6948639750480652\n",
            "epoch: 1 step: 107, loss is 0.6975880861282349\n",
            "epoch: 1 step: 108, loss is 0.6901355981826782\n",
            "epoch: 1 step: 109, loss is 0.685562014579773\n",
            "epoch: 1 step: 110, loss is 0.6987394690513611\n",
            "epoch: 1 step: 111, loss is 0.693466067314148\n",
            "epoch: 1 step: 112, loss is 0.703914999961853\n",
            "epoch: 1 step: 113, loss is 0.700533926486969\n",
            "epoch: 1 step: 114, loss is 0.6903296113014221\n",
            "epoch: 1 step: 115, loss is 0.7040225267410278\n",
            "epoch: 1 step: 116, loss is 0.6890841722488403\n",
            "epoch: 1 step: 117, loss is 0.6928360462188721\n",
            "epoch: 1 step: 118, loss is 0.6933487057685852\n",
            "epoch: 1 step: 119, loss is 0.6866042017936707\n",
            "epoch: 1 step: 120, loss is 0.6880441308021545\n",
            "epoch: 1 step: 121, loss is 0.7069657444953918\n",
            "epoch: 1 step: 122, loss is 0.6965672373771667\n",
            "epoch: 1 step: 123, loss is 0.6880927085876465\n",
            "epoch: 1 step: 124, loss is 0.6900198459625244\n",
            "epoch: 1 step: 125, loss is 0.6887746453285217\n",
            "epoch: 1 step: 126, loss is 0.6984105110168457\n",
            "epoch: 1 step: 127, loss is 0.6988440155982971\n",
            "epoch: 1 step: 128, loss is 0.6999165415763855\n",
            "epoch: 1 step: 129, loss is 0.7016907930374146\n",
            "epoch: 1 step: 130, loss is 0.6895570755004883\n",
            "epoch: 1 step: 131, loss is 0.7039445638656616\n",
            "epoch: 1 step: 132, loss is 0.6937410831451416\n",
            "epoch: 1 step: 133, loss is 0.696840763092041\n",
            "epoch: 1 step: 134, loss is 0.6981317400932312\n",
            "epoch: 1 step: 135, loss is 0.686008632183075\n",
            "epoch: 1 step: 136, loss is 0.7019826769828796\n",
            "epoch: 1 step: 137, loss is 0.6900944113731384\n",
            "epoch: 1 step: 138, loss is 0.6902071833610535\n",
            "epoch: 1 step: 139, loss is 0.7049218416213989\n",
            "epoch: 1 step: 140, loss is 0.7014430165290833\n",
            "epoch: 1 step: 141, loss is 0.6894552707672119\n",
            "epoch: 1 step: 142, loss is 0.691638708114624\n",
            "epoch: 1 step: 143, loss is 0.6945614814758301\n",
            "epoch: 1 step: 144, loss is 0.6964080333709717\n",
            "epoch: 1 step: 145, loss is 0.6904349327087402\n",
            "epoch: 1 step: 146, loss is 0.6992873549461365\n",
            "epoch: 1 step: 147, loss is 0.6956863403320312\n",
            "epoch: 1 step: 148, loss is 0.6944047212600708\n",
            "epoch: 1 step: 149, loss is 0.6980724930763245\n",
            "epoch: 1 step: 150, loss is 0.6936995983123779\n",
            "epoch: 1 step: 151, loss is 0.700842559337616\n",
            "epoch: 1 step: 152, loss is 0.6965991258621216\n",
            "epoch: 1 step: 153, loss is 0.6928093433380127\n",
            "epoch: 1 step: 154, loss is 0.7036843299865723\n",
            "epoch: 1 step: 155, loss is 0.69074946641922\n",
            "epoch: 1 step: 156, loss is 0.6982903480529785\n",
            "epoch: 1 step: 157, loss is 0.6949613094329834\n",
            "epoch: 1 step: 158, loss is 0.6950411796569824\n",
            "epoch: 1 step: 159, loss is 0.6942325234413147\n",
            "epoch: 1 step: 160, loss is 0.7024160027503967\n",
            "epoch: 1 step: 161, loss is 0.6956087350845337\n",
            "epoch: 1 step: 162, loss is 0.6867196559906006\n",
            "epoch: 1 step: 163, loss is 0.6950234174728394\n",
            "epoch: 1 step: 164, loss is 0.6932864785194397\n",
            "epoch: 1 step: 165, loss is 0.6926336884498596\n",
            "epoch: 1 step: 166, loss is 0.6903300881385803\n",
            "epoch: 1 step: 167, loss is 0.7022291421890259\n",
            "epoch: 1 step: 168, loss is 0.6864921450614929\n",
            "epoch: 1 step: 169, loss is 0.7008697986602783\n",
            "epoch: 1 step: 170, loss is 0.6831248998641968\n",
            "epoch: 1 step: 171, loss is 0.6929771900177002\n",
            "epoch: 1 step: 172, loss is 0.6879545450210571\n",
            "epoch: 1 step: 173, loss is 0.6925060153007507\n",
            "epoch: 1 step: 174, loss is 0.6962918043136597\n",
            "epoch: 1 step: 175, loss is 0.7079952359199524\n",
            "epoch: 1 step: 176, loss is 0.7094786763191223\n",
            "epoch: 1 step: 177, loss is 0.683459997177124\n",
            "epoch: 1 step: 178, loss is 0.7013601064682007\n",
            "epoch: 1 step: 179, loss is 0.6932086944580078\n",
            "epoch: 1 step: 180, loss is 0.6949560046195984\n",
            "epoch: 1 step: 181, loss is 0.7000206112861633\n",
            "epoch: 1 step: 182, loss is 0.6956149339675903\n",
            "epoch: 1 step: 183, loss is 0.6958950757980347\n",
            "epoch: 1 step: 184, loss is 0.6910685896873474\n",
            "epoch: 1 step: 185, loss is 0.6793504357337952\n",
            "epoch: 1 step: 186, loss is 0.6946254968643188\n",
            "epoch: 1 step: 187, loss is 0.7035732865333557\n",
            "epoch: 1 step: 188, loss is 0.7005570530891418\n",
            "epoch: 1 step: 189, loss is 0.6894770860671997\n",
            "epoch: 1 step: 190, loss is 0.6891342401504517\n",
            "epoch: 1 step: 191, loss is 0.6918807029724121\n",
            "epoch: 1 step: 192, loss is 0.6980902552604675\n",
            "epoch: 1 step: 193, loss is 0.6894233822822571\n",
            "epoch: 1 step: 194, loss is 0.6988391280174255\n",
            "epoch: 1 step: 195, loss is 0.7049533724784851\n",
            "epoch: 1 step: 196, loss is 0.6886671185493469\n",
            "epoch: 1 step: 197, loss is 0.6973271369934082\n",
            "epoch: 1 step: 198, loss is 0.6902432441711426\n",
            "epoch: 1 step: 199, loss is 0.6945428252220154\n",
            "epoch: 1 step: 200, loss is 0.6857149600982666\n",
            "epoch: 1 step: 201, loss is 0.6897621154785156\n",
            "epoch: 1 step: 202, loss is 0.6929669976234436\n",
            "epoch: 1 step: 203, loss is 0.6898146271705627\n",
            "epoch: 1 step: 204, loss is 0.6905364990234375\n",
            "epoch: 1 step: 205, loss is 0.6788695454597473\n",
            "epoch: 1 step: 206, loss is 0.7006324529647827\n",
            "epoch: 1 step: 207, loss is 0.6904219388961792\n",
            "epoch: 1 step: 208, loss is 0.686365008354187\n",
            "epoch: 1 step: 209, loss is 0.6982825994491577\n",
            "epoch: 1 step: 210, loss is 0.6866698265075684\n",
            "epoch: 1 step: 211, loss is 0.6778402924537659\n",
            "epoch: 1 step: 212, loss is 0.7022767066955566\n",
            "epoch: 1 step: 213, loss is 0.6963901519775391\n",
            "epoch: 1 step: 214, loss is 0.6932980418205261\n",
            "epoch: 1 step: 215, loss is 0.7004206776618958\n",
            "epoch: 1 step: 216, loss is 0.6905875205993652\n",
            "epoch: 1 step: 217, loss is 0.6892564296722412\n",
            "epoch: 1 step: 218, loss is 0.6896443963050842\n",
            "epoch: 1 step: 219, loss is 0.6852138638496399\n",
            "epoch: 1 step: 220, loss is 0.6889238357543945\n",
            "epoch: 1 step: 221, loss is 0.6898089647293091\n",
            "epoch: 1 step: 222, loss is 0.688495934009552\n",
            "epoch: 1 step: 223, loss is 0.6840725541114807\n",
            "epoch: 1 step: 224, loss is 0.6905592083930969\n",
            "epoch: 1 step: 225, loss is 0.683579683303833\n",
            "epoch: 1 step: 226, loss is 0.6893635988235474\n",
            "epoch: 1 step: 227, loss is 0.6923111081123352\n",
            "epoch: 1 step: 228, loss is 0.682215690612793\n",
            "epoch: 1 step: 229, loss is 0.6863845586776733\n",
            "epoch: 1 step: 230, loss is 0.7094827890396118\n",
            "epoch: 1 step: 231, loss is 0.7125450372695923\n",
            "epoch: 1 step: 232, loss is 0.6988060474395752\n",
            "epoch: 1 step: 233, loss is 0.6767780184745789\n",
            "epoch: 1 step: 234, loss is 0.6845096349716187\n",
            "epoch: 1 step: 235, loss is 0.7093637585639954\n",
            "epoch: 1 step: 236, loss is 0.6925737857818604\n",
            "epoch: 1 step: 237, loss is 0.6879714727401733\n",
            "epoch: 1 step: 238, loss is 0.6831971406936646\n",
            "epoch: 1 step: 239, loss is 0.6942439079284668\n",
            "epoch: 1 step: 240, loss is 0.6809312701225281\n",
            "epoch: 1 step: 241, loss is 0.6925543546676636\n",
            "epoch: 1 step: 242, loss is 0.6907261610031128\n",
            "epoch: 1 step: 243, loss is 0.6870590448379517\n",
            "epoch: 1 step: 244, loss is 0.6961172223091125\n",
            "epoch: 1 step: 245, loss is 0.6929119825363159\n",
            "epoch: 1 step: 246, loss is 0.6854236125946045\n",
            "epoch: 1 step: 247, loss is 0.6778867840766907\n",
            "epoch: 1 step: 248, loss is 0.7019920945167542\n",
            "epoch: 1 step: 249, loss is 0.6978740692138672\n",
            "epoch: 1 step: 250, loss is 0.7006723880767822\n",
            "epoch: 1 step: 251, loss is 0.6851679086685181\n",
            "epoch: 1 step: 252, loss is 0.7033947110176086\n",
            "epoch: 1 step: 253, loss is 0.693394124507904\n",
            "epoch: 1 step: 254, loss is 0.6803473234176636\n",
            "epoch: 1 step: 255, loss is 0.7200613617897034\n",
            "epoch: 1 step: 256, loss is 0.6752220988273621\n",
            "epoch: 1 step: 257, loss is 0.6904289722442627\n",
            "epoch: 1 step: 258, loss is 0.6858232617378235\n",
            "epoch: 1 step: 259, loss is 0.689167857170105\n",
            "epoch: 1 step: 260, loss is 0.6957014799118042\n",
            "epoch: 1 step: 261, loss is 0.6869341135025024\n",
            "epoch: 1 step: 262, loss is 0.6902563571929932\n",
            "epoch: 1 step: 263, loss is 0.6751717925071716\n",
            "epoch: 1 step: 264, loss is 0.6990472078323364\n",
            "epoch: 1 step: 265, loss is 0.687027633190155\n",
            "epoch: 1 step: 266, loss is 0.672916829586029\n",
            "epoch: 1 step: 267, loss is 0.7061980962753296\n",
            "epoch: 1 step: 268, loss is 0.6922844648361206\n",
            "epoch: 1 step: 269, loss is 0.6921579837799072\n",
            "epoch: 1 step: 270, loss is 0.6835106015205383\n",
            "epoch: 1 step: 271, loss is 0.6901363134384155\n",
            "epoch: 1 step: 272, loss is 0.6918822526931763\n",
            "epoch: 1 step: 273, loss is 0.6762747764587402\n",
            "epoch: 1 step: 274, loss is 0.6963613033294678\n",
            "epoch: 1 step: 275, loss is 0.6781391501426697\n",
            "epoch: 1 step: 276, loss is 0.7069553136825562\n",
            "epoch: 1 step: 277, loss is 0.6831216216087341\n",
            "epoch: 1 step: 278, loss is 0.6905504465103149\n",
            "epoch: 1 step: 279, loss is 0.6639905571937561\n",
            "epoch: 1 step: 280, loss is 0.6863094568252563\n",
            "epoch: 1 step: 281, loss is 0.6985094547271729\n",
            "epoch: 1 step: 282, loss is 0.701927125453949\n",
            "epoch: 1 step: 283, loss is 0.6928132772445679\n",
            "epoch: 1 step: 284, loss is 0.6703652739524841\n",
            "epoch: 1 step: 285, loss is 0.6703124642372131\n",
            "epoch: 1 step: 286, loss is 0.6879972815513611\n",
            "epoch: 1 step: 287, loss is 0.6857654452323914\n",
            "epoch: 1 step: 288, loss is 0.6892313957214355\n",
            "epoch: 1 step: 289, loss is 0.690041720867157\n",
            "epoch: 1 step: 290, loss is 0.6894563436508179\n",
            "epoch: 1 step: 291, loss is 0.6849129796028137\n",
            "epoch: 1 step: 292, loss is 0.6758890748023987\n",
            "epoch: 1 step: 293, loss is 0.7068350315093994\n",
            "epoch: 1 step: 294, loss is 0.6750896573066711\n",
            "epoch: 1 step: 295, loss is 0.6897359490394592\n",
            "epoch: 1 step: 296, loss is 0.706746518611908\n",
            "epoch: 1 step: 297, loss is 0.6878296732902527\n",
            "epoch: 1 step: 298, loss is 0.6819795966148376\n",
            "epoch: 1 step: 299, loss is 0.6832496523857117\n",
            "epoch: 1 step: 300, loss is 0.686646044254303\n",
            "epoch: 1 step: 301, loss is 0.6865111589431763\n",
            "epoch: 1 step: 302, loss is 0.7030655145645142\n",
            "epoch: 1 step: 303, loss is 0.6939084529876709\n",
            "epoch: 1 step: 304, loss is 0.6971865296363831\n",
            "epoch: 1 step: 305, loss is 0.6857391595840454\n",
            "epoch: 1 step: 306, loss is 0.6889328956604004\n",
            "epoch: 1 step: 307, loss is 0.7005211710929871\n",
            "epoch: 1 step: 308, loss is 0.6909115314483643\n",
            "epoch: 1 step: 309, loss is 0.6696113348007202\n",
            "epoch: 1 step: 310, loss is 0.6931300759315491\n",
            "epoch: 1 step: 311, loss is 0.6808501482009888\n",
            "epoch: 1 step: 312, loss is 0.687725841999054\n",
            "epoch: 1 step: 313, loss is 0.7014504671096802\n",
            "epoch: 1 step: 314, loss is 0.6772522330284119\n",
            "epoch: 1 step: 315, loss is 0.6719499826431274\n",
            "epoch: 1 step: 316, loss is 0.7068342566490173\n",
            "epoch: 1 step: 317, loss is 0.6938825845718384\n",
            "epoch: 1 step: 318, loss is 0.6856754422187805\n",
            "epoch: 1 step: 319, loss is 0.7021414041519165\n",
            "epoch: 1 step: 320, loss is 0.7048759460449219\n",
            "epoch: 1 step: 321, loss is 0.7089439630508423\n",
            "epoch: 1 step: 322, loss is 0.6730933785438538\n",
            "epoch: 1 step: 323, loss is 0.7103606462478638\n",
            "epoch: 1 step: 324, loss is 0.6929431557655334\n",
            "epoch: 1 step: 325, loss is 0.6721933484077454\n",
            "epoch: 1 step: 326, loss is 0.6959709525108337\n",
            "epoch: 1 step: 327, loss is 0.6787337064743042\n",
            "epoch: 1 step: 328, loss is 0.6917310357093811\n",
            "epoch: 1 step: 329, loss is 0.6744436025619507\n",
            "epoch: 1 step: 330, loss is 0.6627122759819031\n",
            "epoch: 1 step: 331, loss is 0.6505628824234009\n",
            "epoch: 1 step: 332, loss is 0.677608072757721\n",
            "epoch: 1 step: 333, loss is 0.6863166093826294\n",
            "epoch: 1 step: 334, loss is 0.6872514486312866\n",
            "epoch: 1 step: 335, loss is 0.6836310029029846\n",
            "epoch: 1 step: 336, loss is 0.6731727123260498\n",
            "epoch: 1 step: 337, loss is 0.6955075860023499\n",
            "epoch: 1 step: 338, loss is 0.6891034245491028\n",
            "epoch: 1 step: 339, loss is 0.6865708827972412\n",
            "epoch: 1 step: 340, loss is 0.6673340201377869\n",
            "epoch: 1 step: 341, loss is 0.6852658987045288\n",
            "epoch: 1 step: 342, loss is 0.6935227513313293\n",
            "epoch: 1 step: 343, loss is 0.652823805809021\n",
            "epoch: 1 step: 344, loss is 0.6707181930541992\n",
            "epoch: 1 step: 345, loss is 0.705314040184021\n",
            "epoch: 1 step: 346, loss is 0.6805851459503174\n",
            "epoch: 1 step: 347, loss is 0.6723379492759705\n",
            "epoch: 1 step: 348, loss is 0.6886091828346252\n",
            "epoch: 1 step: 349, loss is 0.6696802377700806\n",
            "epoch: 1 step: 350, loss is 0.6784532070159912\n",
            "epoch: 1 step: 351, loss is 0.6817978024482727\n",
            "epoch: 1 step: 352, loss is 0.6912509202957153\n",
            "epoch: 1 step: 353, loss is 0.6768752932548523\n",
            "epoch: 1 step: 354, loss is 0.6785151958465576\n",
            "epoch: 1 step: 355, loss is 0.682604193687439\n",
            "epoch: 1 step: 356, loss is 0.6908044219017029\n",
            "epoch: 1 step: 357, loss is 0.6898553371429443\n",
            "epoch: 1 step: 358, loss is 0.6718819737434387\n",
            "epoch: 1 step: 359, loss is 0.6744499206542969\n",
            "epoch: 1 step: 360, loss is 0.6831732988357544\n",
            "epoch: 1 step: 361, loss is 0.6417686939239502\n",
            "epoch: 1 step: 362, loss is 0.6820105314254761\n",
            "epoch: 1 step: 363, loss is 0.7078918218612671\n",
            "epoch: 1 step: 364, loss is 0.7072797417640686\n",
            "epoch: 1 step: 365, loss is 0.693633496761322\n",
            "epoch: 1 step: 366, loss is 0.6603375673294067\n",
            "epoch: 1 step: 367, loss is 0.6679695844650269\n",
            "epoch: 1 step: 368, loss is 0.6814994812011719\n",
            "epoch: 1 step: 369, loss is 0.6855903267860413\n",
            "epoch: 1 step: 370, loss is 0.7160149812698364\n",
            "epoch: 1 step: 371, loss is 0.667167603969574\n",
            "epoch: 1 step: 372, loss is 0.708419144153595\n",
            "epoch: 1 step: 373, loss is 0.6795842051506042\n",
            "epoch: 1 step: 374, loss is 0.7055099010467529\n",
            "epoch: 1 step: 375, loss is 0.7251514196395874\n",
            "epoch: 1 step: 376, loss is 0.6576706767082214\n",
            "epoch: 1 step: 377, loss is 0.6987359523773193\n",
            "epoch: 1 step: 378, loss is 0.6501293778419495\n",
            "epoch: 1 step: 379, loss is 0.7081981301307678\n",
            "epoch: 1 step: 380, loss is 0.6544146537780762\n",
            "epoch: 1 step: 381, loss is 0.7021160125732422\n",
            "epoch: 1 step: 382, loss is 0.67411208152771\n",
            "epoch: 1 step: 383, loss is 0.6844295263290405\n",
            "epoch: 1 step: 384, loss is 0.6430535912513733\n",
            "epoch: 1 step: 385, loss is 0.6989084482192993\n",
            "epoch: 1 step: 386, loss is 0.6995216012001038\n",
            "epoch: 1 step: 387, loss is 0.6797486543655396\n",
            "epoch: 1 step: 388, loss is 0.689468502998352\n",
            "epoch: 1 step: 389, loss is 0.6527639627456665\n",
            "epoch: 1 step: 390, loss is 0.6916155219078064\n",
            "epoch: 1 step: 391, loss is 0.6898539066314697\n",
            "epoch: 1 step: 392, loss is 0.6846041679382324\n",
            "epoch: 1 step: 393, loss is 0.7201618552207947\n",
            "epoch: 1 step: 394, loss is 0.6840261220932007\n",
            "epoch: 1 step: 395, loss is 0.6784024238586426\n",
            "epoch: 1 step: 396, loss is 0.6759828329086304\n",
            "epoch: 1 step: 397, loss is 0.671062171459198\n",
            "epoch: 1 step: 398, loss is 0.687682032585144\n",
            "epoch: 1 step: 399, loss is 0.6919824481010437\n",
            "epoch: 1 step: 400, loss is 0.6434904336929321\n",
            "epoch: 1 step: 401, loss is 0.6512121558189392\n",
            "epoch: 1 step: 402, loss is 0.6467164158821106\n",
            "epoch: 1 step: 403, loss is 0.7107667326927185\n",
            "epoch: 1 step: 404, loss is 0.6251147389411926\n",
            "epoch: 1 step: 405, loss is 0.6978501081466675\n",
            "epoch: 1 step: 406, loss is 0.670627236366272\n",
            "epoch: 1 step: 407, loss is 0.6152095794677734\n",
            "epoch: 1 step: 408, loss is 0.722051739692688\n",
            "epoch: 1 step: 409, loss is 0.654280960559845\n",
            "epoch: 1 step: 410, loss is 0.7049723863601685\n",
            "epoch: 1 step: 411, loss is 0.6593997478485107\n",
            "epoch: 1 step: 412, loss is 0.6114150881767273\n",
            "epoch: 1 step: 413, loss is 0.7512044906616211\n",
            "epoch: 1 step: 414, loss is 0.737090528011322\n",
            "epoch: 1 step: 415, loss is 0.8172857165336609\n",
            "epoch: 1 step: 416, loss is 0.6891701221466064\n",
            "epoch: 1 step: 417, loss is 0.659893810749054\n",
            "epoch: 1 step: 418, loss is 0.648154616355896\n",
            "epoch: 1 step: 419, loss is 0.7049063444137573\n",
            "epoch: 1 step: 420, loss is 0.6581015586853027\n",
            "epoch: 1 step: 421, loss is 0.6609350442886353\n",
            "epoch: 1 step: 422, loss is 0.6776505708694458\n",
            "epoch: 1 step: 423, loss is 0.6422725915908813\n",
            "epoch: 1 step: 424, loss is 0.6665869355201721\n",
            "epoch: 1 step: 425, loss is 0.7024574279785156\n",
            "epoch: 1 step: 426, loss is 0.6729800701141357\n",
            "epoch: 1 step: 427, loss is 0.6320035457611084\n",
            "epoch: 1 step: 428, loss is 0.6866655945777893\n",
            "epoch: 1 step: 429, loss is 0.616833508014679\n",
            "epoch: 1 step: 430, loss is 0.718923032283783\n",
            "epoch: 1 step: 431, loss is 0.6703829169273376\n",
            "epoch: 1 step: 432, loss is 0.6517183184623718\n",
            "epoch: 1 step: 433, loss is 0.6975739598274231\n",
            "epoch: 1 step: 434, loss is 0.6695551872253418\n",
            "epoch: 1 step: 435, loss is 0.6129075884819031\n",
            "epoch: 1 step: 436, loss is 0.6123392581939697\n",
            "epoch: 1 step: 437, loss is 0.7094430327415466\n",
            "epoch: 1 step: 438, loss is 0.6223771572113037\n",
            "epoch: 1 step: 439, loss is 0.601232647895813\n",
            "epoch: 1 step: 440, loss is 0.7377956509590149\n",
            "epoch: 1 step: 441, loss is 0.6980872750282288\n",
            "epoch: 1 step: 442, loss is 0.6457094550132751\n",
            "epoch: 1 step: 443, loss is 0.6937225461006165\n",
            "epoch: 1 step: 444, loss is 0.6618034243583679\n",
            "epoch: 1 step: 445, loss is 0.6935014128684998\n",
            "epoch: 1 step: 446, loss is 0.7120884656906128\n",
            "epoch: 1 step: 447, loss is 0.5966779589653015\n",
            "epoch: 1 step: 448, loss is 0.69385826587677\n",
            "epoch: 1 step: 449, loss is 0.6383137702941895\n",
            "epoch: 1 step: 450, loss is 0.6608139872550964\n",
            "epoch: 1 step: 451, loss is 0.6756152510643005\n",
            "epoch: 1 step: 452, loss is 0.6304727792739868\n",
            "epoch: 1 step: 453, loss is 0.7072678804397583\n",
            "epoch: 1 step: 454, loss is 0.6813696622848511\n",
            "epoch: 1 step: 455, loss is 0.6133853793144226\n",
            "epoch: 1 step: 456, loss is 0.7454485297203064\n",
            "epoch: 1 step: 457, loss is 0.6820679903030396\n",
            "epoch: 1 step: 458, loss is 0.6067937612533569\n",
            "epoch: 1 step: 459, loss is 0.6268755793571472\n",
            "epoch: 1 step: 460, loss is 0.6500570774078369\n",
            "epoch: 1 step: 461, loss is 0.6699959635734558\n",
            "epoch: 1 step: 462, loss is 0.6608424782752991\n",
            "epoch: 1 step: 463, loss is 0.6872699856758118\n",
            "epoch: 1 step: 464, loss is 0.6780538558959961\n",
            "epoch: 1 step: 465, loss is 0.6290098428726196\n",
            "epoch: 1 step: 466, loss is 0.6321264505386353\n",
            "epoch: 1 step: 467, loss is 0.6464684009552002\n",
            "epoch: 1 step: 468, loss is 0.6708202958106995\n",
            "epoch: 1 step: 469, loss is 0.6258053779602051\n",
            "epoch: 1 step: 470, loss is 0.6858773827552795\n",
            "epoch: 1 step: 471, loss is 0.6310694217681885\n",
            "epoch: 1 step: 472, loss is 0.5560392737388611\n",
            "epoch: 1 step: 473, loss is 0.6217127442359924\n",
            "epoch: 1 step: 474, loss is 0.7392265796661377\n",
            "epoch: 1 step: 475, loss is 0.710616409778595\n",
            "epoch: 1 step: 476, loss is 0.6525411605834961\n",
            "epoch: 1 step: 477, loss is 0.653500497341156\n",
            "epoch: 1 step: 478, loss is 0.6322804689407349\n",
            "epoch: 1 step: 479, loss is 0.7399182915687561\n",
            "epoch: 1 step: 480, loss is 0.6823172569274902\n",
            "epoch: 1 step: 481, loss is 0.7373174428939819\n",
            "epoch: 1 step: 482, loss is 0.6693794131278992\n",
            "epoch: 1 step: 483, loss is 0.6158904433250427\n",
            "epoch: 1 step: 484, loss is 0.640646755695343\n",
            "epoch: 1 step: 485, loss is 0.6303815841674805\n",
            "epoch: 1 step: 486, loss is 0.5925341248512268\n",
            "epoch: 1 step: 487, loss is 0.6505820155143738\n",
            "epoch: 1 step: 488, loss is 0.6033093333244324\n",
            "epoch: 1 step: 489, loss is 0.6493911743164062\n",
            "epoch: 1 step: 490, loss is 0.6071644425392151\n",
            "epoch: 1 step: 491, loss is 0.661602795124054\n",
            "epoch: 1 step: 492, loss is 0.63209068775177\n",
            "epoch: 1 step: 493, loss is 0.6374101042747498\n",
            "epoch: 1 step: 494, loss is 0.7317372560501099\n",
            "epoch: 1 step: 495, loss is 0.6375373005867004\n",
            "epoch: 1 step: 496, loss is 0.6618205308914185\n",
            "epoch: 1 step: 497, loss is 0.678009569644928\n",
            "epoch: 1 step: 498, loss is 0.5937930345535278\n",
            "epoch: 1 step: 499, loss is 0.6821846961975098\n",
            "epoch: 1 step: 500, loss is 0.6267560124397278\n",
            "epoch: 1 step: 501, loss is 0.5768582224845886\n",
            "epoch: 1 step: 502, loss is 0.6429316401481628\n",
            "epoch: 1 step: 503, loss is 0.694294273853302\n",
            "epoch: 1 step: 504, loss is 0.6818758249282837\n",
            "epoch: 1 step: 505, loss is 0.6063408255577087\n",
            "epoch: 1 step: 506, loss is 0.6114470958709717\n",
            "epoch: 1 step: 507, loss is 0.5967381000518799\n",
            "epoch: 1 step: 508, loss is 0.5772680044174194\n",
            "epoch: 1 step: 509, loss is 0.6370266675949097\n",
            "epoch: 1 step: 510, loss is 0.6188560128211975\n",
            "epoch: 1 step: 511, loss is 0.6245624423027039\n",
            "epoch: 1 step: 512, loss is 0.5544335842132568\n",
            "epoch: 1 step: 513, loss is 0.5727812647819519\n",
            "epoch: 1 step: 514, loss is 0.5930472612380981\n",
            "epoch: 1 step: 515, loss is 0.5558836460113525\n",
            "epoch: 1 step: 516, loss is 0.6449970006942749\n",
            "epoch: 1 step: 517, loss is 0.6136997938156128\n",
            "epoch: 1 step: 518, loss is 0.7193812131881714\n",
            "epoch: 1 step: 519, loss is 0.5979987382888794\n",
            "epoch: 1 step: 520, loss is 0.4941166341304779\n",
            "epoch: 1 step: 521, loss is 0.6823101043701172\n",
            "epoch: 1 step: 522, loss is 0.7283341288566589\n",
            "epoch: 1 step: 523, loss is 0.6211692094802856\n",
            "epoch: 1 step: 524, loss is 0.6117395162582397\n",
            "epoch: 1 step: 525, loss is 0.5957359671592712\n",
            "epoch: 1 step: 526, loss is 0.5962098836898804\n",
            "epoch: 1 step: 527, loss is 0.5556791424751282\n",
            "epoch: 1 step: 528, loss is 0.4927036464214325\n",
            "epoch: 1 step: 529, loss is 0.5819196701049805\n",
            "epoch: 1 step: 530, loss is 0.5900376439094543\n",
            "epoch: 1 step: 531, loss is 0.47419285774230957\n",
            "epoch: 1 step: 532, loss is 0.6211837530136108\n",
            "epoch: 1 step: 533, loss is 0.7058423161506653\n",
            "epoch: 1 step: 534, loss is 0.6749300360679626\n",
            "epoch: 1 step: 535, loss is 0.6954102516174316\n",
            "epoch: 1 step: 536, loss is 0.6240885257720947\n",
            "epoch: 1 step: 537, loss is 0.5735026001930237\n",
            "epoch: 1 step: 538, loss is 0.557855486869812\n",
            "epoch: 1 step: 539, loss is 0.6613571643829346\n",
            "epoch: 1 step: 540, loss is 0.6460375785827637\n",
            "epoch: 1 step: 541, loss is 0.6098790764808655\n",
            "epoch: 1 step: 542, loss is 0.6019520163536072\n",
            "epoch: 1 step: 543, loss is 0.6229703426361084\n",
            "epoch: 1 step: 544, loss is 0.6344442963600159\n",
            "epoch: 1 step: 545, loss is 0.5395938754081726\n",
            "epoch: 1 step: 546, loss is 0.5542559623718262\n",
            "epoch: 1 step: 547, loss is 0.5446009039878845\n",
            "epoch: 1 step: 548, loss is 0.5199307203292847\n",
            "epoch: 1 step: 549, loss is 0.6537032127380371\n",
            "epoch: 1 step: 550, loss is 0.664564311504364\n",
            "epoch: 1 step: 551, loss is 0.5097041130065918\n",
            "epoch: 1 step: 552, loss is 0.5496016144752502\n",
            "epoch: 1 step: 553, loss is 0.5735283493995667\n",
            "epoch: 1 step: 554, loss is 0.6324288249015808\n",
            "epoch: 1 step: 555, loss is 0.5603973269462585\n",
            "epoch: 1 step: 556, loss is 0.6096088290214539\n",
            "epoch: 1 step: 557, loss is 0.5184287428855896\n",
            "epoch: 1 step: 558, loss is 0.5326154828071594\n",
            "epoch: 1 step: 559, loss is 0.46673426032066345\n",
            "epoch: 1 step: 560, loss is 0.5058383941650391\n",
            "epoch: 1 step: 561, loss is 0.6985037326812744\n",
            "epoch: 1 step: 562, loss is 0.6105363368988037\n",
            "epoch: 1 step: 563, loss is 0.626401960849762\n",
            "epoch: 1 step: 564, loss is 0.6754906177520752\n",
            "epoch: 1 step: 565, loss is 0.6236918568611145\n",
            "epoch: 1 step: 566, loss is 0.6366518139839172\n",
            "epoch: 1 step: 567, loss is 0.5657841563224792\n",
            "epoch: 1 step: 568, loss is 0.6770520806312561\n",
            "epoch: 1 step: 569, loss is 0.6226996183395386\n",
            "epoch: 1 step: 570, loss is 0.575346827507019\n",
            "epoch: 1 step: 571, loss is 0.49456748366355896\n",
            "epoch: 1 step: 572, loss is 0.5012642741203308\n",
            "epoch: 1 step: 573, loss is 0.5201464295387268\n",
            "epoch: 1 step: 574, loss is 0.4689842462539673\n",
            "epoch: 1 step: 575, loss is 0.7101649045944214\n",
            "epoch: 1 step: 576, loss is 0.5853714346885681\n",
            "epoch: 1 step: 577, loss is 0.5236693620681763\n",
            "epoch: 1 step: 578, loss is 0.5566930770874023\n",
            "epoch: 1 step: 579, loss is 0.6109449863433838\n",
            "epoch: 1 step: 580, loss is 0.566241443157196\n",
            "epoch: 1 step: 581, loss is 0.5421283841133118\n",
            "epoch: 1 step: 582, loss is 0.5702142715454102\n",
            "epoch: 1 step: 583, loss is 0.5498954057693481\n",
            "epoch: 1 step: 584, loss is 0.5318076610565186\n",
            "epoch: 1 step: 585, loss is 0.5540319681167603\n",
            "epoch: 1 step: 586, loss is 0.479570209980011\n",
            "epoch: 1 step: 587, loss is 0.5798605680465698\n",
            "epoch: 1 step: 588, loss is 0.5687892436981201\n",
            "epoch: 1 step: 589, loss is 0.4789024591445923\n",
            "epoch: 1 step: 590, loss is 0.5650785565376282\n",
            "epoch: 1 step: 591, loss is 0.6387099623680115\n",
            "epoch: 1 step: 592, loss is 0.6015493869781494\n",
            "epoch: 1 step: 593, loss is 0.49443018436431885\n",
            "epoch: 1 step: 594, loss is 0.6434151530265808\n",
            "epoch: 1 step: 595, loss is 0.5893633961677551\n",
            "epoch: 1 step: 596, loss is 0.639614462852478\n",
            "epoch: 1 step: 597, loss is 0.5191877484321594\n",
            "epoch: 1 step: 598, loss is 0.6950625777244568\n",
            "epoch: 1 step: 599, loss is 0.5305758118629456\n",
            "epoch: 1 step: 600, loss is 0.5128224492073059\n",
            "epoch: 1 step: 601, loss is 0.576866865158081\n",
            "epoch: 1 step: 602, loss is 0.5680476427078247\n",
            "epoch: 1 step: 603, loss is 0.6775860786437988\n",
            "epoch: 1 step: 604, loss is 0.4172145128250122\n",
            "epoch: 1 step: 605, loss is 0.6686550974845886\n",
            "epoch: 1 step: 606, loss is 0.6760149598121643\n",
            "epoch: 1 step: 607, loss is 0.6136229038238525\n",
            "epoch: 1 step: 608, loss is 0.5190155506134033\n",
            "epoch: 1 step: 609, loss is 0.5015629529953003\n",
            "epoch: 1 step: 610, loss is 0.5593352317810059\n",
            "epoch: 1 step: 611, loss is 0.5383731722831726\n",
            "epoch: 1 step: 612, loss is 0.49674615263938904\n",
            "epoch: 1 step: 613, loss is 0.5500619411468506\n",
            "epoch: 1 step: 614, loss is 0.5220416784286499\n",
            "epoch: 1 step: 615, loss is 0.487151563167572\n",
            "epoch: 1 step: 616, loss is 0.5497440695762634\n",
            "epoch: 1 step: 617, loss is 0.5823968648910522\n",
            "epoch: 1 step: 618, loss is 0.4958614110946655\n",
            "epoch: 1 step: 619, loss is 0.5840944647789001\n",
            "epoch: 1 step: 620, loss is 0.5853807330131531\n",
            "epoch: 1 step: 621, loss is 0.5512509346008301\n",
            "epoch: 1 step: 622, loss is 0.5339853167533875\n",
            "epoch: 1 step: 623, loss is 0.5126968622207642\n",
            "epoch: 1 step: 624, loss is 0.5183298587799072\n",
            "epoch: 1 step: 625, loss is 0.5700675845146179\n",
            "epoch: 1 step: 626, loss is 0.49728628993034363\n",
            "epoch: 1 step: 627, loss is 0.6265552043914795\n",
            "epoch: 1 step: 628, loss is 0.6478642821311951\n",
            "epoch: 1 step: 629, loss is 0.5797843933105469\n",
            "epoch: 1 step: 630, loss is 0.5448189973831177\n",
            "epoch: 1 step: 631, loss is 0.5799512267112732\n",
            "epoch: 1 step: 632, loss is 0.467292457818985\n",
            "epoch: 1 step: 633, loss is 0.506595253944397\n",
            "epoch: 1 step: 634, loss is 0.6271001100540161\n",
            "epoch: 1 step: 635, loss is 0.44350388646125793\n",
            "epoch: 1 step: 636, loss is 0.4881492853164673\n",
            "epoch: 1 step: 637, loss is 0.5839865803718567\n",
            "epoch: 1 step: 638, loss is 0.4366280436515808\n",
            "epoch: 1 step: 639, loss is 0.6524454951286316\n",
            "epoch: 1 step: 640, loss is 0.5423611402511597\n",
            "epoch: 1 step: 641, loss is 0.6135732531547546\n",
            "epoch: 1 step: 642, loss is 0.5703092813491821\n",
            "epoch: 1 step: 643, loss is 0.5972796678543091\n",
            "epoch: 1 step: 644, loss is 0.5555474162101746\n",
            "epoch: 1 step: 645, loss is 0.45058462023735046\n",
            "epoch: 1 step: 646, loss is 0.5130931735038757\n",
            "epoch: 1 step: 647, loss is 0.5554258227348328\n",
            "epoch: 1 step: 648, loss is 0.5297825336456299\n",
            "epoch: 1 step: 649, loss is 0.6271121501922607\n",
            "epoch: 1 step: 650, loss is 0.43790796399116516\n",
            "epoch: 1 step: 651, loss is 0.4457549750804901\n",
            "epoch: 1 step: 652, loss is 0.5713872909545898\n",
            "epoch: 1 step: 653, loss is 0.6050320863723755\n",
            "epoch: 1 step: 654, loss is 0.5127296447753906\n",
            "epoch: 1 step: 655, loss is 0.5131939649581909\n",
            "epoch: 1 step: 656, loss is 0.5286626815795898\n",
            "epoch: 1 step: 657, loss is 0.677604079246521\n",
            "epoch: 1 step: 658, loss is 0.583441972732544\n",
            "epoch: 1 step: 659, loss is 0.6108309626579285\n",
            "epoch: 1 step: 660, loss is 0.49702200293540955\n",
            "epoch: 1 step: 661, loss is 0.6133779287338257\n",
            "epoch: 1 step: 662, loss is 0.5884425044059753\n",
            "epoch: 1 step: 663, loss is 0.5754870772361755\n",
            "epoch: 1 step: 664, loss is 0.5069624185562134\n",
            "epoch: 1 step: 665, loss is 0.4544946849346161\n",
            "epoch: 1 step: 666, loss is 0.6356541514396667\n",
            "epoch: 1 step: 667, loss is 0.535383403301239\n",
            "epoch: 1 step: 668, loss is 0.46623238921165466\n",
            "epoch: 1 step: 669, loss is 0.6248729228973389\n",
            "epoch: 1 step: 670, loss is 0.5623921155929565\n",
            "epoch: 1 step: 671, loss is 0.5454456806182861\n",
            "epoch: 1 step: 672, loss is 0.5179013609886169\n",
            "epoch: 1 step: 673, loss is 0.510921061038971\n",
            "epoch: 1 step: 674, loss is 0.7014778256416321\n",
            "epoch: 1 step: 675, loss is 0.5357911586761475\n",
            "epoch: 1 step: 676, loss is 0.6708158850669861\n",
            "epoch: 1 step: 677, loss is 0.38672953844070435\n",
            "epoch: 1 step: 678, loss is 0.4786040782928467\n",
            "epoch: 1 step: 679, loss is 0.6291224956512451\n",
            "epoch: 1 step: 680, loss is 0.5397372841835022\n",
            "epoch: 1 step: 681, loss is 0.5017088651657104\n",
            "epoch: 1 step: 682, loss is 0.4716404974460602\n",
            "epoch: 1 step: 683, loss is 0.4572339951992035\n",
            "epoch: 1 step: 684, loss is 0.5388644337654114\n",
            "epoch: 1 step: 685, loss is 0.5365272164344788\n",
            "epoch: 1 step: 686, loss is 0.6096603870391846\n",
            "epoch: 1 step: 687, loss is 0.40325281023979187\n",
            "epoch: 1 step: 688, loss is 0.49187904596328735\n",
            "epoch: 1 step: 689, loss is 0.4593542814254761\n",
            "epoch: 1 step: 690, loss is 0.48553410172462463\n",
            "epoch: 1 step: 691, loss is 0.512054979801178\n",
            "epoch: 1 step: 692, loss is 0.49138951301574707\n",
            "epoch: 1 step: 693, loss is 0.5506548881530762\n",
            "epoch: 1 step: 694, loss is 0.49545902013778687\n",
            "epoch: 1 step: 695, loss is 0.4351002275943756\n",
            "epoch: 1 step: 696, loss is 0.7204155921936035\n",
            "epoch: 1 step: 697, loss is 0.6396999359130859\n",
            "epoch: 1 step: 698, loss is 0.3978491425514221\n",
            "epoch: 1 step: 699, loss is 0.5837178826332092\n",
            "epoch: 1 step: 700, loss is 0.5243771076202393\n",
            "epoch: 1 step: 701, loss is 0.44428563117980957\n",
            "epoch: 1 step: 702, loss is 0.5469678044319153\n",
            "epoch: 1 step: 703, loss is 0.5322901606559753\n",
            "epoch: 1 step: 704, loss is 0.45471125841140747\n",
            "epoch: 1 step: 705, loss is 0.5875457525253296\n",
            "epoch: 1 step: 706, loss is 0.48558202385902405\n",
            "epoch: 1 step: 707, loss is 0.38045966625213623\n",
            "epoch: 1 step: 708, loss is 0.5448607206344604\n",
            "epoch: 1 step: 709, loss is 0.6046561002731323\n",
            "epoch: 1 step: 710, loss is 0.4153099060058594\n",
            "epoch: 1 step: 711, loss is 0.5595044493675232\n",
            "epoch: 1 step: 712, loss is 0.46257826685905457\n",
            "epoch: 1 step: 713, loss is 0.5490467548370361\n",
            "epoch: 1 step: 714, loss is 0.459842711687088\n",
            "epoch: 1 step: 715, loss is 0.5483726859092712\n",
            "epoch: 1 step: 716, loss is 0.5986050963401794\n",
            "epoch: 1 step: 717, loss is 0.45735636353492737\n",
            "epoch: 1 step: 718, loss is 0.48021650314331055\n",
            "epoch: 1 step: 719, loss is 0.5801387429237366\n",
            "epoch: 1 step: 720, loss is 0.497416615486145\n",
            "epoch: 1 step: 721, loss is 0.4547678232192993\n",
            "epoch: 1 step: 722, loss is 0.5510981678962708\n",
            "epoch: 1 step: 723, loss is 0.5038931369781494\n",
            "epoch: 1 step: 724, loss is 0.5403672456741333\n",
            "epoch: 1 step: 725, loss is 0.5305365920066833\n",
            "epoch: 1 step: 726, loss is 0.5639674067497253\n",
            "epoch: 1 step: 727, loss is 0.4166138470172882\n",
            "epoch: 1 step: 728, loss is 0.519874632358551\n",
            "epoch: 1 step: 729, loss is 0.5463904142379761\n",
            "epoch: 1 step: 730, loss is 0.45859500765800476\n",
            "epoch: 1 step: 731, loss is 0.34615474939346313\n",
            "epoch: 1 step: 732, loss is 0.4339706301689148\n",
            "epoch: 1 step: 733, loss is 0.5955371260643005\n",
            "epoch: 1 step: 734, loss is 0.5236591100692749\n",
            "epoch: 1 step: 735, loss is 0.3817000985145569\n",
            "epoch: 1 step: 736, loss is 0.5831658244132996\n",
            "epoch: 1 step: 737, loss is 0.4658489227294922\n",
            "epoch: 1 step: 738, loss is 0.45424291491508484\n",
            "epoch: 1 step: 739, loss is 0.33951613306999207\n",
            "epoch: 1 step: 740, loss is 0.5077391266822815\n",
            "epoch: 1 step: 741, loss is 0.475955069065094\n",
            "epoch: 1 step: 742, loss is 0.40827831625938416\n",
            "epoch: 1 step: 743, loss is 0.46240416169166565\n",
            "epoch: 1 step: 744, loss is 0.41791436076164246\n",
            "epoch: 1 step: 745, loss is 0.5271431803703308\n",
            "epoch: 1 step: 746, loss is 0.48015785217285156\n",
            "epoch: 1 step: 747, loss is 0.5193383097648621\n",
            "epoch: 1 step: 748, loss is 0.4530274271965027\n",
            "epoch: 1 step: 749, loss is 0.4905393719673157\n",
            "epoch: 1 step: 750, loss is 0.46786075830459595\n",
            "epoch: 1 step: 751, loss is 0.49857333302497864\n",
            "epoch: 1 step: 752, loss is 0.5665082335472107\n",
            "epoch: 1 step: 753, loss is 0.3240990936756134\n",
            "epoch: 1 step: 754, loss is 0.46478337049484253\n",
            "epoch: 1 step: 755, loss is 0.40825513005256653\n",
            "epoch: 1 step: 756, loss is 0.4468420147895813\n",
            "epoch: 1 step: 757, loss is 0.40791770815849304\n",
            "epoch: 1 step: 758, loss is 0.46625465154647827\n",
            "epoch: 1 step: 759, loss is 0.49484163522720337\n",
            "epoch: 1 step: 760, loss is 0.6697922348976135\n",
            "epoch: 1 step: 761, loss is 0.40158116817474365\n",
            "epoch: 1 step: 762, loss is 0.40800344944000244\n",
            "epoch: 1 step: 763, loss is 0.34240731596946716\n",
            "epoch: 1 step: 764, loss is 0.3697196841239929\n",
            "epoch: 1 step: 765, loss is 0.39345505833625793\n",
            "epoch: 1 step: 766, loss is 0.4883114695549011\n",
            "epoch: 1 step: 767, loss is 0.39371249079704285\n",
            "epoch: 1 step: 768, loss is 0.6746389269828796\n",
            "epoch: 1 step: 769, loss is 0.38906964659690857\n",
            "epoch: 1 step: 770, loss is 0.47202375531196594\n",
            "epoch: 1 step: 771, loss is 0.4798823297023773\n",
            "epoch: 1 step: 772, loss is 0.4573720693588257\n",
            "epoch: 1 step: 773, loss is 0.43457189202308655\n",
            "epoch: 1 step: 774, loss is 0.4170999228954315\n",
            "epoch: 1 step: 775, loss is 0.4420619606971741\n",
            "epoch: 1 step: 776, loss is 0.3563828468322754\n",
            "epoch: 1 step: 777, loss is 0.40662285685539246\n",
            "epoch: 1 step: 778, loss is 0.3905448615550995\n",
            "epoch: 1 step: 779, loss is 0.44963619112968445\n",
            "epoch: 1 step: 780, loss is 0.3469884395599365\n",
            "epoch: 1 step: 781, loss is 0.3894864022731781\n",
            "epoch: 1 step: 782, loss is 0.6335585713386536\n",
            "epoch: 1 step: 783, loss is 0.49091753363609314\n",
            "epoch: 1 step: 784, loss is 0.3741290867328644\n",
            "epoch: 1 step: 785, loss is 0.43042755126953125\n",
            "epoch: 1 step: 786, loss is 0.4362580180168152\n",
            "epoch: 1 step: 787, loss is 0.5261279940605164\n",
            "epoch: 1 step: 788, loss is 0.3308126926422119\n",
            "epoch: 1 step: 789, loss is 0.41956406831741333\n",
            "epoch: 1 step: 790, loss is 0.49723297357559204\n",
            "epoch: 1 step: 791, loss is 0.4386337995529175\n",
            "epoch: 1 step: 792, loss is 0.4068520665168762\n",
            "epoch: 1 step: 793, loss is 0.3668734133243561\n",
            "epoch: 1 step: 794, loss is 0.4422955811023712\n",
            "epoch: 1 step: 795, loss is 0.7809700965881348\n",
            "epoch: 1 step: 796, loss is 0.8049705624580383\n",
            "epoch: 1 step: 797, loss is 0.8183956742286682\n",
            "epoch: 1 step: 798, loss is 0.7873495817184448\n",
            "epoch: 1 step: 799, loss is 0.671779215335846\n",
            "epoch: 1 step: 800, loss is 0.6748451590538025\n",
            "epoch: 1 step: 801, loss is 0.7035366296768188\n",
            "epoch: 1 step: 802, loss is 0.8021093010902405\n",
            "epoch: 1 step: 803, loss is 0.7702345252037048\n",
            "epoch: 1 step: 804, loss is 0.7641492486000061\n",
            "epoch: 1 step: 805, loss is 0.4132309556007385\n",
            "epoch: 1 step: 806, loss is 0.2828536331653595\n",
            "epoch: 1 step: 807, loss is 0.36039984226226807\n",
            "epoch: 1 step: 808, loss is 0.3521646559238434\n",
            "epoch: 1 step: 809, loss is 0.41402924060821533\n",
            "epoch: 1 step: 810, loss is 0.4068019986152649\n",
            "epoch: 1 step: 811, loss is 0.6414774060249329\n",
            "epoch: 1 step: 812, loss is 0.4874081611633301\n",
            "epoch: 1 step: 813, loss is 0.4320259988307953\n",
            "epoch: 1 step: 814, loss is 0.6160188913345337\n",
            "epoch: 1 step: 815, loss is 0.31550127267837524\n",
            "epoch: 1 step: 816, loss is 0.5398610234260559\n",
            "epoch: 1 step: 817, loss is 0.3776247203350067\n",
            "epoch: 1 step: 818, loss is 0.4143737554550171\n",
            "epoch: 1 step: 819, loss is 0.5096532702445984\n",
            "epoch: 1 step: 820, loss is 0.4060935974121094\n",
            "epoch: 1 step: 821, loss is 0.4116133451461792\n",
            "epoch: 1 step: 822, loss is 0.4644830822944641\n",
            "epoch: 1 step: 823, loss is 0.5047199726104736\n",
            "epoch: 1 step: 824, loss is 0.4779483675956726\n",
            "epoch: 1 step: 825, loss is 0.4572853744029999\n",
            "epoch: 1 step: 826, loss is 0.48945021629333496\n",
            "epoch: 1 step: 827, loss is 0.3265596330165863\n",
            "epoch: 1 step: 828, loss is 0.3669264614582062\n",
            "epoch: 1 step: 829, loss is 0.566841185092926\n",
            "epoch: 1 step: 830, loss is 0.43089184165000916\n",
            "epoch: 1 step: 831, loss is 0.5772539973258972\n",
            "epoch: 1 step: 832, loss is 0.3883163332939148\n",
            "epoch: 1 step: 833, loss is 0.35969486832618713\n",
            "epoch: 1 step: 834, loss is 0.3885830342769623\n",
            "epoch: 1 step: 835, loss is 0.5014677047729492\n",
            "epoch: 1 step: 836, loss is 0.4315647482872009\n",
            "epoch: 1 step: 837, loss is 0.4372202754020691\n",
            "epoch: 1 step: 838, loss is 0.3532821536064148\n",
            "epoch: 1 step: 839, loss is 0.5092505812644958\n",
            "epoch: 1 step: 840, loss is 0.3121396601200104\n",
            "epoch: 1 step: 841, loss is 0.4886206388473511\n",
            "epoch: 1 step: 842, loss is 0.35702380537986755\n",
            "epoch: 1 step: 843, loss is 0.5661091208457947\n",
            "epoch: 1 step: 844, loss is 0.4792895019054413\n",
            "epoch: 1 step: 845, loss is 0.4795004725456238\n",
            "epoch: 1 step: 846, loss is 0.4349749684333801\n",
            "epoch: 1 step: 847, loss is 0.3876318633556366\n",
            "epoch: 1 step: 848, loss is 0.4361417591571808\n",
            "epoch: 1 step: 849, loss is 0.40493708848953247\n",
            "epoch: 1 step: 850, loss is 0.3876466155052185\n",
            "epoch: 1 step: 851, loss is 0.49593648314476013\n",
            "epoch: 1 step: 852, loss is 0.38075289130210876\n",
            "epoch: 1 step: 853, loss is 0.3569622337818146\n",
            "epoch: 1 step: 854, loss is 0.4765954911708832\n",
            "epoch: 1 step: 855, loss is 0.43906405568122864\n",
            "epoch: 1 step: 856, loss is 0.3268839418888092\n",
            "epoch: 1 step: 857, loss is 0.39110851287841797\n",
            "epoch: 1 step: 858, loss is 0.27468955516815186\n",
            "epoch: 1 step: 859, loss is 0.28484031558036804\n",
            "epoch: 1 step: 860, loss is 0.34189265966415405\n",
            "epoch: 1 step: 861, loss is 0.5670815706253052\n",
            "epoch: 1 step: 862, loss is 0.4148413836956024\n",
            "epoch: 1 step: 863, loss is 0.32791629433631897\n",
            "epoch: 1 step: 864, loss is 0.45952484011650085\n",
            "epoch: 1 step: 865, loss is 0.4198307693004608\n",
            "epoch: 1 step: 866, loss is 0.3183797597885132\n",
            "epoch: 1 step: 867, loss is 0.4707793593406677\n",
            "epoch: 1 step: 868, loss is 0.3657762110233307\n",
            "epoch: 1 step: 869, loss is 0.4382588863372803\n",
            "epoch: 1 step: 870, loss is 0.3121526837348938\n",
            "epoch: 1 step: 871, loss is 0.4195218086242676\n",
            "epoch: 1 step: 872, loss is 0.38194528222084045\n",
            "epoch: 1 step: 873, loss is 0.3843924105167389\n",
            "epoch: 1 step: 874, loss is 0.28415197134017944\n",
            "epoch: 1 step: 875, loss is 0.29138678312301636\n",
            "epoch: 1 step: 876, loss is 0.41587045788764954\n",
            "epoch: 1 step: 877, loss is 0.42252376675605774\n",
            "epoch: 1 step: 878, loss is 0.3296152651309967\n",
            "epoch: 1 step: 879, loss is 0.37612512707710266\n",
            "epoch: 1 step: 880, loss is 0.25647151470184326\n",
            "epoch: 1 step: 881, loss is 0.37134528160095215\n",
            "epoch: 1 step: 882, loss is 0.6460896134376526\n",
            "epoch: 1 step: 883, loss is 0.503872811794281\n",
            "epoch: 1 step: 884, loss is 0.4236874580383301\n",
            "epoch: 1 step: 885, loss is 0.7031435370445251\n",
            "epoch: 1 step: 886, loss is 0.4728362560272217\n",
            "epoch: 1 step: 887, loss is 0.4155728220939636\n",
            "epoch: 1 step: 888, loss is 0.4304732084274292\n",
            "epoch: 1 step: 889, loss is 0.5458261370658875\n",
            "epoch: 1 step: 890, loss is 0.3998764157295227\n",
            "epoch: 1 step: 891, loss is 0.4287528395652771\n",
            "epoch: 1 step: 892, loss is 0.3827662765979767\n",
            "epoch: 1 step: 893, loss is 0.42681294679641724\n",
            "epoch: 1 step: 894, loss is 0.37265175580978394\n",
            "epoch: 1 step: 895, loss is 0.3971956968307495\n",
            "epoch: 1 step: 896, loss is 0.36399397253990173\n",
            "epoch: 1 step: 897, loss is 0.42398062348365784\n",
            "epoch: 1 step: 898, loss is 0.40487101674079895\n",
            "epoch: 1 step: 899, loss is 0.46161848306655884\n",
            "epoch: 1 step: 900, loss is 0.45773032307624817\n",
            "epoch: 1 step: 901, loss is 0.39371901750564575\n",
            "epoch: 1 step: 902, loss is 0.3889376223087311\n",
            "epoch: 1 step: 903, loss is 0.4121101200580597\n",
            "epoch: 1 step: 904, loss is 0.3105800747871399\n",
            "epoch: 1 step: 905, loss is 0.3500272333621979\n",
            "epoch: 1 step: 906, loss is 0.3082822561264038\n",
            "epoch: 1 step: 907, loss is 0.4327969551086426\n",
            "epoch: 1 step: 908, loss is 0.37854713201522827\n",
            "epoch: 1 step: 909, loss is 0.3931862711906433\n",
            "epoch: 1 step: 910, loss is 0.3471004366874695\n",
            "epoch: 1 step: 911, loss is 0.31624558568000793\n",
            "epoch: 1 step: 912, loss is 0.37138134241104126\n",
            "epoch: 1 step: 913, loss is 0.23585961759090424\n",
            "epoch: 1 step: 914, loss is 0.5124668478965759\n",
            "epoch: 1 step: 915, loss is 0.5390404462814331\n",
            "epoch: 1 step: 916, loss is 0.3469333052635193\n",
            "epoch: 1 step: 917, loss is 0.4265874922275543\n",
            "epoch: 1 step: 918, loss is 0.39904239773750305\n",
            "epoch: 1 step: 919, loss is 0.5065922737121582\n",
            "epoch: 1 step: 920, loss is 0.38601431250572205\n",
            "epoch: 1 step: 921, loss is 0.31890320777893066\n",
            "epoch: 1 step: 922, loss is 0.39993709325790405\n",
            "epoch: 1 step: 923, loss is 0.3615364134311676\n",
            "epoch: 1 step: 924, loss is 0.30476921796798706\n",
            "epoch: 1 step: 925, loss is 0.2954820692539215\n",
            "epoch: 1 step: 926, loss is 0.3529413044452667\n",
            "epoch: 1 step: 927, loss is 0.39197590947151184\n",
            "epoch: 1 step: 928, loss is 0.31916162371635437\n",
            "epoch: 1 step: 929, loss is 0.4108462333679199\n",
            "epoch: 1 step: 930, loss is 0.4912409484386444\n",
            "epoch: 1 step: 931, loss is 0.4007861316204071\n",
            "epoch: 1 step: 932, loss is 0.2645713686943054\n",
            "epoch: 1 step: 933, loss is 0.2556861639022827\n",
            "epoch: 1 step: 934, loss is 0.488768994808197\n",
            "epoch: 1 step: 935, loss is 0.6384485960006714\n",
            "epoch: 1 step: 936, loss is 0.2644941210746765\n",
            "epoch: 1 step: 937, loss is 0.4151230752468109\n",
            "epoch: 1 step: 938, loss is 0.24980299174785614\n",
            "epoch: 1 step: 939, loss is 0.28839585185050964\n",
            "epoch: 1 step: 940, loss is 0.3804357945919037\n",
            "epoch: 1 step: 941, loss is 0.475246787071228\n",
            "epoch: 1 step: 942, loss is 0.39401257038116455\n",
            "epoch: 1 step: 943, loss is 0.21642504632472992\n",
            "epoch: 1 step: 944, loss is 0.3672694265842438\n",
            "epoch: 1 step: 945, loss is 0.4266153872013092\n",
            "epoch: 1 step: 946, loss is 0.424081414937973\n",
            "epoch: 1 step: 947, loss is 0.4776581823825836\n",
            "epoch: 1 step: 948, loss is 0.3922921419143677\n",
            "epoch: 1 step: 949, loss is 0.312063068151474\n",
            "epoch: 1 step: 950, loss is 0.37111738324165344\n",
            "epoch: 1 step: 951, loss is 0.319494366645813\n",
            "epoch: 1 step: 952, loss is 0.31566062569618225\n",
            "epoch: 1 step: 953, loss is 0.35522350668907166\n",
            "epoch: 1 step: 954, loss is 0.4309787452220917\n",
            "epoch: 1 step: 955, loss is 0.3536294400691986\n",
            "epoch: 1 step: 956, loss is 0.44988343119621277\n",
            "epoch: 1 step: 957, loss is 0.4135811924934387\n",
            "epoch: 1 step: 958, loss is 0.3517722189426422\n",
            "epoch: 1 step: 959, loss is 0.3612028956413269\n",
            "epoch: 1 step: 960, loss is 0.364707350730896\n",
            "epoch: 1 step: 961, loss is 0.2745828330516815\n",
            "epoch: 1 step: 962, loss is 0.21439044177532196\n",
            "epoch: 1 step: 963, loss is 0.34693819284439087\n",
            "epoch: 1 step: 964, loss is 0.23538941144943237\n",
            "epoch: 1 step: 965, loss is 0.29550400376319885\n",
            "epoch: 1 step: 966, loss is 0.3124580979347229\n",
            "epoch: 1 step: 967, loss is 0.4751761853694916\n",
            "epoch: 1 step: 968, loss is 0.21096552908420563\n",
            "epoch: 1 step: 969, loss is 0.4384464621543884\n",
            "epoch: 1 step: 970, loss is 0.40261027216911316\n",
            "epoch: 1 step: 971, loss is 0.3278907239437103\n",
            "epoch: 1 step: 972, loss is 0.27317196130752563\n",
            "epoch: 1 step: 973, loss is 0.2720751166343689\n",
            "epoch: 1 step: 974, loss is 0.2247605174779892\n",
            "epoch: 1 step: 975, loss is 0.30111122131347656\n",
            "epoch: 1 step: 976, loss is 0.3023805320262909\n",
            "epoch: 1 step: 977, loss is 0.2779028117656708\n",
            "epoch: 1 step: 978, loss is 0.26631149649620056\n",
            "epoch: 1 step: 979, loss is 0.3575804829597473\n",
            "epoch: 1 step: 980, loss is 0.5118642449378967\n",
            "epoch: 1 step: 981, loss is 0.2804306149482727\n",
            "epoch: 1 step: 982, loss is 0.34772709012031555\n",
            "epoch: 1 step: 983, loss is 0.2581220269203186\n",
            "epoch: 1 step: 984, loss is 0.20412372052669525\n",
            "epoch: 1 step: 985, loss is 0.2585279941558838\n",
            "epoch: 1 step: 986, loss is 0.26731443405151367\n",
            "epoch: 1 step: 987, loss is 0.4082445800304413\n",
            "epoch: 1 step: 988, loss is 0.3011452257633209\n",
            "epoch: 1 step: 989, loss is 0.3211298882961273\n",
            "epoch: 1 step: 990, loss is 0.29681482911109924\n",
            "epoch: 1 step: 991, loss is 0.46196165680885315\n",
            "epoch: 1 step: 992, loss is 0.2753462493419647\n",
            "epoch: 1 step: 993, loss is 0.4112781584262848\n",
            "epoch: 1 step: 994, loss is 0.29442137479782104\n",
            "epoch: 1 step: 995, loss is 0.23806780576705933\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WARNING] ME(1671:136248148721664,MainProcess):2025-04-19-14:19:22.230.000 [mindspore/train/callback/_early_stop.py:221] Early stopping is conditioned on accuracy, which is not available. Available choices are: {'loss'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 2 step: 1, loss is 0.39881014823913574\n",
            "epoch: 2 step: 2, loss is 0.19867396354675293\n",
            "epoch: 2 step: 3, loss is 0.5148831605911255\n",
            "epoch: 2 step: 4, loss is 0.2081117331981659\n",
            "epoch: 2 step: 5, loss is 0.19081637263298035\n",
            "epoch: 2 step: 6, loss is 0.143000528216362\n",
            "epoch: 2 step: 7, loss is 0.29270055890083313\n",
            "epoch: 2 step: 8, loss is 0.3898705840110779\n",
            "epoch: 2 step: 9, loss is 0.42192256450653076\n",
            "epoch: 2 step: 10, loss is 0.3258231580257416\n",
            "epoch: 2 step: 11, loss is 0.23715050518512726\n",
            "epoch: 2 step: 12, loss is 0.2735876142978668\n",
            "epoch: 2 step: 13, loss is 0.3026379942893982\n",
            "epoch: 2 step: 14, loss is 0.30240654945373535\n",
            "epoch: 2 step: 15, loss is 0.4172680974006653\n",
            "epoch: 2 step: 16, loss is 0.14388228952884674\n",
            "epoch: 2 step: 17, loss is 0.23629628121852875\n",
            "epoch: 2 step: 18, loss is 0.28162041306495667\n",
            "epoch: 2 step: 19, loss is 0.45047348737716675\n",
            "epoch: 2 step: 20, loss is 0.4209933876991272\n",
            "epoch: 2 step: 21, loss is 0.24537727236747742\n",
            "epoch: 2 step: 22, loss is 0.23495113849639893\n",
            "epoch: 2 step: 23, loss is 0.39751049876213074\n",
            "epoch: 2 step: 24, loss is 0.18676577508449554\n",
            "epoch: 2 step: 25, loss is 0.40455135703086853\n",
            "epoch: 2 step: 26, loss is 0.33731499314308167\n",
            "epoch: 2 step: 27, loss is 0.22493816912174225\n",
            "epoch: 2 step: 28, loss is 0.22590017318725586\n",
            "epoch: 2 step: 29, loss is 0.340370774269104\n",
            "epoch: 2 step: 30, loss is 0.24348820745944977\n",
            "epoch: 2 step: 31, loss is 0.29623186588287354\n",
            "epoch: 2 step: 32, loss is 0.39316892623901367\n",
            "epoch: 2 step: 33, loss is 0.2688457667827606\n",
            "epoch: 2 step: 34, loss is 0.28494346141815186\n",
            "epoch: 2 step: 35, loss is 0.42632392048835754\n",
            "epoch: 2 step: 36, loss is 0.3522428870201111\n",
            "epoch: 2 step: 37, loss is 0.2745773494243622\n",
            "epoch: 2 step: 38, loss is 0.15364226698875427\n",
            "epoch: 2 step: 39, loss is 0.19422733783721924\n",
            "epoch: 2 step: 40, loss is 0.24903789162635803\n",
            "epoch: 2 step: 41, loss is 0.20069976150989532\n",
            "epoch: 2 step: 42, loss is 0.46774807572364807\n",
            "epoch: 2 step: 43, loss is 0.313230961561203\n",
            "epoch: 2 step: 44, loss is 0.18465174734592438\n",
            "epoch: 2 step: 45, loss is 0.15328198671340942\n",
            "epoch: 2 step: 46, loss is 0.33719658851623535\n",
            "epoch: 2 step: 47, loss is 0.3624935746192932\n",
            "epoch: 2 step: 48, loss is 0.2687385380268097\n",
            "epoch: 2 step: 49, loss is 0.2282296121120453\n",
            "epoch: 2 step: 50, loss is 0.27943289279937744\n",
            "epoch: 2 step: 51, loss is 0.2964598536491394\n",
            "epoch: 2 step: 52, loss is 0.605393648147583\n",
            "epoch: 2 step: 53, loss is 0.24859872460365295\n",
            "epoch: 2 step: 54, loss is 0.20924422144889832\n",
            "epoch: 2 step: 55, loss is 0.26477500796318054\n",
            "epoch: 2 step: 56, loss is 0.5369899868965149\n",
            "epoch: 2 step: 57, loss is 0.35495132207870483\n",
            "epoch: 2 step: 58, loss is 0.2895298898220062\n",
            "epoch: 2 step: 59, loss is 0.22474193572998047\n",
            "epoch: 2 step: 60, loss is 0.29903483390808105\n",
            "epoch: 2 step: 61, loss is 0.2784789502620697\n",
            "epoch: 2 step: 62, loss is 0.4831199645996094\n",
            "epoch: 2 step: 63, loss is 0.2909058630466461\n",
            "epoch: 2 step: 64, loss is 0.2073480635881424\n",
            "epoch: 2 step: 65, loss is 0.21200017631053925\n",
            "epoch: 2 step: 66, loss is 0.28891390562057495\n",
            "epoch: 2 step: 67, loss is 0.21681897342205048\n",
            "epoch: 2 step: 68, loss is 0.2686960697174072\n",
            "epoch: 2 step: 69, loss is 0.46836137771606445\n",
            "epoch: 2 step: 70, loss is 0.21455800533294678\n",
            "epoch: 2 step: 71, loss is 0.47850918769836426\n",
            "epoch: 2 step: 72, loss is 0.29982152581214905\n",
            "epoch: 2 step: 73, loss is 0.4703293740749359\n",
            "epoch: 2 step: 74, loss is 0.4477692246437073\n",
            "epoch: 2 step: 75, loss is 0.2955363392829895\n",
            "epoch: 2 step: 76, loss is 0.3507433831691742\n",
            "epoch: 2 step: 77, loss is 0.24547573924064636\n",
            "epoch: 2 step: 78, loss is 0.1924155056476593\n",
            "epoch: 2 step: 79, loss is 0.23362508416175842\n",
            "epoch: 2 step: 80, loss is 0.3614363968372345\n",
            "epoch: 2 step: 81, loss is 0.30121421813964844\n",
            "epoch: 2 step: 82, loss is 0.31088685989379883\n",
            "epoch: 2 step: 83, loss is 0.32177668809890747\n",
            "epoch: 2 step: 84, loss is 0.21424676477909088\n",
            "epoch: 2 step: 85, loss is 0.17872516810894012\n",
            "epoch: 2 step: 86, loss is 0.31797894835472107\n",
            "epoch: 2 step: 87, loss is 0.26988446712493896\n",
            "epoch: 2 step: 88, loss is 0.339648574590683\n",
            "epoch: 2 step: 89, loss is 0.42437267303466797\n",
            "epoch: 2 step: 90, loss is 0.1243329718708992\n",
            "epoch: 2 step: 91, loss is 0.2455526441335678\n",
            "epoch: 2 step: 92, loss is 0.3064645826816559\n",
            "epoch: 2 step: 93, loss is 0.33109310269355774\n",
            "epoch: 2 step: 94, loss is 0.22536422312259674\n",
            "epoch: 2 step: 95, loss is 0.16005416214466095\n",
            "epoch: 2 step: 96, loss is 0.133906289935112\n",
            "epoch: 2 step: 97, loss is 0.18751193583011627\n",
            "epoch: 2 step: 98, loss is 0.42165449261665344\n",
            "epoch: 2 step: 99, loss is 0.29840606451034546\n",
            "epoch: 2 step: 100, loss is 0.23666493594646454\n",
            "epoch: 2 step: 101, loss is 0.313131719827652\n",
            "epoch: 2 step: 102, loss is 0.29122355580329895\n",
            "epoch: 2 step: 103, loss is 0.2153674215078354\n",
            "epoch: 2 step: 104, loss is 0.13072443008422852\n",
            "epoch: 2 step: 105, loss is 0.22734512388706207\n",
            "epoch: 2 step: 106, loss is 0.2226344347000122\n",
            "epoch: 2 step: 107, loss is 0.4102989435195923\n",
            "epoch: 2 step: 108, loss is 0.22815975546836853\n",
            "epoch: 2 step: 109, loss is 0.11521795392036438\n",
            "epoch: 2 step: 110, loss is 0.3709770739078522\n",
            "epoch: 2 step: 111, loss is 0.2996177077293396\n",
            "epoch: 2 step: 112, loss is 0.20868320763111115\n",
            "epoch: 2 step: 113, loss is 0.2142227292060852\n",
            "epoch: 2 step: 114, loss is 0.0921284630894661\n",
            "epoch: 2 step: 115, loss is 0.15490302443504333\n",
            "epoch: 2 step: 116, loss is 0.26737064123153687\n",
            "epoch: 2 step: 117, loss is 0.19896073639392853\n",
            "epoch: 2 step: 118, loss is 0.35543087124824524\n",
            "epoch: 2 step: 119, loss is 0.27615147829055786\n",
            "epoch: 2 step: 120, loss is 0.1508643925189972\n",
            "epoch: 2 step: 121, loss is 0.1554347574710846\n",
            "epoch: 2 step: 122, loss is 0.14310593903064728\n",
            "epoch: 2 step: 123, loss is 0.26624801754951477\n",
            "epoch: 2 step: 124, loss is 0.3306579291820526\n",
            "epoch: 2 step: 125, loss is 0.2685791254043579\n",
            "epoch: 2 step: 126, loss is 0.3917350172996521\n",
            "epoch: 2 step: 127, loss is 0.39914146065711975\n",
            "epoch: 2 step: 128, loss is 0.2116355001926422\n",
            "epoch: 2 step: 129, loss is 0.25297412276268005\n",
            "epoch: 2 step: 130, loss is 0.2826695144176483\n",
            "epoch: 2 step: 131, loss is 0.302398145198822\n",
            "epoch: 2 step: 132, loss is 0.15422336757183075\n",
            "epoch: 2 step: 133, loss is 0.16153974831104279\n",
            "epoch: 2 step: 134, loss is 0.3114432096481323\n",
            "epoch: 2 step: 135, loss is 0.4772573411464691\n",
            "epoch: 2 step: 136, loss is 0.1962825208902359\n",
            "epoch: 2 step: 137, loss is 0.23578843474388123\n",
            "epoch: 2 step: 138, loss is 0.15460248291492462\n",
            "epoch: 2 step: 139, loss is 0.17224742472171783\n",
            "epoch: 2 step: 140, loss is 0.21787257492542267\n",
            "epoch: 2 step: 141, loss is 0.36032265424728394\n",
            "epoch: 2 step: 142, loss is 0.3446298837661743\n",
            "epoch: 2 step: 143, loss is 0.3391309678554535\n",
            "epoch: 2 step: 144, loss is 0.42966097593307495\n",
            "epoch: 2 step: 145, loss is 0.27258697152137756\n",
            "epoch: 2 step: 146, loss is 0.26594626903533936\n",
            "epoch: 2 step: 147, loss is 0.2295704483985901\n",
            "epoch: 2 step: 148, loss is 0.3508836030960083\n",
            "epoch: 2 step: 149, loss is 0.2586432099342346\n",
            "epoch: 2 step: 150, loss is 0.28493183851242065\n",
            "epoch: 2 step: 151, loss is 0.19133152067661285\n",
            "epoch: 2 step: 152, loss is 0.20633935928344727\n",
            "epoch: 2 step: 153, loss is 0.20040178298950195\n",
            "epoch: 2 step: 154, loss is 0.5304891467094421\n",
            "epoch: 2 step: 155, loss is 0.12680824100971222\n",
            "epoch: 2 step: 156, loss is 0.3851804733276367\n",
            "epoch: 2 step: 157, loss is 0.3956514596939087\n",
            "epoch: 2 step: 158, loss is 0.2628406584262848\n",
            "epoch: 2 step: 159, loss is 0.23558415472507477\n",
            "epoch: 2 step: 160, loss is 0.24262635409832\n",
            "epoch: 2 step: 161, loss is 0.31679674983024597\n",
            "epoch: 2 step: 162, loss is 0.4134155809879303\n",
            "epoch: 2 step: 163, loss is 0.27974197268486023\n",
            "epoch: 2 step: 164, loss is 0.2819284200668335\n",
            "epoch: 2 step: 165, loss is 0.28504541516304016\n",
            "epoch: 2 step: 166, loss is 0.15839757025241852\n",
            "epoch: 2 step: 167, loss is 0.26715436577796936\n",
            "epoch: 2 step: 168, loss is 0.33521872758865356\n",
            "epoch: 2 step: 169, loss is 0.29008156061172485\n",
            "epoch: 2 step: 170, loss is 0.16511014103889465\n",
            "epoch: 2 step: 171, loss is 0.2010488212108612\n",
            "epoch: 2 step: 172, loss is 0.11309074610471725\n",
            "epoch: 2 step: 173, loss is 0.1933693140745163\n",
            "epoch: 2 step: 174, loss is 0.20299850404262543\n",
            "epoch: 2 step: 175, loss is 0.18644751608371735\n",
            "epoch: 2 step: 176, loss is 0.19695109128952026\n",
            "epoch: 2 step: 177, loss is 0.27118292450904846\n",
            "epoch: 2 step: 178, loss is 0.25602445006370544\n",
            "epoch: 2 step: 179, loss is 0.34306302666664124\n",
            "epoch: 2 step: 180, loss is 0.09936422854661942\n",
            "epoch: 2 step: 181, loss is 0.1664869338274002\n",
            "epoch: 2 step: 182, loss is 0.2565789818763733\n",
            "epoch: 2 step: 183, loss is 0.19393983483314514\n",
            "epoch: 2 step: 184, loss is 0.26744040846824646\n",
            "epoch: 2 step: 185, loss is 0.24064604938030243\n",
            "epoch: 2 step: 186, loss is 0.21997515857219696\n",
            "epoch: 2 step: 187, loss is 0.22904755175113678\n",
            "epoch: 2 step: 188, loss is 0.3101925551891327\n",
            "epoch: 2 step: 189, loss is 0.1895773857831955\n",
            "epoch: 2 step: 190, loss is 0.1791100651025772\n",
            "epoch: 2 step: 191, loss is 0.24236653745174408\n",
            "epoch: 2 step: 192, loss is 0.29575109481811523\n",
            "epoch: 2 step: 193, loss is 0.30600646138191223\n",
            "epoch: 2 step: 194, loss is 0.6022182106971741\n",
            "epoch: 2 step: 195, loss is 0.17495855689048767\n",
            "epoch: 2 step: 196, loss is 0.23647668957710266\n",
            "epoch: 2 step: 197, loss is 0.153301402926445\n",
            "epoch: 2 step: 198, loss is 0.2089235782623291\n",
            "epoch: 2 step: 199, loss is 0.15064142644405365\n",
            "epoch: 2 step: 200, loss is 0.19625167548656464\n",
            "epoch: 2 step: 201, loss is 0.37884750962257385\n",
            "epoch: 2 step: 202, loss is 0.31196489930152893\n",
            "epoch: 2 step: 203, loss is 0.32396623492240906\n",
            "epoch: 2 step: 204, loss is 0.22641009092330933\n",
            "epoch: 2 step: 205, loss is 0.14756116271018982\n",
            "epoch: 2 step: 206, loss is 0.1818598210811615\n",
            "epoch: 2 step: 207, loss is 0.1809687316417694\n",
            "epoch: 2 step: 208, loss is 0.2298469841480255\n",
            "epoch: 2 step: 209, loss is 0.29671475291252136\n",
            "epoch: 2 step: 210, loss is 0.2537097632884979\n",
            "epoch: 2 step: 211, loss is 0.3585816025733948\n",
            "epoch: 2 step: 212, loss is 0.18176759779453278\n",
            "epoch: 2 step: 213, loss is 0.2685416042804718\n",
            "epoch: 2 step: 214, loss is 0.2696031928062439\n",
            "epoch: 2 step: 215, loss is 0.18254205584526062\n",
            "epoch: 2 step: 216, loss is 0.2505321204662323\n",
            "epoch: 2 step: 217, loss is 0.2056458741426468\n",
            "epoch: 2 step: 218, loss is 0.24986661970615387\n",
            "epoch: 2 step: 219, loss is 0.5454931259155273\n",
            "epoch: 2 step: 220, loss is 0.31690672039985657\n",
            "epoch: 2 step: 221, loss is 0.1608293056488037\n",
            "epoch: 2 step: 222, loss is 0.33379867672920227\n",
            "epoch: 2 step: 223, loss is 0.27924469113349915\n",
            "epoch: 2 step: 224, loss is 0.12933629751205444\n",
            "epoch: 2 step: 225, loss is 0.2624141573905945\n",
            "epoch: 2 step: 226, loss is 0.24458232522010803\n",
            "epoch: 2 step: 227, loss is 0.24948127567768097\n",
            "epoch: 2 step: 228, loss is 0.23242661356925964\n",
            "epoch: 2 step: 229, loss is 0.20283392071723938\n",
            "epoch: 2 step: 230, loss is 0.25616440176963806\n",
            "epoch: 2 step: 231, loss is 0.36387839913368225\n",
            "epoch: 2 step: 232, loss is 0.22384153306484222\n",
            "epoch: 2 step: 233, loss is 0.36270198225975037\n",
            "epoch: 2 step: 234, loss is 0.17242753505706787\n",
            "epoch: 2 step: 235, loss is 0.1806400716304779\n",
            "epoch: 2 step: 236, loss is 0.16538240015506744\n",
            "epoch: 2 step: 237, loss is 0.10057640075683594\n",
            "epoch: 2 step: 238, loss is 0.15265634655952454\n",
            "epoch: 2 step: 239, loss is 0.33850809931755066\n",
            "epoch: 2 step: 240, loss is 0.17681753635406494\n",
            "epoch: 2 step: 241, loss is 0.327085018157959\n",
            "epoch: 2 step: 242, loss is 0.11867314577102661\n",
            "epoch: 2 step: 243, loss is 0.17343784868717194\n",
            "epoch: 2 step: 244, loss is 0.15282998979091644\n",
            "epoch: 2 step: 245, loss is 0.11538362503051758\n",
            "epoch: 2 step: 246, loss is 0.1650804579257965\n",
            "epoch: 2 step: 247, loss is 0.19821110367774963\n",
            "epoch: 2 step: 248, loss is 0.19358524680137634\n",
            "epoch: 2 step: 249, loss is 0.17954163253307343\n",
            "epoch: 2 step: 250, loss is 0.10595766454935074\n",
            "epoch: 2 step: 251, loss is 0.23064152896404266\n",
            "epoch: 2 step: 252, loss is 0.1765023171901703\n",
            "epoch: 2 step: 253, loss is 0.1439259797334671\n",
            "epoch: 2 step: 254, loss is 0.27911004424095154\n",
            "epoch: 2 step: 255, loss is 0.16558338701725006\n",
            "epoch: 2 step: 256, loss is 0.24611473083496094\n",
            "epoch: 2 step: 257, loss is 0.3398171365261078\n",
            "epoch: 2 step: 258, loss is 0.20574404299259186\n",
            "epoch: 2 step: 259, loss is 0.2221132218837738\n",
            "epoch: 2 step: 260, loss is 0.572704553604126\n",
            "epoch: 2 step: 261, loss is 0.371419757604599\n",
            "epoch: 2 step: 262, loss is 0.12308290600776672\n",
            "epoch: 2 step: 263, loss is 0.20175398886203766\n",
            "epoch: 2 step: 264, loss is 0.35398727655410767\n",
            "epoch: 2 step: 265, loss is 0.4455939829349518\n",
            "epoch: 2 step: 266, loss is 0.437738835811615\n",
            "epoch: 2 step: 267, loss is 0.23533247411251068\n",
            "epoch: 2 step: 268, loss is 0.14707961678504944\n",
            "epoch: 2 step: 269, loss is 0.24737675487995148\n",
            "epoch: 2 step: 270, loss is 0.18898224830627441\n",
            "epoch: 2 step: 271, loss is 0.19182035326957703\n",
            "epoch: 2 step: 272, loss is 0.19813129305839539\n",
            "epoch: 2 step: 273, loss is 0.26407480239868164\n",
            "epoch: 2 step: 274, loss is 0.3448210060596466\n",
            "epoch: 2 step: 275, loss is 0.08811961114406586\n",
            "epoch: 2 step: 276, loss is 0.1259235292673111\n",
            "epoch: 2 step: 277, loss is 0.27619513869285583\n",
            "epoch: 2 step: 278, loss is 0.1277724951505661\n",
            "epoch: 2 step: 279, loss is 0.13662606477737427\n",
            "epoch: 2 step: 280, loss is 0.13564518094062805\n",
            "epoch: 2 step: 281, loss is 0.2025110274553299\n",
            "epoch: 2 step: 282, loss is 0.16602188348770142\n",
            "epoch: 2 step: 283, loss is 0.1105351373553276\n",
            "epoch: 2 step: 284, loss is 0.09549280256032944\n",
            "epoch: 2 step: 285, loss is 0.20963738858699799\n",
            "epoch: 2 step: 286, loss is 0.2826336622238159\n",
            "epoch: 2 step: 287, loss is 0.09538386762142181\n",
            "epoch: 2 step: 288, loss is 0.1327381432056427\n",
            "epoch: 2 step: 289, loss is 0.07847674190998077\n",
            "epoch: 2 step: 290, loss is 0.10632279515266418\n",
            "epoch: 2 step: 291, loss is 0.31765955686569214\n",
            "epoch: 2 step: 292, loss is 0.2808710038661957\n",
            "epoch: 2 step: 293, loss is 0.11004108935594559\n",
            "epoch: 2 step: 294, loss is 0.16069796681404114\n",
            "epoch: 2 step: 295, loss is 0.41417813301086426\n",
            "epoch: 2 step: 296, loss is 0.17510221898555756\n",
            "epoch: 2 step: 297, loss is 0.27630433440208435\n",
            "epoch: 2 step: 298, loss is 0.1273498833179474\n",
            "epoch: 2 step: 299, loss is 0.20703677833080292\n",
            "epoch: 2 step: 300, loss is 0.21766963601112366\n",
            "epoch: 2 step: 301, loss is 0.2113855928182602\n",
            "epoch: 2 step: 302, loss is 0.20709282159805298\n",
            "epoch: 2 step: 303, loss is 0.1816827803850174\n",
            "epoch: 2 step: 304, loss is 0.1450965702533722\n",
            "epoch: 2 step: 305, loss is 0.11090284585952759\n",
            "epoch: 2 step: 306, loss is 0.10283177345991135\n",
            "epoch: 2 step: 307, loss is 0.0988755151629448\n",
            "epoch: 2 step: 308, loss is 0.32014310359954834\n",
            "epoch: 2 step: 309, loss is 0.16735272109508514\n",
            "epoch: 2 step: 310, loss is 0.1393963098526001\n",
            "epoch: 2 step: 311, loss is 0.34642520546913147\n",
            "epoch: 2 step: 312, loss is 0.08675571531057358\n",
            "epoch: 2 step: 313, loss is 0.15669310092926025\n",
            "epoch: 2 step: 314, loss is 0.2509818375110626\n",
            "epoch: 2 step: 315, loss is 0.6398345232009888\n",
            "epoch: 2 step: 316, loss is 0.13996024429798126\n",
            "epoch: 2 step: 317, loss is 0.23460325598716736\n",
            "epoch: 2 step: 318, loss is 0.13604165613651276\n",
            "epoch: 2 step: 319, loss is 0.27459052205085754\n",
            "epoch: 2 step: 320, loss is 0.08925359696149826\n",
            "epoch: 2 step: 321, loss is 0.24626988172531128\n",
            "epoch: 2 step: 322, loss is 0.134963721036911\n",
            "epoch: 2 step: 323, loss is 0.1253490447998047\n",
            "epoch: 2 step: 324, loss is 0.07581634819507599\n",
            "epoch: 2 step: 325, loss is 0.13557790219783783\n",
            "epoch: 2 step: 326, loss is 0.2605917453765869\n",
            "epoch: 2 step: 327, loss is 0.17649994790554047\n",
            "epoch: 2 step: 328, loss is 0.12519434094429016\n",
            "epoch: 2 step: 329, loss is 0.2864123284816742\n",
            "epoch: 2 step: 330, loss is 0.22879374027252197\n",
            "epoch: 2 step: 331, loss is 0.22999155521392822\n",
            "epoch: 2 step: 332, loss is 0.15656545758247375\n",
            "epoch: 2 step: 333, loss is 0.21281450986862183\n",
            "epoch: 2 step: 334, loss is 0.1604599803686142\n",
            "epoch: 2 step: 335, loss is 0.37965822219848633\n",
            "epoch: 2 step: 336, loss is 0.17771150171756744\n",
            "epoch: 2 step: 337, loss is 0.17126643657684326\n",
            "epoch: 2 step: 338, loss is 0.21390371024608612\n",
            "epoch: 2 step: 339, loss is 0.17349305748939514\n",
            "epoch: 2 step: 340, loss is 0.3729681670665741\n",
            "epoch: 2 step: 341, loss is 0.32901227474212646\n",
            "epoch: 2 step: 342, loss is 0.23582981526851654\n",
            "epoch: 2 step: 343, loss is 0.22003912925720215\n",
            "epoch: 2 step: 344, loss is 0.17658543586730957\n",
            "epoch: 2 step: 345, loss is 0.1854802966117859\n",
            "epoch: 2 step: 346, loss is 0.22157558798789978\n",
            "epoch: 2 step: 347, loss is 0.18316109478473663\n",
            "epoch: 2 step: 348, loss is 0.09131214022636414\n",
            "epoch: 2 step: 349, loss is 0.07866229116916656\n",
            "epoch: 2 step: 350, loss is 0.12625710666179657\n",
            "epoch: 2 step: 351, loss is 0.1864469349384308\n",
            "epoch: 2 step: 352, loss is 0.16064754128456116\n",
            "epoch: 2 step: 353, loss is 0.15238775312900543\n",
            "epoch: 2 step: 354, loss is 0.15485860407352448\n",
            "epoch: 2 step: 355, loss is 0.09308208525180817\n",
            "epoch: 2 step: 356, loss is 0.24213284254074097\n",
            "epoch: 2 step: 357, loss is 0.23470664024353027\n",
            "epoch: 2 step: 358, loss is 0.16538678109645844\n",
            "epoch: 2 step: 359, loss is 0.1651492416858673\n",
            "epoch: 2 step: 360, loss is 0.29509201645851135\n",
            "epoch: 2 step: 361, loss is 0.37583306431770325\n",
            "epoch: 2 step: 362, loss is 0.2426576018333435\n",
            "epoch: 2 step: 363, loss is 0.08119038492441177\n",
            "epoch: 2 step: 364, loss is 0.12023276090621948\n",
            "epoch: 2 step: 365, loss is 0.1761348396539688\n",
            "epoch: 2 step: 366, loss is 0.16375389695167542\n",
            "epoch: 2 step: 367, loss is 0.11296030133962631\n",
            "epoch: 2 step: 368, loss is 0.18636466562747955\n",
            "epoch: 2 step: 369, loss is 0.08495643734931946\n",
            "epoch: 2 step: 370, loss is 0.1072157472372055\n",
            "epoch: 2 step: 371, loss is 0.4366992712020874\n",
            "epoch: 2 step: 372, loss is 0.16209925711154938\n",
            "epoch: 2 step: 373, loss is 0.07275116443634033\n",
            "epoch: 2 step: 374, loss is 0.11569328606128693\n",
            "epoch: 2 step: 375, loss is 0.14745807647705078\n",
            "epoch: 2 step: 376, loss is 0.1091376543045044\n",
            "epoch: 2 step: 377, loss is 0.31908753514289856\n",
            "epoch: 2 step: 378, loss is 0.10190801322460175\n",
            "epoch: 2 step: 379, loss is 0.08386921137571335\n",
            "epoch: 2 step: 380, loss is 0.06701776385307312\n",
            "epoch: 2 step: 381, loss is 0.1289212554693222\n",
            "epoch: 2 step: 382, loss is 0.2557905614376068\n",
            "epoch: 2 step: 383, loss is 0.16360828280448914\n",
            "epoch: 2 step: 384, loss is 0.2978209853172302\n",
            "epoch: 2 step: 385, loss is 0.1701255589723587\n",
            "epoch: 2 step: 386, loss is 0.1645171195268631\n",
            "epoch: 2 step: 387, loss is 0.2463809698820114\n",
            "epoch: 2 step: 388, loss is 0.33130913972854614\n",
            "epoch: 2 step: 389, loss is 0.3961336612701416\n",
            "epoch: 2 step: 390, loss is 0.16482172906398773\n",
            "epoch: 2 step: 391, loss is 0.1659853756427765\n",
            "epoch: 2 step: 392, loss is 0.07023042440414429\n",
            "epoch: 2 step: 393, loss is 0.4055691063404083\n",
            "epoch: 2 step: 394, loss is 0.08696967363357544\n",
            "epoch: 2 step: 395, loss is 0.172600656747818\n",
            "epoch: 2 step: 396, loss is 0.15277540683746338\n",
            "epoch: 2 step: 397, loss is 0.238840252161026\n",
            "epoch: 2 step: 398, loss is 0.160715252161026\n",
            "epoch: 2 step: 399, loss is 0.5590803623199463\n",
            "epoch: 2 step: 400, loss is 0.1416829526424408\n",
            "epoch: 2 step: 401, loss is 0.07171764969825745\n",
            "epoch: 2 step: 402, loss is 0.09995254874229431\n",
            "epoch: 2 step: 403, loss is 0.3756939172744751\n",
            "epoch: 2 step: 404, loss is 0.2770974636077881\n",
            "epoch: 2 step: 405, loss is 0.29954633116722107\n",
            "epoch: 2 step: 406, loss is 0.34787845611572266\n",
            "epoch: 2 step: 407, loss is 0.11585420370101929\n",
            "epoch: 2 step: 408, loss is 0.3147514760494232\n",
            "epoch: 2 step: 409, loss is 0.09203561395406723\n",
            "epoch: 2 step: 410, loss is 0.5007606744766235\n",
            "epoch: 2 step: 411, loss is 0.1703803986310959\n",
            "epoch: 2 step: 412, loss is 0.07668189704418182\n",
            "epoch: 2 step: 413, loss is 0.45211535692214966\n",
            "epoch: 2 step: 414, loss is 0.15575554966926575\n",
            "epoch: 2 step: 415, loss is 0.23388534784317017\n",
            "epoch: 2 step: 416, loss is 0.15585222840309143\n",
            "epoch: 2 step: 417, loss is 0.2400115430355072\n",
            "epoch: 2 step: 418, loss is 0.4078739583492279\n",
            "epoch: 2 step: 419, loss is 0.2468441128730774\n",
            "epoch: 2 step: 420, loss is 0.15647053718566895\n",
            "epoch: 2 step: 421, loss is 0.40824392437934875\n",
            "epoch: 2 step: 422, loss is 0.04607342928647995\n",
            "epoch: 2 step: 423, loss is 0.20668818056583405\n",
            "epoch: 2 step: 424, loss is 0.22316387295722961\n",
            "epoch: 2 step: 425, loss is 0.11683148145675659\n",
            "epoch: 2 step: 426, loss is 0.12882161140441895\n",
            "epoch: 2 step: 427, loss is 0.1547403186559677\n",
            "epoch: 2 step: 428, loss is 0.17132075130939484\n",
            "epoch: 2 step: 429, loss is 0.21580618619918823\n",
            "epoch: 2 step: 430, loss is 0.12308932840824127\n",
            "epoch: 2 step: 431, loss is 0.1175774559378624\n",
            "epoch: 2 step: 432, loss is 0.19118164479732513\n",
            "epoch: 2 step: 433, loss is 0.1252356320619583\n",
            "epoch: 2 step: 434, loss is 0.12579374015331268\n",
            "epoch: 2 step: 435, loss is 0.10494096577167511\n",
            "epoch: 2 step: 436, loss is 0.1350593864917755\n",
            "epoch: 2 step: 437, loss is 0.1615757942199707\n",
            "epoch: 2 step: 438, loss is 0.15562006831169128\n",
            "epoch: 2 step: 439, loss is 0.07443225383758545\n",
            "epoch: 2 step: 440, loss is 0.1712847501039505\n",
            "epoch: 2 step: 441, loss is 0.194763645529747\n",
            "epoch: 2 step: 442, loss is 0.14397701621055603\n",
            "epoch: 2 step: 443, loss is 0.08660392463207245\n",
            "epoch: 2 step: 444, loss is 0.18373294174671173\n",
            "epoch: 2 step: 445, loss is 0.08748629689216614\n",
            "epoch: 2 step: 446, loss is 0.08647829294204712\n",
            "epoch: 2 step: 447, loss is 0.061301443725824356\n",
            "epoch: 2 step: 448, loss is 0.1632877141237259\n",
            "epoch: 2 step: 449, loss is 0.19293344020843506\n",
            "epoch: 2 step: 450, loss is 0.3251347541809082\n",
            "epoch: 2 step: 451, loss is 0.0679410770535469\n",
            "epoch: 2 step: 452, loss is 0.15792424976825714\n",
            "epoch: 2 step: 453, loss is 0.148386612534523\n",
            "epoch: 2 step: 454, loss is 0.3489006459712982\n",
            "epoch: 2 step: 455, loss is 0.09232094138860703\n",
            "epoch: 2 step: 456, loss is 0.35383641719818115\n",
            "epoch: 2 step: 457, loss is 0.042043037712574005\n",
            "epoch: 2 step: 458, loss is 0.24013939499855042\n",
            "epoch: 2 step: 459, loss is 0.1707850694656372\n",
            "epoch: 2 step: 460, loss is 0.20291568338871002\n",
            "epoch: 2 step: 461, loss is 0.09180966019630432\n",
            "epoch: 2 step: 462, loss is 0.14330320060253143\n",
            "epoch: 2 step: 463, loss is 0.2644295394420624\n",
            "epoch: 2 step: 464, loss is 0.07610777020454407\n",
            "epoch: 2 step: 465, loss is 0.12530553340911865\n",
            "epoch: 2 step: 466, loss is 0.05703231319785118\n",
            "epoch: 2 step: 467, loss is 0.14794284105300903\n",
            "epoch: 2 step: 468, loss is 0.08403133600950241\n",
            "epoch: 2 step: 469, loss is 0.10540129244327545\n",
            "epoch: 2 step: 470, loss is 0.2505258321762085\n",
            "epoch: 2 step: 471, loss is 0.1407574713230133\n",
            "epoch: 2 step: 472, loss is 0.24564532935619354\n",
            "epoch: 2 step: 473, loss is 0.21686819195747375\n",
            "epoch: 2 step: 474, loss is 0.17689107358455658\n",
            "epoch: 2 step: 475, loss is 0.1836262196302414\n",
            "epoch: 2 step: 476, loss is 0.25502026081085205\n",
            "epoch: 2 step: 477, loss is 0.06633280217647552\n",
            "epoch: 2 step: 478, loss is 0.1055242270231247\n",
            "epoch: 2 step: 479, loss is 0.13772107660770416\n",
            "epoch: 2 step: 480, loss is 0.14954246580600739\n",
            "epoch: 2 step: 481, loss is 0.04765211418271065\n",
            "epoch: 2 step: 482, loss is 0.14774592220783234\n",
            "epoch: 2 step: 483, loss is 0.14744071662425995\n",
            "epoch: 2 step: 484, loss is 0.1366914063692093\n",
            "epoch: 2 step: 485, loss is 0.22064921259880066\n",
            "epoch: 2 step: 486, loss is 0.1780986785888672\n",
            "epoch: 2 step: 487, loss is 0.20741429924964905\n",
            "epoch: 2 step: 488, loss is 0.11550965905189514\n",
            "epoch: 2 step: 489, loss is 0.44522538781166077\n",
            "epoch: 2 step: 490, loss is 0.17126353085041046\n",
            "epoch: 2 step: 491, loss is 0.2753601372241974\n",
            "epoch: 2 step: 492, loss is 0.17189829051494598\n",
            "epoch: 2 step: 493, loss is 0.23419183492660522\n",
            "epoch: 2 step: 494, loss is 0.31378087401390076\n",
            "epoch: 2 step: 495, loss is 0.20050205290317535\n",
            "epoch: 2 step: 496, loss is 0.07828164845705032\n",
            "epoch: 2 step: 497, loss is 0.23577360808849335\n",
            "epoch: 2 step: 498, loss is 0.1300647109746933\n",
            "epoch: 2 step: 499, loss is 0.10232757776975632\n",
            "epoch: 2 step: 500, loss is 0.20110850036144257\n",
            "epoch: 2 step: 501, loss is 0.051214154809713364\n",
            "epoch: 2 step: 502, loss is 0.06415627151727676\n",
            "epoch: 2 step: 503, loss is 0.13251261413097382\n",
            "epoch: 2 step: 504, loss is 0.19924503564834595\n",
            "epoch: 2 step: 505, loss is 0.13274408876895905\n",
            "epoch: 2 step: 506, loss is 0.24851523339748383\n",
            "epoch: 2 step: 507, loss is 0.045756757259368896\n",
            "epoch: 2 step: 508, loss is 0.10825416445732117\n",
            "epoch: 2 step: 509, loss is 0.2316828966140747\n",
            "epoch: 2 step: 510, loss is 0.3236044645309448\n",
            "epoch: 2 step: 511, loss is 0.2815781831741333\n",
            "epoch: 2 step: 512, loss is 0.06039856746792793\n",
            "epoch: 2 step: 513, loss is 0.24610064923763275\n",
            "epoch: 2 step: 514, loss is 0.09600431472063065\n",
            "epoch: 2 step: 515, loss is 0.294519305229187\n",
            "epoch: 2 step: 516, loss is 0.07110089063644409\n",
            "epoch: 2 step: 517, loss is 0.08231700956821442\n",
            "epoch: 2 step: 518, loss is 0.2395501434803009\n",
            "epoch: 2 step: 519, loss is 0.15725915133953094\n",
            "epoch: 2 step: 520, loss is 0.12181122601032257\n",
            "epoch: 2 step: 521, loss is 0.07749875634908676\n",
            "epoch: 2 step: 522, loss is 0.07180370390415192\n",
            "epoch: 2 step: 523, loss is 0.06829280406236649\n",
            "epoch: 2 step: 524, loss is 0.07448159158229828\n",
            "epoch: 2 step: 525, loss is 0.0940948948264122\n",
            "epoch: 2 step: 526, loss is 0.11506340652704239\n",
            "epoch: 2 step: 527, loss is 0.06861474364995956\n",
            "epoch: 2 step: 528, loss is 0.13152825832366943\n",
            "epoch: 2 step: 529, loss is 0.03973115235567093\n",
            "epoch: 2 step: 530, loss is 0.19919756054878235\n",
            "epoch: 2 step: 531, loss is 0.22459366917610168\n",
            "epoch: 2 step: 532, loss is 0.055669575929641724\n",
            "epoch: 2 step: 533, loss is 0.0938255712389946\n",
            "epoch: 2 step: 534, loss is 0.3072618246078491\n",
            "epoch: 2 step: 535, loss is 0.1825525015592575\n",
            "epoch: 2 step: 536, loss is 0.13203105330467224\n",
            "epoch: 2 step: 537, loss is 0.1803058385848999\n",
            "epoch: 2 step: 538, loss is 0.27869972586631775\n",
            "epoch: 2 step: 539, loss is 0.09072259068489075\n",
            "epoch: 2 step: 540, loss is 0.19030843675136566\n",
            "epoch: 2 step: 541, loss is 0.048958566039800644\n",
            "epoch: 2 step: 542, loss is 0.09854873269796371\n",
            "epoch: 2 step: 543, loss is 0.16049835085868835\n",
            "epoch: 2 step: 544, loss is 0.19504414498806\n",
            "epoch: 2 step: 545, loss is 0.28869739174842834\n",
            "epoch: 2 step: 546, loss is 0.1925564855337143\n",
            "epoch: 2 step: 547, loss is 0.2560630738735199\n",
            "epoch: 2 step: 548, loss is 0.3224005699157715\n",
            "epoch: 2 step: 549, loss is 0.24487046897411346\n",
            "epoch: 2 step: 550, loss is 0.12954190373420715\n",
            "epoch: 2 step: 551, loss is 0.6458650231361389\n",
            "epoch: 2 step: 552, loss is 0.10929389297962189\n",
            "epoch: 2 step: 553, loss is 0.32251375913619995\n",
            "epoch: 2 step: 554, loss is 0.41488319635391235\n",
            "epoch: 2 step: 555, loss is 0.6088306307792664\n",
            "epoch: 2 step: 556, loss is 0.2871340215206146\n",
            "epoch: 2 step: 557, loss is 0.4558321237564087\n",
            "epoch: 2 step: 558, loss is 0.14685311913490295\n",
            "epoch: 2 step: 559, loss is 0.07434656471014023\n",
            "epoch: 2 step: 560, loss is 0.24877873063087463\n",
            "epoch: 2 step: 561, loss is 0.07194538414478302\n",
            "epoch: 2 step: 562, loss is 0.33958297967910767\n",
            "epoch: 2 step: 563, loss is 0.06656822562217712\n",
            "epoch: 2 step: 564, loss is 0.27332305908203125\n",
            "epoch: 2 step: 565, loss is 0.15322306752204895\n",
            "epoch: 2 step: 566, loss is 0.17344890534877777\n",
            "epoch: 2 step: 567, loss is 0.11136370897293091\n",
            "epoch: 2 step: 568, loss is 0.11376425623893738\n",
            "epoch: 2 step: 569, loss is 0.22385162115097046\n",
            "epoch: 2 step: 570, loss is 0.043017610907554626\n",
            "epoch: 2 step: 571, loss is 0.1745998114347458\n",
            "epoch: 2 step: 572, loss is 0.19617144763469696\n",
            "epoch: 2 step: 573, loss is 0.04088687524199486\n",
            "epoch: 2 step: 574, loss is 0.28414344787597656\n",
            "epoch: 2 step: 575, loss is 0.14446908235549927\n",
            "epoch: 2 step: 576, loss is 0.04968622326850891\n",
            "epoch: 2 step: 577, loss is 0.1846832036972046\n",
            "epoch: 2 step: 578, loss is 0.32844042778015137\n",
            "epoch: 2 step: 579, loss is 0.4216257631778717\n",
            "epoch: 2 step: 580, loss is 0.13537836074829102\n",
            "epoch: 2 step: 581, loss is 0.22526825964450836\n",
            "epoch: 2 step: 582, loss is 0.05672762170433998\n",
            "epoch: 2 step: 583, loss is 0.07623068243265152\n",
            "epoch: 2 step: 584, loss is 0.05530587583780289\n",
            "epoch: 2 step: 585, loss is 0.046360522508621216\n",
            "epoch: 2 step: 586, loss is 0.18465262651443481\n",
            "epoch: 2 step: 587, loss is 0.15186157822608948\n",
            "epoch: 2 step: 588, loss is 0.04922368749976158\n",
            "epoch: 2 step: 589, loss is 0.08861745893955231\n",
            "epoch: 2 step: 590, loss is 0.09522337466478348\n",
            "epoch: 2 step: 591, loss is 0.4545477330684662\n",
            "epoch: 2 step: 592, loss is 0.09937628358602524\n",
            "epoch: 2 step: 593, loss is 0.1283101886510849\n",
            "epoch: 2 step: 594, loss is 0.15556757152080536\n",
            "epoch: 2 step: 595, loss is 0.1776007115840912\n",
            "epoch: 2 step: 596, loss is 0.07914990931749344\n",
            "epoch: 2 step: 597, loss is 0.10503289848566055\n",
            "epoch: 2 step: 598, loss is 0.15569785237312317\n",
            "epoch: 2 step: 599, loss is 0.04450000822544098\n",
            "epoch: 2 step: 600, loss is 0.15642976760864258\n",
            "epoch: 2 step: 601, loss is 0.35632094740867615\n",
            "epoch: 2 step: 602, loss is 0.08192413300275803\n",
            "epoch: 2 step: 603, loss is 0.11739157140254974\n",
            "epoch: 2 step: 604, loss is 0.13417592644691467\n",
            "epoch: 2 step: 605, loss is 0.1486918181180954\n",
            "epoch: 2 step: 606, loss is 0.18736086785793304\n",
            "epoch: 2 step: 607, loss is 0.06557408720254898\n",
            "epoch: 2 step: 608, loss is 0.15253470838069916\n",
            "epoch: 2 step: 609, loss is 0.12424419820308685\n",
            "epoch: 2 step: 610, loss is 0.32839420437812805\n",
            "epoch: 2 step: 611, loss is 0.13409747183322906\n",
            "epoch: 2 step: 612, loss is 0.06712717562913895\n",
            "epoch: 2 step: 613, loss is 0.2796824872493744\n",
            "epoch: 2 step: 614, loss is 0.07680918276309967\n",
            "epoch: 2 step: 615, loss is 0.10682704299688339\n",
            "epoch: 2 step: 616, loss is 0.0783877968788147\n",
            "epoch: 2 step: 617, loss is 0.05497905611991882\n",
            "epoch: 2 step: 618, loss is 0.05568881332874298\n",
            "epoch: 2 step: 619, loss is 0.1473531424999237\n",
            "epoch: 2 step: 620, loss is 0.07897631078958511\n",
            "epoch: 2 step: 621, loss is 0.22895094752311707\n",
            "epoch: 2 step: 622, loss is 0.3689814805984497\n",
            "epoch: 2 step: 623, loss is 0.15221351385116577\n",
            "epoch: 2 step: 624, loss is 0.03237030655145645\n",
            "epoch: 2 step: 625, loss is 0.1259782910346985\n",
            "epoch: 2 step: 626, loss is 0.17919236421585083\n",
            "epoch: 2 step: 627, loss is 0.08847705274820328\n",
            "epoch: 2 step: 628, loss is 0.17585307359695435\n",
            "epoch: 2 step: 629, loss is 0.1879757046699524\n",
            "epoch: 2 step: 630, loss is 0.08276055753231049\n",
            "epoch: 2 step: 631, loss is 0.1328534036874771\n",
            "epoch: 2 step: 632, loss is 0.14338408410549164\n",
            "epoch: 2 step: 633, loss is 0.036124054342508316\n",
            "epoch: 2 step: 634, loss is 0.23117980360984802\n",
            "epoch: 2 step: 635, loss is 0.16333481669425964\n",
            "epoch: 2 step: 636, loss is 0.12784390151500702\n",
            "epoch: 2 step: 637, loss is 0.05651956796646118\n",
            "epoch: 2 step: 638, loss is 0.15505439043045044\n",
            "epoch: 2 step: 639, loss is 0.22754010558128357\n",
            "epoch: 2 step: 640, loss is 0.10388776659965515\n",
            "epoch: 2 step: 641, loss is 0.17733649909496307\n",
            "epoch: 2 step: 642, loss is 0.24673256278038025\n",
            "epoch: 2 step: 643, loss is 0.17850401997566223\n",
            "epoch: 2 step: 644, loss is 0.15806712210178375\n",
            "epoch: 2 step: 645, loss is 0.06222204118967056\n",
            "epoch: 2 step: 646, loss is 0.046296097338199615\n",
            "epoch: 2 step: 647, loss is 0.06998240947723389\n",
            "epoch: 2 step: 648, loss is 0.08686306327581406\n",
            "epoch: 2 step: 649, loss is 0.07470396906137466\n",
            "epoch: 2 step: 650, loss is 0.19503749907016754\n",
            "epoch: 2 step: 651, loss is 0.09678956121206284\n",
            "epoch: 2 step: 652, loss is 0.19750340282917023\n",
            "epoch: 2 step: 653, loss is 0.1172567829489708\n",
            "epoch: 2 step: 654, loss is 0.03479095920920372\n",
            "epoch: 2 step: 655, loss is 0.04940751567482948\n",
            "epoch: 2 step: 656, loss is 0.18012522161006927\n",
            "epoch: 2 step: 657, loss is 0.16394942998886108\n",
            "epoch: 2 step: 658, loss is 0.04057712107896805\n",
            "epoch: 2 step: 659, loss is 0.04378123581409454\n",
            "epoch: 2 step: 660, loss is 0.2825237214565277\n",
            "epoch: 2 step: 661, loss is 0.28491607308387756\n",
            "epoch: 2 step: 662, loss is 0.15502923727035522\n",
            "epoch: 2 step: 663, loss is 0.13838507235050201\n",
            "epoch: 2 step: 664, loss is 0.09048901498317719\n",
            "epoch: 2 step: 665, loss is 0.08744530379772186\n",
            "epoch: 2 step: 666, loss is 0.02104809693992138\n",
            "epoch: 2 step: 667, loss is 0.046322185546159744\n",
            "epoch: 2 step: 668, loss is 0.2100348025560379\n",
            "epoch: 2 step: 669, loss is 0.16962328553199768\n",
            "epoch: 2 step: 670, loss is 0.1669759452342987\n",
            "epoch: 2 step: 671, loss is 0.28021153807640076\n",
            "epoch: 2 step: 672, loss is 0.10303261876106262\n",
            "epoch: 2 step: 673, loss is 0.06371650844812393\n",
            "epoch: 2 step: 674, loss is 0.12953568994998932\n",
            "epoch: 2 step: 675, loss is 0.04754554480314255\n",
            "epoch: 2 step: 676, loss is 0.18593484163284302\n",
            "epoch: 2 step: 677, loss is 0.05908817797899246\n",
            "epoch: 2 step: 678, loss is 0.07747367769479752\n",
            "epoch: 2 step: 679, loss is 0.13622325658798218\n",
            "epoch: 2 step: 680, loss is 0.1748628169298172\n",
            "epoch: 2 step: 681, loss is 0.12865541875362396\n",
            "epoch: 2 step: 682, loss is 0.07464675605297089\n",
            "epoch: 2 step: 683, loss is 0.11186647415161133\n",
            "epoch: 2 step: 684, loss is 0.04964306950569153\n",
            "epoch: 2 step: 685, loss is 0.06279729306697845\n",
            "epoch: 2 step: 686, loss is 0.04686679318547249\n",
            "epoch: 2 step: 687, loss is 0.17594659328460693\n",
            "epoch: 2 step: 688, loss is 0.05222925916314125\n",
            "epoch: 2 step: 689, loss is 0.040020715445280075\n",
            "epoch: 2 step: 690, loss is 0.11802380532026291\n",
            "epoch: 2 step: 691, loss is 0.3020598590373993\n",
            "epoch: 2 step: 692, loss is 0.06129193305969238\n",
            "epoch: 2 step: 693, loss is 0.039841558784246445\n",
            "epoch: 2 step: 694, loss is 0.29947128891944885\n",
            "epoch: 2 step: 695, loss is 0.17654289305210114\n",
            "epoch: 2 step: 696, loss is 0.061696942895650864\n",
            "epoch: 2 step: 697, loss is 0.24094213545322418\n",
            "epoch: 2 step: 698, loss is 0.22736969590187073\n",
            "epoch: 2 step: 699, loss is 0.14611633121967316\n",
            "epoch: 2 step: 700, loss is 0.14342881739139557\n",
            "epoch: 2 step: 701, loss is 0.25458452105522156\n",
            "epoch: 2 step: 702, loss is 0.22192834317684174\n",
            "epoch: 2 step: 703, loss is 0.04200514033436775\n",
            "epoch: 2 step: 704, loss is 0.058686017990112305\n",
            "epoch: 2 step: 705, loss is 0.05894387885928154\n",
            "epoch: 2 step: 706, loss is 0.09468141198158264\n",
            "epoch: 2 step: 707, loss is 0.026173848658800125\n",
            "epoch: 2 step: 708, loss is 0.07874429225921631\n",
            "epoch: 2 step: 709, loss is 0.09419447183609009\n",
            "epoch: 2 step: 710, loss is 0.09075764566659927\n",
            "epoch: 2 step: 711, loss is 0.2328091710805893\n",
            "epoch: 2 step: 712, loss is 0.1665465533733368\n",
            "epoch: 2 step: 713, loss is 0.30968669056892395\n",
            "epoch: 2 step: 714, loss is 0.11699394881725311\n",
            "epoch: 2 step: 715, loss is 0.21359030902385712\n",
            "epoch: 2 step: 716, loss is 0.31575965881347656\n",
            "epoch: 2 step: 717, loss is 0.06376319378614426\n",
            "epoch: 2 step: 718, loss is 0.1225651204586029\n",
            "epoch: 2 step: 719, loss is 0.12474606931209564\n",
            "epoch: 2 step: 720, loss is 0.06759969145059586\n",
            "epoch: 2 step: 721, loss is 0.15923449397087097\n",
            "epoch: 2 step: 722, loss is 0.1450665295124054\n",
            "epoch: 2 step: 723, loss is 0.07372118532657623\n",
            "epoch: 2 step: 724, loss is 0.03673148155212402\n",
            "epoch: 2 step: 725, loss is 0.030918927863240242\n",
            "epoch: 2 step: 726, loss is 0.11148835718631744\n",
            "epoch: 2 step: 727, loss is 0.1778961569070816\n",
            "epoch: 2 step: 728, loss is 0.06003910303115845\n",
            "epoch: 2 step: 729, loss is 0.18505072593688965\n",
            "epoch: 2 step: 730, loss is 0.03581959754228592\n",
            "epoch: 2 step: 731, loss is 0.04161752015352249\n",
            "epoch: 2 step: 732, loss is 0.09028169512748718\n",
            "epoch: 2 step: 733, loss is 0.1897537261247635\n",
            "epoch: 2 step: 734, loss is 0.09887605160474777\n",
            "epoch: 2 step: 735, loss is 0.11851947754621506\n",
            "epoch: 2 step: 736, loss is 0.19018419086933136\n",
            "epoch: 2 step: 737, loss is 0.13045649230480194\n",
            "epoch: 2 step: 738, loss is 0.14841222763061523\n",
            "epoch: 2 step: 739, loss is 0.09140951931476593\n",
            "epoch: 2 step: 740, loss is 0.19460412859916687\n",
            "epoch: 2 step: 741, loss is 0.04756290838122368\n",
            "epoch: 2 step: 742, loss is 0.04671737551689148\n",
            "epoch: 2 step: 743, loss is 0.22870326042175293\n",
            "epoch: 2 step: 744, loss is 0.12895146012306213\n",
            "epoch: 2 step: 745, loss is 0.06410441547632217\n",
            "epoch: 2 step: 746, loss is 0.10662660002708435\n",
            "epoch: 2 step: 747, loss is 0.1252250075340271\n",
            "epoch: 2 step: 748, loss is 0.10613008588552475\n",
            "epoch: 2 step: 749, loss is 0.16647708415985107\n",
            "epoch: 2 step: 750, loss is 0.05345866456627846\n",
            "epoch: 2 step: 751, loss is 0.18039318919181824\n",
            "epoch: 2 step: 752, loss is 0.06529799103736877\n",
            "epoch: 2 step: 753, loss is 0.10955860465765\n",
            "epoch: 2 step: 754, loss is 0.31823235750198364\n",
            "epoch: 2 step: 755, loss is 0.2982901632785797\n",
            "epoch: 2 step: 756, loss is 0.03408289700746536\n",
            "epoch: 2 step: 757, loss is 0.074408620595932\n",
            "epoch: 2 step: 758, loss is 0.11148864030838013\n",
            "epoch: 2 step: 759, loss is 0.09553955495357513\n",
            "epoch: 2 step: 760, loss is 0.32122066617012024\n",
            "epoch: 2 step: 761, loss is 0.10291096568107605\n",
            "epoch: 2 step: 762, loss is 0.04612703621387482\n",
            "epoch: 2 step: 763, loss is 0.15548956394195557\n",
            "epoch: 2 step: 764, loss is 0.22258730232715607\n",
            "epoch: 2 step: 765, loss is 0.07331415265798569\n",
            "epoch: 2 step: 766, loss is 0.03374356031417847\n",
            "epoch: 2 step: 767, loss is 0.3104972839355469\n",
            "epoch: 2 step: 768, loss is 0.14298120141029358\n",
            "epoch: 2 step: 769, loss is 0.09714353829622269\n",
            "epoch: 2 step: 770, loss is 0.05466308444738388\n",
            "epoch: 2 step: 771, loss is 0.16845649480819702\n",
            "epoch: 2 step: 772, loss is 0.06000963971018791\n",
            "epoch: 2 step: 773, loss is 0.06679023057222366\n",
            "epoch: 2 step: 774, loss is 0.2493305206298828\n",
            "epoch: 2 step: 775, loss is 0.25755226612091064\n",
            "epoch: 2 step: 776, loss is 0.04612816497683525\n",
            "epoch: 2 step: 777, loss is 0.1576409637928009\n",
            "epoch: 2 step: 778, loss is 0.07317599654197693\n",
            "epoch: 2 step: 779, loss is 0.2080526202917099\n",
            "epoch: 2 step: 780, loss is 0.24527683854103088\n",
            "epoch: 2 step: 781, loss is 0.03270276263356209\n",
            "epoch: 2 step: 782, loss is 0.11331768333911896\n",
            "epoch: 2 step: 783, loss is 0.2410239726305008\n",
            "epoch: 2 step: 784, loss is 0.27293452620506287\n",
            "epoch: 2 step: 785, loss is 0.1855650246143341\n",
            "epoch: 2 step: 786, loss is 0.16221444308757782\n",
            "epoch: 2 step: 787, loss is 0.23406408727169037\n",
            "epoch: 2 step: 788, loss is 0.15729999542236328\n",
            "epoch: 2 step: 789, loss is 0.0865531787276268\n",
            "epoch: 2 step: 790, loss is 0.14842142164707184\n",
            "epoch: 2 step: 791, loss is 0.034191664308309555\n",
            "epoch: 2 step: 792, loss is 0.03590754047036171\n",
            "epoch: 2 step: 793, loss is 0.3168434798717499\n",
            "epoch: 2 step: 794, loss is 0.16084903478622437\n",
            "epoch: 2 step: 795, loss is 0.0709988996386528\n",
            "epoch: 2 step: 796, loss is 0.05013054981827736\n",
            "epoch: 2 step: 797, loss is 0.04295160993933678\n",
            "epoch: 2 step: 798, loss is 0.39662405848503113\n",
            "epoch: 2 step: 799, loss is 0.22160807251930237\n",
            "epoch: 2 step: 800, loss is 0.06506164371967316\n",
            "epoch: 2 step: 801, loss is 0.032003361731767654\n",
            "epoch: 2 step: 802, loss is 0.17895497381687164\n",
            "epoch: 2 step: 803, loss is 0.32748591899871826\n",
            "epoch: 2 step: 804, loss is 0.11263544112443924\n",
            "epoch: 2 step: 805, loss is 0.04420173913240433\n",
            "epoch: 2 step: 806, loss is 0.35322216153144836\n",
            "epoch: 2 step: 807, loss is 0.1662541776895523\n",
            "epoch: 2 step: 808, loss is 0.05526839941740036\n",
            "epoch: 2 step: 809, loss is 0.05466394126415253\n",
            "epoch: 2 step: 810, loss is 0.07415605336427689\n",
            "epoch: 2 step: 811, loss is 0.08159684389829636\n",
            "epoch: 2 step: 812, loss is 0.22102944552898407\n",
            "epoch: 2 step: 813, loss is 0.3108227550983429\n",
            "epoch: 2 step: 814, loss is 0.07025561481714249\n",
            "epoch: 2 step: 815, loss is 0.04179763048887253\n",
            "epoch: 2 step: 816, loss is 0.08525659888982773\n",
            "epoch: 2 step: 817, loss is 0.23322974145412445\n",
            "epoch: 2 step: 818, loss is 0.216228649020195\n",
            "epoch: 2 step: 819, loss is 0.07584284245967865\n",
            "epoch: 2 step: 820, loss is 0.16432878375053406\n",
            "epoch: 2 step: 821, loss is 0.02861793525516987\n",
            "epoch: 2 step: 822, loss is 0.25988540053367615\n",
            "epoch: 2 step: 823, loss is 0.1203354075551033\n",
            "epoch: 2 step: 824, loss is 0.04945412278175354\n",
            "epoch: 2 step: 825, loss is 0.04729015380144119\n",
            "epoch: 2 step: 826, loss is 0.14201761782169342\n",
            "epoch: 2 step: 827, loss is 0.13922280073165894\n",
            "epoch: 2 step: 828, loss is 0.041054315865039825\n",
            "epoch: 2 step: 829, loss is 0.03644002601504326\n",
            "epoch: 2 step: 830, loss is 0.06878025084733963\n",
            "epoch: 2 step: 831, loss is 0.12897232174873352\n",
            "epoch: 2 step: 832, loss is 0.10724495351314545\n",
            "epoch: 2 step: 833, loss is 0.04574981704354286\n",
            "epoch: 2 step: 834, loss is 0.23898936808109283\n",
            "epoch: 2 step: 835, loss is 0.337184876203537\n",
            "epoch: 2 step: 836, loss is 0.15433141589164734\n",
            "epoch: 2 step: 837, loss is 0.13276690244674683\n",
            "epoch: 2 step: 838, loss is 0.20279988646507263\n",
            "epoch: 2 step: 839, loss is 0.04579838737845421\n",
            "epoch: 2 step: 840, loss is 0.053713247179985046\n",
            "epoch: 2 step: 841, loss is 0.0922376811504364\n",
            "epoch: 2 step: 842, loss is 0.04571881517767906\n",
            "epoch: 2 step: 843, loss is 0.15904362499713898\n",
            "epoch: 2 step: 844, loss is 0.05180975794792175\n",
            "epoch: 2 step: 845, loss is 0.11264906823635101\n",
            "epoch: 2 step: 846, loss is 0.06181234493851662\n",
            "epoch: 2 step: 847, loss is 0.048199720680713654\n",
            "epoch: 2 step: 848, loss is 0.14735294878482819\n",
            "epoch: 2 step: 849, loss is 0.048893075436353683\n",
            "epoch: 2 step: 850, loss is 0.061612535268068314\n",
            "epoch: 2 step: 851, loss is 0.037497516721487045\n",
            "epoch: 2 step: 852, loss is 0.1430557370185852\n",
            "epoch: 2 step: 853, loss is 0.06083733215928078\n",
            "epoch: 2 step: 854, loss is 0.1482849419116974\n",
            "epoch: 2 step: 855, loss is 0.05021116882562637\n",
            "epoch: 2 step: 856, loss is 0.0786680281162262\n",
            "epoch: 2 step: 857, loss is 0.14151863753795624\n",
            "epoch: 2 step: 858, loss is 0.053368207067251205\n",
            "epoch: 2 step: 859, loss is 0.0675446167588234\n",
            "epoch: 2 step: 860, loss is 0.2544350326061249\n",
            "epoch: 2 step: 861, loss is 0.20369310677051544\n",
            "epoch: 2 step: 862, loss is 0.05265786871314049\n",
            "epoch: 2 step: 863, loss is 0.03938615694642067\n",
            "epoch: 2 step: 864, loss is 0.15040308237075806\n",
            "epoch: 2 step: 865, loss is 0.12272051721811295\n",
            "epoch: 2 step: 866, loss is 0.16394205391407013\n",
            "epoch: 2 step: 867, loss is 0.07078851014375687\n",
            "epoch: 2 step: 868, loss is 0.2202630639076233\n",
            "epoch: 2 step: 869, loss is 0.045337896794080734\n",
            "epoch: 2 step: 870, loss is 0.3090696632862091\n",
            "epoch: 2 step: 871, loss is 0.05193893238902092\n",
            "epoch: 2 step: 872, loss is 0.49930059909820557\n",
            "epoch: 2 step: 873, loss is 0.14877845346927643\n",
            "epoch: 2 step: 874, loss is 0.08627255260944366\n",
            "epoch: 2 step: 875, loss is 0.06230713054537773\n",
            "epoch: 2 step: 876, loss is 0.176796093583107\n",
            "epoch: 2 step: 877, loss is 0.03294806927442551\n",
            "epoch: 2 step: 878, loss is 0.22987431287765503\n",
            "epoch: 2 step: 879, loss is 0.0754965990781784\n",
            "epoch: 2 step: 880, loss is 0.11887187510728836\n",
            "epoch: 2 step: 881, loss is 0.2604405879974365\n",
            "epoch: 2 step: 882, loss is 0.17877043783664703\n",
            "epoch: 2 step: 883, loss is 0.16622471809387207\n",
            "epoch: 2 step: 884, loss is 0.200903981924057\n",
            "epoch: 2 step: 885, loss is 0.022056076675653458\n",
            "epoch: 2 step: 886, loss is 0.055308952927589417\n",
            "epoch: 2 step: 887, loss is 0.17311112582683563\n",
            "epoch: 2 step: 888, loss is 0.03723208233714104\n",
            "epoch: 2 step: 889, loss is 0.08550863713026047\n",
            "epoch: 2 step: 890, loss is 0.021962622180581093\n",
            "epoch: 2 step: 891, loss is 0.10482557862997055\n",
            "epoch: 2 step: 892, loss is 0.07511760294437408\n",
            "epoch: 2 step: 893, loss is 0.19596248865127563\n",
            "epoch: 2 step: 894, loss is 0.05906002223491669\n",
            "epoch: 2 step: 895, loss is 0.10432669520378113\n",
            "epoch: 2 step: 896, loss is 0.1108044981956482\n",
            "epoch: 2 step: 897, loss is 0.10167337208986282\n",
            "epoch: 2 step: 898, loss is 0.03976067155599594\n",
            "epoch: 2 step: 899, loss is 0.23765963315963745\n",
            "epoch: 2 step: 900, loss is 0.06654860079288483\n",
            "epoch: 2 step: 901, loss is 0.10753151774406433\n",
            "epoch: 2 step: 902, loss is 0.1374955028295517\n",
            "epoch: 2 step: 903, loss is 0.08497805893421173\n",
            "epoch: 2 step: 904, loss is 0.15163929760456085\n",
            "epoch: 2 step: 905, loss is 0.07231021672487259\n",
            "epoch: 2 step: 906, loss is 0.20924533903598785\n",
            "epoch: 2 step: 907, loss is 0.02991306222975254\n",
            "epoch: 2 step: 908, loss is 0.04785386845469475\n",
            "epoch: 2 step: 909, loss is 0.05844033136963844\n",
            "epoch: 2 step: 910, loss is 0.0823289304971695\n",
            "epoch: 2 step: 911, loss is 0.12679363787174225\n",
            "epoch: 2 step: 912, loss is 0.06531427800655365\n",
            "epoch: 2 step: 913, loss is 0.07367098331451416\n",
            "epoch: 2 step: 914, loss is 0.05009368062019348\n",
            "epoch: 2 step: 915, loss is 0.02682107873260975\n",
            "epoch: 2 step: 916, loss is 0.11719589680433273\n",
            "epoch: 2 step: 917, loss is 0.23205748200416565\n",
            "epoch: 2 step: 918, loss is 0.05661997199058533\n",
            "epoch: 2 step: 919, loss is 0.16307063400745392\n",
            "epoch: 2 step: 920, loss is 0.059141725301742554\n",
            "epoch: 2 step: 921, loss is 0.11421803385019302\n",
            "epoch: 2 step: 922, loss is 0.0989217609167099\n",
            "epoch: 2 step: 923, loss is 0.04553144797682762\n",
            "epoch: 2 step: 924, loss is 0.1450059860944748\n",
            "epoch: 2 step: 925, loss is 0.048151176422834396\n",
            "epoch: 2 step: 926, loss is 0.1271032691001892\n",
            "epoch: 2 step: 927, loss is 0.033660322427749634\n",
            "epoch: 2 step: 928, loss is 0.14677786827087402\n",
            "epoch: 2 step: 929, loss is 0.035794250667095184\n",
            "epoch: 2 step: 930, loss is 0.15685737133026123\n",
            "epoch: 2 step: 931, loss is 0.034673914313316345\n",
            "epoch: 2 step: 932, loss is 0.06303317099809647\n",
            "epoch: 2 step: 933, loss is 0.21048960089683533\n",
            "epoch: 2 step: 934, loss is 0.042654894292354584\n",
            "epoch: 2 step: 935, loss is 0.0750875174999237\n",
            "epoch: 2 step: 936, loss is 0.020064430311322212\n",
            "epoch: 2 step: 937, loss is 0.20895959436893463\n",
            "epoch: 2 step: 938, loss is 0.1852770745754242\n",
            "epoch: 2 step: 939, loss is 0.18077152967453003\n",
            "epoch: 2 step: 940, loss is 0.17173057794570923\n",
            "epoch: 2 step: 941, loss is 0.37140339612960815\n",
            "epoch: 2 step: 942, loss is 0.2743745446205139\n",
            "epoch: 2 step: 943, loss is 0.2121070921421051\n",
            "epoch: 2 step: 944, loss is 0.10961034893989563\n",
            "epoch: 2 step: 945, loss is 0.08833391964435577\n",
            "epoch: 2 step: 946, loss is 0.07510221004486084\n",
            "epoch: 2 step: 947, loss is 0.0358080230653286\n",
            "epoch: 2 step: 948, loss is 0.10235487669706345\n",
            "epoch: 2 step: 949, loss is 0.19965027272701263\n",
            "epoch: 2 step: 950, loss is 0.22415435314178467\n",
            "epoch: 2 step: 951, loss is 0.03131016716361046\n",
            "epoch: 2 step: 952, loss is 0.03876335546374321\n",
            "epoch: 2 step: 953, loss is 0.02235913835465908\n",
            "epoch: 2 step: 954, loss is 0.1142398864030838\n",
            "epoch: 2 step: 955, loss is 0.12428911030292511\n",
            "epoch: 2 step: 956, loss is 0.016733091324567795\n",
            "epoch: 2 step: 957, loss is 0.10563112795352936\n",
            "epoch: 2 step: 958, loss is 0.11861217021942139\n",
            "epoch: 2 step: 959, loss is 0.2169068157672882\n",
            "epoch: 2 step: 960, loss is 0.09399953484535217\n",
            "epoch: 2 step: 961, loss is 0.14214526116847992\n",
            "epoch: 2 step: 962, loss is 0.12374605238437653\n",
            "epoch: 2 step: 963, loss is 0.03671455383300781\n",
            "epoch: 2 step: 964, loss is 0.17929130792617798\n",
            "epoch: 2 step: 965, loss is 0.05242463946342468\n",
            "epoch: 2 step: 966, loss is 0.1855604648590088\n",
            "epoch: 2 step: 967, loss is 0.10886122286319733\n",
            "epoch: 2 step: 968, loss is 0.03203699737787247\n",
            "epoch: 2 step: 969, loss is 0.07586049288511276\n",
            "epoch: 2 step: 970, loss is 0.10609506815671921\n",
            "epoch: 2 step: 971, loss is 0.3799531161785126\n",
            "epoch: 2 step: 972, loss is 0.20047716796398163\n",
            "epoch: 2 step: 973, loss is 0.08001279830932617\n",
            "epoch: 2 step: 974, loss is 0.09782549738883972\n",
            "epoch: 2 step: 975, loss is 0.3786536455154419\n",
            "epoch: 2 step: 976, loss is 0.05621817708015442\n",
            "epoch: 2 step: 977, loss is 0.05231751129031181\n",
            "epoch: 2 step: 978, loss is 0.054573941975831985\n",
            "epoch: 2 step: 979, loss is 0.04402706027030945\n",
            "epoch: 2 step: 980, loss is 0.15689344704151154\n",
            "epoch: 2 step: 981, loss is 0.06674804538488388\n",
            "epoch: 2 step: 982, loss is 0.16576306521892548\n",
            "epoch: 2 step: 983, loss is 0.030426913872361183\n",
            "epoch: 2 step: 984, loss is 0.15173503756523132\n",
            "epoch: 2 step: 985, loss is 0.13353414833545685\n",
            "epoch: 2 step: 986, loss is 0.232109934091568\n",
            "epoch: 2 step: 987, loss is 0.0380396768450737\n",
            "epoch: 2 step: 988, loss is 0.15719689428806305\n",
            "epoch: 2 step: 989, loss is 0.15360091626644135\n",
            "epoch: 2 step: 990, loss is 0.05821593850851059\n",
            "epoch: 2 step: 991, loss is 0.11010725051164627\n",
            "epoch: 2 step: 992, loss is 0.10825631022453308\n",
            "epoch: 2 step: 993, loss is 0.09968548268079758\n",
            "epoch: 2 step: 994, loss is 0.04681795462965965\n",
            "epoch: 2 step: 995, loss is 0.03528343886137009\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WARNING] ME(1671:136248148721664,MainProcess):2025-04-19-15:00:27.993.000 [mindspore/train/callback/_early_stop.py:221] Early stopping is conditioned on accuracy, which is not available. Available choices are: {'loss'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 3 step: 1, loss is 0.050761930644512177\n",
            "epoch: 3 step: 2, loss is 0.042721379548311234\n",
            "epoch: 3 step: 3, loss is 0.15446273982524872\n",
            "epoch: 3 step: 4, loss is 0.08598857372999191\n",
            "epoch: 3 step: 5, loss is 0.03846484050154686\n",
            "epoch: 3 step: 6, loss is 0.06409306079149246\n",
            "epoch: 3 step: 7, loss is 0.11355197429656982\n",
            "epoch: 3 step: 8, loss is 0.13566583395004272\n",
            "epoch: 3 step: 9, loss is 0.04392684996128082\n",
            "epoch: 3 step: 10, loss is 0.2177165299654007\n",
            "epoch: 3 step: 11, loss is 0.18399398028850555\n",
            "epoch: 3 step: 12, loss is 0.16995030641555786\n",
            "epoch: 3 step: 13, loss is 0.09828925877809525\n",
            "epoch: 3 step: 14, loss is 0.02122863382101059\n",
            "epoch: 3 step: 15, loss is 0.05428694933652878\n",
            "epoch: 3 step: 16, loss is 0.06423508375883102\n",
            "epoch: 3 step: 17, loss is 0.14063630998134613\n",
            "epoch: 3 step: 18, loss is 0.1186385378241539\n",
            "epoch: 3 step: 19, loss is 0.05218574032187462\n",
            "epoch: 3 step: 20, loss is 0.0488957054913044\n",
            "epoch: 3 step: 21, loss is 0.1390858143568039\n",
            "epoch: 3 step: 22, loss is 0.172599196434021\n",
            "epoch: 3 step: 23, loss is 0.1390417516231537\n",
            "epoch: 3 step: 24, loss is 0.1283157467842102\n",
            "epoch: 3 step: 25, loss is 0.2046584039926529\n",
            "epoch: 3 step: 26, loss is 0.2254381775856018\n",
            "epoch: 3 step: 27, loss is 0.03617767617106438\n",
            "epoch: 3 step: 28, loss is 0.14870871603488922\n",
            "epoch: 3 step: 29, loss is 0.19061227142810822\n",
            "epoch: 3 step: 30, loss is 0.020103713497519493\n",
            "epoch: 3 step: 31, loss is 0.269266277551651\n",
            "epoch: 3 step: 32, loss is 0.11704381555318832\n",
            "epoch: 3 step: 33, loss is 0.15684984624385834\n",
            "epoch: 3 step: 34, loss is 0.30793750286102295\n",
            "epoch: 3 step: 35, loss is 0.12609387934207916\n",
            "epoch: 3 step: 36, loss is 0.07401254773139954\n",
            "epoch: 3 step: 37, loss is 0.2327781319618225\n",
            "epoch: 3 step: 38, loss is 0.15296094119548798\n",
            "epoch: 3 step: 39, loss is 0.12039713561534882\n",
            "epoch: 3 step: 40, loss is 0.19323036074638367\n",
            "epoch: 3 step: 41, loss is 0.13428013026714325\n",
            "epoch: 3 step: 42, loss is 0.030643822625279427\n",
            "epoch: 3 step: 43, loss is 0.06769178807735443\n",
            "epoch: 3 step: 44, loss is 0.036717332899570465\n",
            "epoch: 3 step: 45, loss is 0.04642460495233536\n",
            "epoch: 3 step: 46, loss is 0.15615518391132355\n",
            "epoch: 3 step: 47, loss is 0.24003192782402039\n",
            "epoch: 3 step: 48, loss is 0.02932765707373619\n",
            "epoch: 3 step: 49, loss is 0.03857896104454994\n",
            "epoch: 3 step: 50, loss is 0.04748278483748436\n",
            "epoch: 3 step: 51, loss is 0.04560159891843796\n",
            "epoch: 3 step: 52, loss is 0.02777859941124916\n",
            "epoch: 3 step: 53, loss is 0.0542851947247982\n",
            "epoch: 3 step: 54, loss is 0.05805805325508118\n",
            "epoch: 3 step: 55, loss is 0.030523521825671196\n",
            "epoch: 3 step: 56, loss is 0.1578940749168396\n",
            "epoch: 3 step: 57, loss is 0.03053935244679451\n",
            "epoch: 3 step: 58, loss is 0.10944388806819916\n",
            "epoch: 3 step: 59, loss is 0.12146133184432983\n",
            "epoch: 3 step: 60, loss is 0.032255079597234726\n",
            "epoch: 3 step: 61, loss is 0.057398729026317596\n",
            "epoch: 3 step: 62, loss is 0.03946276381611824\n",
            "epoch: 3 step: 63, loss is 0.09692764282226562\n",
            "epoch: 3 step: 64, loss is 0.06850949674844742\n",
            "epoch: 3 step: 65, loss is 0.03346897289156914\n",
            "epoch: 3 step: 66, loss is 0.05631374567747116\n",
            "epoch: 3 step: 67, loss is 0.03340495005249977\n",
            "epoch: 3 step: 68, loss is 0.03493201732635498\n",
            "epoch: 3 step: 69, loss is 0.24269138276576996\n",
            "epoch: 3 step: 70, loss is 0.03089083358645439\n",
            "epoch: 3 step: 71, loss is 0.041340358555316925\n",
            "epoch: 3 step: 72, loss is 0.1077660620212555\n",
            "epoch: 3 step: 73, loss is 0.058390580117702484\n",
            "epoch: 3 step: 74, loss is 0.2038397639989853\n",
            "epoch: 3 step: 75, loss is 0.10318557918071747\n",
            "epoch: 3 step: 76, loss is 0.26204320788383484\n",
            "epoch: 3 step: 77, loss is 0.15540121495723724\n",
            "epoch: 3 step: 78, loss is 0.09644822776317596\n",
            "epoch: 3 step: 79, loss is 0.05892545357346535\n",
            "epoch: 3 step: 80, loss is 0.07245112955570221\n",
            "epoch: 3 step: 81, loss is 0.035842929035425186\n",
            "epoch: 3 step: 82, loss is 0.07745273411273956\n",
            "epoch: 3 step: 83, loss is 0.10924993455410004\n",
            "epoch: 3 step: 84, loss is 0.039250750094652176\n",
            "epoch: 3 step: 85, loss is 0.199411541223526\n",
            "epoch: 3 step: 86, loss is 0.04495486617088318\n",
            "epoch: 3 step: 87, loss is 0.0766395851969719\n",
            "epoch: 3 step: 88, loss is 0.04745322838425636\n",
            "epoch: 3 step: 89, loss is 0.028269492089748383\n",
            "epoch: 3 step: 90, loss is 0.1578035056591034\n",
            "epoch: 3 step: 91, loss is 0.13526789844036102\n",
            "epoch: 3 step: 92, loss is 0.052879154682159424\n",
            "epoch: 3 step: 93, loss is 0.09405602514743805\n",
            "epoch: 3 step: 94, loss is 0.0906091183423996\n",
            "epoch: 3 step: 95, loss is 0.16415469348430634\n",
            "epoch: 3 step: 96, loss is 0.12895095348358154\n",
            "epoch: 3 step: 97, loss is 0.029023276641964912\n",
            "epoch: 3 step: 98, loss is 0.07131100445985794\n",
            "epoch: 3 step: 99, loss is 0.18998432159423828\n",
            "epoch: 3 step: 100, loss is 0.07014292478561401\n",
            "epoch: 3 step: 101, loss is 0.05534908547997475\n",
            "epoch: 3 step: 102, loss is 0.03642614185810089\n",
            "epoch: 3 step: 103, loss is 0.096611388027668\n",
            "epoch: 3 step: 104, loss is 0.016780911013484\n",
            "epoch: 3 step: 105, loss is 0.287803590297699\n",
            "epoch: 3 step: 106, loss is 0.12820108234882355\n",
            "epoch: 3 step: 107, loss is 0.019588865339756012\n",
            "epoch: 3 step: 108, loss is 0.05917778238654137\n",
            "epoch: 3 step: 109, loss is 0.2318066656589508\n",
            "epoch: 3 step: 110, loss is 0.05219520255923271\n",
            "epoch: 3 step: 111, loss is 0.03992870822548866\n",
            "epoch: 3 step: 112, loss is 0.12589633464813232\n",
            "epoch: 3 step: 113, loss is 0.16507002711296082\n",
            "epoch: 3 step: 114, loss is 0.052823640406131744\n",
            "epoch: 3 step: 115, loss is 0.06522531807422638\n",
            "epoch: 3 step: 116, loss is 0.07598897814750671\n",
            "epoch: 3 step: 117, loss is 0.027392908930778503\n",
            "epoch: 3 step: 118, loss is 0.07902592420578003\n",
            "epoch: 3 step: 119, loss is 0.03866840898990631\n",
            "epoch: 3 step: 120, loss is 0.0711086243391037\n",
            "epoch: 3 step: 121, loss is 0.043733298778533936\n",
            "epoch: 3 step: 122, loss is 0.04837075620889664\n",
            "epoch: 3 step: 123, loss is 0.040644414722919464\n",
            "epoch: 3 step: 124, loss is 0.07166524231433868\n",
            "epoch: 3 step: 125, loss is 0.041359975934028625\n",
            "epoch: 3 step: 126, loss is 0.11570270359516144\n",
            "epoch: 3 step: 127, loss is 0.49331557750701904\n",
            "epoch: 3 step: 128, loss is 0.016418147832155228\n",
            "epoch: 3 step: 129, loss is 0.23287753760814667\n",
            "epoch: 3 step: 130, loss is 0.1515101045370102\n",
            "epoch: 3 step: 131, loss is 0.026680242270231247\n",
            "epoch: 3 step: 132, loss is 0.16838598251342773\n",
            "epoch: 3 step: 133, loss is 0.20465341210365295\n",
            "epoch: 3 step: 134, loss is 0.01945084147155285\n",
            "epoch: 3 step: 135, loss is 0.03614811226725578\n",
            "epoch: 3 step: 136, loss is 0.05495162308216095\n",
            "epoch: 3 step: 137, loss is 0.0287266056984663\n",
            "epoch: 3 step: 138, loss is 0.029515430331230164\n",
            "epoch: 3 step: 139, loss is 0.03070368617773056\n",
            "epoch: 3 step: 140, loss is 0.03621334210038185\n",
            "epoch: 3 step: 141, loss is 0.11235000193119049\n",
            "epoch: 3 step: 142, loss is 0.23907308280467987\n",
            "epoch: 3 step: 143, loss is 0.05058278143405914\n",
            "epoch: 3 step: 144, loss is 0.09363614022731781\n",
            "epoch: 3 step: 145, loss is 0.04723828658461571\n",
            "epoch: 3 step: 146, loss is 0.03750080615282059\n",
            "epoch: 3 step: 147, loss is 0.31976771354675293\n",
            "epoch: 3 step: 148, loss is 0.02082010917365551\n",
            "epoch: 3 step: 149, loss is 0.11999358981847763\n",
            "epoch: 3 step: 150, loss is 0.1512776017189026\n",
            "epoch: 3 step: 151, loss is 0.042668212205171585\n",
            "epoch: 3 step: 152, loss is 0.07443893700838089\n",
            "epoch: 3 step: 153, loss is 0.06846567243337631\n",
            "epoch: 3 step: 154, loss is 0.08533406257629395\n",
            "epoch: 3 step: 155, loss is 0.2563103437423706\n",
            "epoch: 3 step: 156, loss is 0.19866248965263367\n",
            "epoch: 3 step: 157, loss is 0.05796920880675316\n",
            "epoch: 3 step: 158, loss is 0.2134326547384262\n",
            "epoch: 3 step: 159, loss is 0.30071109533309937\n",
            "epoch: 3 step: 160, loss is 0.07787925750017166\n",
            "epoch: 3 step: 161, loss is 0.16464345157146454\n",
            "epoch: 3 step: 162, loss is 0.08559362590312958\n",
            "epoch: 3 step: 163, loss is 0.06531725078821182\n",
            "epoch: 3 step: 164, loss is 0.072867751121521\n",
            "epoch: 3 step: 165, loss is 0.052098844200372696\n",
            "epoch: 3 step: 166, loss is 0.020195985212922096\n",
            "epoch: 3 step: 167, loss is 0.19210311770439148\n",
            "epoch: 3 step: 168, loss is 0.03984168916940689\n",
            "epoch: 3 step: 169, loss is 0.035420011729002\n",
            "epoch: 3 step: 170, loss is 0.04148843139410019\n",
            "epoch: 3 step: 171, loss is 0.04758455604314804\n",
            "epoch: 3 step: 172, loss is 0.013865710236132145\n",
            "epoch: 3 step: 173, loss is 0.08456861227750778\n",
            "epoch: 3 step: 174, loss is 0.39803647994995117\n",
            "epoch: 3 step: 175, loss is 0.16695557534694672\n",
            "epoch: 3 step: 176, loss is 0.0556519478559494\n",
            "epoch: 3 step: 177, loss is 0.2370576113462448\n",
            "epoch: 3 step: 178, loss is 0.024089770391583443\n",
            "epoch: 3 step: 179, loss is 0.02871609292924404\n",
            "epoch: 3 step: 180, loss is 0.10719797015190125\n",
            "epoch: 3 step: 181, loss is 0.35506194829940796\n",
            "epoch: 3 step: 182, loss is 0.042522165924310684\n",
            "epoch: 3 step: 183, loss is 0.34903833270072937\n",
            "epoch: 3 step: 184, loss is 0.03440679982304573\n",
            "epoch: 3 step: 185, loss is 0.035267073661088943\n",
            "epoch: 3 step: 186, loss is 0.24397052824497223\n",
            "epoch: 3 step: 187, loss is 0.06880827248096466\n",
            "epoch: 3 step: 188, loss is 0.06790140271186829\n",
            "epoch: 3 step: 189, loss is 0.2167052924633026\n",
            "epoch: 3 step: 190, loss is 0.15532809495925903\n",
            "epoch: 3 step: 191, loss is 0.0345684215426445\n",
            "epoch: 3 step: 192, loss is 0.03515801951289177\n",
            "epoch: 3 step: 193, loss is 0.031986936926841736\n",
            "epoch: 3 step: 194, loss is 0.17061783373355865\n",
            "epoch: 3 step: 195, loss is 0.02293231710791588\n",
            "epoch: 3 step: 196, loss is 0.1169709712266922\n",
            "epoch: 3 step: 197, loss is 0.3131648302078247\n",
            "epoch: 3 step: 198, loss is 0.29397517442703247\n",
            "epoch: 3 step: 199, loss is 0.0801190435886383\n",
            "epoch: 3 step: 200, loss is 0.21604695916175842\n",
            "epoch: 3 step: 201, loss is 0.028583811596035957\n",
            "epoch: 3 step: 202, loss is 0.06126759201288223\n",
            "epoch: 3 step: 203, loss is 0.02327978052198887\n",
            "epoch: 3 step: 204, loss is 0.03813765570521355\n",
            "epoch: 3 step: 205, loss is 0.17125865817070007\n",
            "epoch: 3 step: 206, loss is 0.12970633804798126\n",
            "epoch: 3 step: 207, loss is 0.10335200279951096\n",
            "epoch: 3 step: 208, loss is 0.4513787031173706\n",
            "epoch: 3 step: 209, loss is 0.06637538969516754\n",
            "epoch: 3 step: 210, loss is 0.03902781009674072\n",
            "epoch: 3 step: 211, loss is 0.12774796783924103\n",
            "epoch: 3 step: 212, loss is 0.04239208623766899\n",
            "epoch: 3 step: 213, loss is 0.10595910251140594\n",
            "epoch: 3 step: 214, loss is 0.06683297455310822\n",
            "epoch: 3 step: 215, loss is 0.027886291965842247\n",
            "epoch: 3 step: 216, loss is 0.04659363254904747\n",
            "epoch: 3 step: 217, loss is 0.03511479124426842\n",
            "epoch: 3 step: 218, loss is 0.08940783888101578\n",
            "epoch: 3 step: 219, loss is 0.15085186064243317\n",
            "epoch: 3 step: 220, loss is 0.03226330876350403\n",
            "epoch: 3 step: 221, loss is 0.049377914518117905\n",
            "epoch: 3 step: 222, loss is 0.0975431352853775\n",
            "epoch: 3 step: 223, loss is 0.02072959765791893\n",
            "epoch: 3 step: 224, loss is 0.12546700239181519\n",
            "epoch: 3 step: 225, loss is 0.1135709285736084\n",
            "epoch: 3 step: 226, loss is 0.43581289052963257\n",
            "epoch: 3 step: 227, loss is 0.277347594499588\n",
            "epoch: 3 step: 228, loss is 0.08092831820249557\n",
            "epoch: 3 step: 229, loss is 0.07292153686285019\n",
            "epoch: 3 step: 230, loss is 0.06179320067167282\n",
            "epoch: 3 step: 231, loss is 0.04983292520046234\n",
            "epoch: 3 step: 232, loss is 0.027323709800839424\n",
            "epoch: 3 step: 233, loss is 0.023064950481057167\n",
            "epoch: 3 step: 234, loss is 0.04367265850305557\n",
            "epoch: 3 step: 235, loss is 0.20376549661159515\n",
            "epoch: 3 step: 236, loss is 0.03284580260515213\n",
            "epoch: 3 step: 237, loss is 0.025203034281730652\n",
            "epoch: 3 step: 238, loss is 0.20336489379405975\n",
            "epoch: 3 step: 239, loss is 0.021831875666975975\n",
            "epoch: 3 step: 240, loss is 0.02325684390962124\n",
            "epoch: 3 step: 241, loss is 0.19830437004566193\n",
            "epoch: 3 step: 242, loss is 0.051855962723493576\n",
            "epoch: 3 step: 243, loss is 0.1363464593887329\n",
            "epoch: 3 step: 244, loss is 0.02203802764415741\n",
            "epoch: 3 step: 245, loss is 0.042565133422613144\n",
            "epoch: 3 step: 246, loss is 0.10130860656499863\n",
            "epoch: 3 step: 247, loss is 0.10573135316371918\n",
            "epoch: 3 step: 248, loss is 0.0621996633708477\n",
            "epoch: 3 step: 249, loss is 0.1016039326786995\n",
            "epoch: 3 step: 250, loss is 0.12119007110595703\n",
            "epoch: 3 step: 251, loss is 0.0507240854203701\n",
            "epoch: 3 step: 252, loss is 0.12068906426429749\n",
            "epoch: 3 step: 253, loss is 0.024537980556488037\n",
            "epoch: 3 step: 254, loss is 0.03413870558142662\n",
            "epoch: 3 step: 255, loss is 0.08353599160909653\n",
            "epoch: 3 step: 256, loss is 0.025675347074866295\n",
            "epoch: 3 step: 257, loss is 0.25921741127967834\n",
            "epoch: 3 step: 258, loss is 0.016534904018044472\n",
            "epoch: 3 step: 259, loss is 0.17071154713630676\n",
            "epoch: 3 step: 260, loss is 0.03201708570122719\n",
            "epoch: 3 step: 261, loss is 0.0422142930328846\n",
            "epoch: 3 step: 262, loss is 0.031630102545022964\n",
            "epoch: 3 step: 263, loss is 0.1108173280954361\n",
            "epoch: 3 step: 264, loss is 0.13016919791698456\n",
            "epoch: 3 step: 265, loss is 0.08589714765548706\n",
            "epoch: 3 step: 266, loss is 0.03448313847184181\n",
            "epoch: 3 step: 267, loss is 0.22915229201316833\n",
            "epoch: 3 step: 268, loss is 0.06155663728713989\n",
            "epoch: 3 step: 269, loss is 0.04908723384141922\n",
            "epoch: 3 step: 270, loss is 0.1624945104122162\n",
            "epoch: 3 step: 271, loss is 0.08809240162372589\n",
            "epoch: 3 step: 272, loss is 0.03260718286037445\n",
            "epoch: 3 step: 273, loss is 0.02268453873693943\n",
            "epoch: 3 step: 274, loss is 0.23656226694583893\n",
            "epoch: 3 step: 275, loss is 0.18172353506088257\n",
            "epoch: 3 step: 276, loss is 0.05580422282218933\n",
            "epoch: 3 step: 277, loss is 0.06869158148765564\n",
            "epoch: 3 step: 278, loss is 0.025705017149448395\n",
            "epoch: 3 step: 279, loss is 0.19417640566825867\n",
            "epoch: 3 step: 280, loss is 0.03803684934973717\n",
            "epoch: 3 step: 281, loss is 0.01752706989645958\n",
            "epoch: 3 step: 282, loss is 0.03350896015763283\n",
            "epoch: 3 step: 283, loss is 0.11235974729061127\n",
            "epoch: 3 step: 284, loss is 0.08902471512556076\n",
            "epoch: 3 step: 285, loss is 0.03581540286540985\n",
            "epoch: 3 step: 286, loss is 0.02323611080646515\n",
            "epoch: 3 step: 287, loss is 0.10262464731931686\n",
            "epoch: 3 step: 288, loss is 0.1378483921289444\n",
            "epoch: 3 step: 289, loss is 0.08683644235134125\n",
            "epoch: 3 step: 290, loss is 0.046903837472200394\n",
            "epoch: 3 step: 291, loss is 0.06326910108327866\n",
            "epoch: 3 step: 292, loss is 0.037182025611400604\n",
            "epoch: 3 step: 293, loss is 0.17699803411960602\n",
            "epoch: 3 step: 294, loss is 0.13988375663757324\n",
            "epoch: 3 step: 295, loss is 0.023572947829961777\n",
            "epoch: 3 step: 296, loss is 0.28295961022377014\n",
            "epoch: 3 step: 297, loss is 0.108706533908844\n",
            "epoch: 3 step: 298, loss is 0.038078129291534424\n",
            "epoch: 3 step: 299, loss is 0.048702891916036606\n",
            "epoch: 3 step: 300, loss is 0.1131972149014473\n",
            "epoch: 3 step: 301, loss is 0.11924774199724197\n",
            "epoch: 3 step: 302, loss is 0.04052281007170677\n",
            "epoch: 3 step: 303, loss is 0.04973496124148369\n",
            "epoch: 3 step: 304, loss is 0.062002211809158325\n",
            "epoch: 3 step: 305, loss is 0.0341896116733551\n",
            "epoch: 3 step: 306, loss is 0.038178879767656326\n",
            "epoch: 3 step: 307, loss is 0.12391270697116852\n",
            "epoch: 3 step: 308, loss is 0.06708897650241852\n",
            "epoch: 3 step: 309, loss is 0.10298044979572296\n",
            "epoch: 3 step: 310, loss is 0.07703462243080139\n",
            "epoch: 3 step: 311, loss is 0.04201493412256241\n",
            "epoch: 3 step: 312, loss is 0.16563335061073303\n",
            "epoch: 3 step: 313, loss is 0.43011853098869324\n",
            "epoch: 3 step: 314, loss is 0.024152982980012894\n",
            "epoch: 3 step: 315, loss is 0.03738952428102493\n",
            "epoch: 3 step: 316, loss is 0.031182022765278816\n",
            "epoch: 3 step: 317, loss is 0.032072484493255615\n",
            "epoch: 3 step: 318, loss is 0.4231869578361511\n",
            "epoch: 3 step: 319, loss is 0.034403230994939804\n",
            "epoch: 3 step: 320, loss is 0.05981364846229553\n",
            "epoch: 3 step: 321, loss is 0.03055371157824993\n",
            "epoch: 3 step: 322, loss is 0.0878211259841919\n",
            "epoch: 3 step: 323, loss is 0.027111567556858063\n",
            "epoch: 3 step: 324, loss is 0.02386830747127533\n",
            "epoch: 3 step: 325, loss is 0.11163060367107391\n",
            "epoch: 3 step: 326, loss is 0.03451189771294594\n",
            "epoch: 3 step: 327, loss is 0.035811636596918106\n",
            "epoch: 3 step: 328, loss is 0.2248089462518692\n",
            "epoch: 3 step: 329, loss is 0.07479988038539886\n",
            "epoch: 3 step: 330, loss is 0.05440999194979668\n",
            "epoch: 3 step: 331, loss is 0.02913190796971321\n",
            "epoch: 3 step: 332, loss is 0.20778951048851013\n",
            "epoch: 3 step: 333, loss is 0.022172896191477776\n",
            "epoch: 3 step: 334, loss is 0.4299907684326172\n",
            "epoch: 3 step: 335, loss is 0.23244459927082062\n",
            "epoch: 3 step: 336, loss is 0.20331571996212006\n",
            "epoch: 3 step: 337, loss is 0.0504642091691494\n",
            "epoch: 3 step: 338, loss is 0.09135222434997559\n",
            "epoch: 3 step: 339, loss is 0.04518305882811546\n",
            "epoch: 3 step: 340, loss is 0.02424333058297634\n",
            "epoch: 3 step: 341, loss is 0.04905826598405838\n",
            "epoch: 3 step: 342, loss is 0.02745562046766281\n",
            "epoch: 3 step: 343, loss is 0.20153145492076874\n",
            "epoch: 3 step: 344, loss is 0.17604286968708038\n",
            "epoch: 3 step: 345, loss is 0.05239642411470413\n",
            "epoch: 3 step: 346, loss is 0.02563886158168316\n",
            "epoch: 3 step: 347, loss is 0.03747083246707916\n",
            "epoch: 3 step: 348, loss is 0.06969732791185379\n",
            "epoch: 3 step: 349, loss is 0.09581068158149719\n",
            "epoch: 3 step: 350, loss is 0.31987932324409485\n",
            "epoch: 3 step: 351, loss is 0.16557350754737854\n",
            "epoch: 3 step: 352, loss is 0.021362530067563057\n",
            "epoch: 3 step: 353, loss is 0.09445315599441528\n",
            "epoch: 3 step: 354, loss is 0.11985119432210922\n",
            "epoch: 3 step: 355, loss is 0.021688155829906464\n",
            "epoch: 3 step: 356, loss is 0.050411202013492584\n",
            "epoch: 3 step: 357, loss is 0.024673834443092346\n",
            "epoch: 3 step: 358, loss is 0.10306640714406967\n",
            "epoch: 3 step: 359, loss is 0.017233002930879593\n",
            "epoch: 3 step: 360, loss is 0.1087893471121788\n",
            "epoch: 3 step: 361, loss is 0.05307450145483017\n",
            "epoch: 3 step: 362, loss is 0.03378382325172424\n",
            "epoch: 3 step: 363, loss is 0.017876172438263893\n",
            "epoch: 3 step: 364, loss is 0.14559705555438995\n",
            "epoch: 3 step: 365, loss is 0.16039006412029266\n",
            "epoch: 3 step: 366, loss is 0.021713629364967346\n",
            "epoch: 3 step: 367, loss is 0.22908812761306763\n",
            "epoch: 3 step: 368, loss is 0.1701892912387848\n",
            "epoch: 3 step: 369, loss is 0.041942253708839417\n",
            "epoch: 3 step: 370, loss is 0.0797761082649231\n",
            "epoch: 3 step: 371, loss is 0.06516324728727341\n",
            "epoch: 3 step: 372, loss is 0.11815847456455231\n",
            "epoch: 3 step: 373, loss is 0.0656210407614708\n",
            "epoch: 3 step: 374, loss is 0.014886642806231976\n",
            "epoch: 3 step: 375, loss is 0.04216308519244194\n",
            "epoch: 3 step: 376, loss is 0.038307227194309235\n",
            "epoch: 3 step: 377, loss is 0.1509532630443573\n",
            "epoch: 3 step: 378, loss is 0.18452680110931396\n",
            "epoch: 3 step: 379, loss is 0.06600181758403778\n",
            "epoch: 3 step: 380, loss is 0.13624991476535797\n",
            "epoch: 3 step: 381, loss is 0.14238940179347992\n",
            "epoch: 3 step: 382, loss is 0.030292419716715813\n",
            "epoch: 3 step: 383, loss is 0.08970648050308228\n",
            "epoch: 3 step: 384, loss is 0.25472602248191833\n",
            "epoch: 3 step: 385, loss is 0.04991043731570244\n",
            "epoch: 3 step: 386, loss is 0.03651874139904976\n",
            "epoch: 3 step: 387, loss is 0.03966400772333145\n",
            "epoch: 3 step: 388, loss is 0.0683220848441124\n",
            "epoch: 3 step: 389, loss is 0.0188940167427063\n",
            "epoch: 3 step: 390, loss is 0.03745049238204956\n",
            "epoch: 3 step: 391, loss is 0.06675821542739868\n",
            "epoch: 3 step: 392, loss is 0.014518138952553272\n",
            "epoch: 3 step: 393, loss is 0.09479827433824539\n",
            "epoch: 3 step: 394, loss is 0.048629723489284515\n",
            "epoch: 3 step: 395, loss is 0.07210925221443176\n",
            "epoch: 3 step: 396, loss is 0.2986029088497162\n",
            "epoch: 3 step: 397, loss is 0.31154200434684753\n",
            "epoch: 3 step: 398, loss is 0.2258920669555664\n",
            "epoch: 3 step: 399, loss is 0.02252410724759102\n",
            "epoch: 3 step: 400, loss is 0.10260792076587677\n",
            "epoch: 3 step: 401, loss is 0.0926787257194519\n",
            "epoch: 3 step: 402, loss is 0.18981681764125824\n",
            "epoch: 3 step: 403, loss is 0.07488541305065155\n",
            "epoch: 3 step: 404, loss is 0.1267595738172531\n",
            "epoch: 3 step: 405, loss is 0.12506967782974243\n",
            "epoch: 3 step: 406, loss is 0.033188942819833755\n",
            "epoch: 3 step: 407, loss is 0.0651453360915184\n",
            "epoch: 3 step: 408, loss is 0.07583235949277878\n",
            "epoch: 3 step: 409, loss is 0.1890064924955368\n",
            "epoch: 3 step: 410, loss is 0.030513379722833633\n",
            "epoch: 3 step: 411, loss is 0.12683433294296265\n",
            "epoch: 3 step: 412, loss is 0.08314603567123413\n",
            "epoch: 3 step: 413, loss is 0.035213690251111984\n",
            "epoch: 3 step: 414, loss is 0.042642101645469666\n",
            "epoch: 3 step: 415, loss is 0.135378897190094\n",
            "epoch: 3 step: 416, loss is 0.14122159779071808\n",
            "epoch: 3 step: 417, loss is 0.11188754439353943\n",
            "epoch: 3 step: 418, loss is 0.03157028928399086\n",
            "epoch: 3 step: 419, loss is 0.05658462271094322\n",
            "epoch: 3 step: 420, loss is 0.045626308768987656\n",
            "epoch: 3 step: 421, loss is 0.02453869953751564\n",
            "epoch: 3 step: 422, loss is 0.1448953002691269\n",
            "epoch: 3 step: 423, loss is 0.06986171752214432\n",
            "epoch: 3 step: 424, loss is 0.07639969885349274\n",
            "epoch: 3 step: 425, loss is 0.19005972146987915\n",
            "epoch: 3 step: 426, loss is 0.044791292399168015\n",
            "epoch: 3 step: 427, loss is 0.045721203088760376\n",
            "epoch: 3 step: 428, loss is 0.03123042918741703\n",
            "epoch: 3 step: 429, loss is 0.03711636736989021\n",
            "epoch: 3 step: 430, loss is 0.15861783921718597\n",
            "epoch: 3 step: 431, loss is 0.03426811844110489\n",
            "epoch: 3 step: 432, loss is 0.12655696272850037\n",
            "epoch: 3 step: 433, loss is 0.04374212771654129\n",
            "epoch: 3 step: 434, loss is 0.10437465459108353\n",
            "epoch: 3 step: 435, loss is 0.040473949164152145\n",
            "epoch: 3 step: 436, loss is 0.07001027464866638\n",
            "epoch: 3 step: 437, loss is 0.10315681248903275\n",
            "epoch: 3 step: 438, loss is 0.12647224962711334\n",
            "epoch: 3 step: 439, loss is 0.03915414959192276\n",
            "epoch: 3 step: 440, loss is 0.05425269529223442\n",
            "epoch: 3 step: 441, loss is 0.05073948949575424\n",
            "epoch: 3 step: 442, loss is 0.07695290446281433\n",
            "epoch: 3 step: 443, loss is 0.16210734844207764\n",
            "epoch: 3 step: 444, loss is 0.028037143871188164\n",
            "epoch: 3 step: 445, loss is 0.015262089669704437\n",
            "epoch: 3 step: 446, loss is 0.10871235281229019\n",
            "epoch: 3 step: 447, loss is 0.02534102275967598\n",
            "epoch: 3 step: 448, loss is 0.19082006812095642\n",
            "epoch: 3 step: 449, loss is 0.05384659394621849\n",
            "epoch: 3 step: 450, loss is 0.14355888962745667\n",
            "epoch: 3 step: 451, loss is 0.028728080913424492\n",
            "epoch: 3 step: 452, loss is 0.0602133683860302\n",
            "epoch: 3 step: 453, loss is 0.05004296451807022\n",
            "epoch: 3 step: 454, loss is 0.023339375853538513\n",
            "epoch: 3 step: 455, loss is 0.03444599732756615\n",
            "epoch: 3 step: 456, loss is 0.32076141238212585\n",
            "epoch: 3 step: 457, loss is 0.19428764283657074\n",
            "epoch: 3 step: 458, loss is 0.07798430323600769\n",
            "epoch: 3 step: 459, loss is 0.15268248319625854\n",
            "epoch: 3 step: 460, loss is 0.1528329998254776\n",
            "epoch: 3 step: 461, loss is 0.11264390498399734\n",
            "epoch: 3 step: 462, loss is 0.02886633388698101\n",
            "epoch: 3 step: 463, loss is 0.04047245532274246\n",
            "epoch: 3 step: 464, loss is 0.07199828326702118\n",
            "epoch: 3 step: 465, loss is 0.14665430784225464\n",
            "epoch: 3 step: 466, loss is 0.028549503535032272\n",
            "epoch: 3 step: 467, loss is 0.027348987758159637\n",
            "epoch: 3 step: 468, loss is 0.22780142724514008\n",
            "epoch: 3 step: 469, loss is 0.06231774389743805\n",
            "epoch: 3 step: 470, loss is 0.035883329808712006\n",
            "epoch: 3 step: 471, loss is 0.08621733635663986\n",
            "epoch: 3 step: 472, loss is 0.04930153861641884\n",
            "epoch: 3 step: 473, loss is 0.019366910681128502\n",
            "epoch: 3 step: 474, loss is 0.019912229850888252\n",
            "epoch: 3 step: 475, loss is 0.05854972451925278\n",
            "epoch: 3 step: 476, loss is 0.216464102268219\n",
            "epoch: 3 step: 477, loss is 0.14147499203681946\n",
            "epoch: 3 step: 478, loss is 0.1436101347208023\n",
            "epoch: 3 step: 479, loss is 0.0402776338160038\n",
            "epoch: 3 step: 480, loss is 0.3371516466140747\n",
            "epoch: 3 step: 481, loss is 0.011689607985317707\n",
            "epoch: 3 step: 482, loss is 0.0512067973613739\n",
            "epoch: 3 step: 483, loss is 0.02738124690949917\n",
            "epoch: 3 step: 484, loss is 0.2937171459197998\n",
            "epoch: 3 step: 485, loss is 0.11121254414319992\n",
            "epoch: 3 step: 486, loss is 0.03436078503727913\n",
            "epoch: 3 step: 487, loss is 0.06442621350288391\n",
            "epoch: 3 step: 488, loss is 0.12838223576545715\n",
            "epoch: 3 step: 489, loss is 0.03980682045221329\n",
            "epoch: 3 step: 490, loss is 0.04310779273509979\n",
            "epoch: 3 step: 491, loss is 0.01441748347133398\n",
            "epoch: 3 step: 492, loss is 0.043125834316015244\n",
            "epoch: 3 step: 493, loss is 0.016765130683779716\n",
            "epoch: 3 step: 494, loss is 0.09687748551368713\n",
            "epoch: 3 step: 495, loss is 0.022418193519115448\n",
            "epoch: 3 step: 496, loss is 0.02869448810815811\n",
            "epoch: 3 step: 497, loss is 0.040697596967220306\n",
            "epoch: 3 step: 498, loss is 0.0418175645172596\n",
            "epoch: 3 step: 499, loss is 0.15198518335819244\n",
            "epoch: 3 step: 500, loss is 0.035116128623485565\n",
            "epoch: 3 step: 501, loss is 0.014489218592643738\n",
            "epoch: 3 step: 502, loss is 0.040084708482027054\n",
            "epoch: 3 step: 503, loss is 0.03168269991874695\n",
            "epoch: 3 step: 504, loss is 0.05254259333014488\n",
            "epoch: 3 step: 505, loss is 0.020071184262633324\n",
            "epoch: 3 step: 506, loss is 0.038596946746110916\n",
            "epoch: 3 step: 507, loss is 0.028119584545493126\n",
            "epoch: 3 step: 508, loss is 0.10378651320934296\n",
            "epoch: 3 step: 509, loss is 0.15020063519477844\n",
            "epoch: 3 step: 510, loss is 0.053092699497938156\n",
            "epoch: 3 step: 511, loss is 0.35079625248908997\n",
            "epoch: 3 step: 512, loss is 0.01816227100789547\n",
            "epoch: 3 step: 513, loss is 0.06271066516637802\n",
            "epoch: 3 step: 514, loss is 0.02504274807870388\n",
            "epoch: 3 step: 515, loss is 0.02922731265425682\n",
            "epoch: 3 step: 516, loss is 0.06283555179834366\n",
            "epoch: 3 step: 517, loss is 0.030824504792690277\n",
            "epoch: 3 step: 518, loss is 0.059000253677368164\n",
            "epoch: 3 step: 519, loss is 0.01717693917453289\n",
            "epoch: 3 step: 520, loss is 0.033358123153448105\n",
            "epoch: 3 step: 521, loss is 0.06798472255468369\n",
            "epoch: 3 step: 522, loss is 0.12542113661766052\n",
            "epoch: 3 step: 523, loss is 0.11332953721284866\n",
            "epoch: 3 step: 524, loss is 0.039258964359760284\n",
            "epoch: 3 step: 525, loss is 0.2809237837791443\n",
            "epoch: 3 step: 526, loss is 0.022174976766109467\n",
            "epoch: 3 step: 527, loss is 0.017194276675581932\n",
            "epoch: 3 step: 528, loss is 0.10354014486074448\n",
            "epoch: 3 step: 529, loss is 0.052909571677446365\n",
            "epoch: 3 step: 530, loss is 0.11143554002046585\n",
            "epoch: 3 step: 531, loss is 0.023328367620706558\n",
            "epoch: 3 step: 532, loss is 0.09959454089403152\n",
            "epoch: 3 step: 533, loss is 0.07847832888364792\n",
            "epoch: 3 step: 534, loss is 0.0681513100862503\n",
            "epoch: 3 step: 535, loss is 0.13780514895915985\n",
            "epoch: 3 step: 536, loss is 0.033698100596666336\n",
            "epoch: 3 step: 537, loss is 0.04807976260781288\n",
            "epoch: 3 step: 538, loss is 0.15649846196174622\n",
            "epoch: 3 step: 539, loss is 0.2658170163631439\n",
            "epoch: 3 step: 540, loss is 0.15627916157245636\n",
            "epoch: 3 step: 541, loss is 0.06026893109083176\n",
            "epoch: 3 step: 542, loss is 0.09422051906585693\n",
            "epoch: 3 step: 543, loss is 0.2145560085773468\n",
            "epoch: 3 step: 544, loss is 0.16823430359363556\n",
            "epoch: 3 step: 545, loss is 0.15335194766521454\n",
            "epoch: 3 step: 546, loss is 0.03302513808012009\n",
            "epoch: 3 step: 547, loss is 0.01774045079946518\n",
            "epoch: 3 step: 548, loss is 0.0394638255238533\n",
            "epoch: 3 step: 549, loss is 0.037772808223962784\n",
            "epoch: 3 step: 550, loss is 0.012036104686558247\n",
            "epoch: 3 step: 551, loss is 0.24181696772575378\n",
            "epoch: 3 step: 552, loss is 0.0485791377723217\n",
            "epoch: 3 step: 553, loss is 0.07962673902511597\n",
            "epoch: 3 step: 554, loss is 0.10050296038389206\n",
            "epoch: 3 step: 555, loss is 0.05272151902318001\n",
            "epoch: 3 step: 556, loss is 0.4877847731113434\n",
            "epoch: 3 step: 557, loss is 0.05295530706644058\n",
            "epoch: 3 step: 558, loss is 0.02944250777363777\n",
            "epoch: 3 step: 559, loss is 0.16313861310482025\n",
            "epoch: 3 step: 560, loss is 0.03952457755804062\n",
            "epoch: 3 step: 561, loss is 0.27516818046569824\n",
            "epoch: 3 step: 562, loss is 0.07561422884464264\n",
            "epoch: 3 step: 563, loss is 0.1224265992641449\n",
            "epoch: 3 step: 564, loss is 0.0468246228992939\n",
            "epoch: 3 step: 565, loss is 0.035595282912254333\n",
            "epoch: 3 step: 566, loss is 0.12195510417222977\n",
            "epoch: 3 step: 567, loss is 0.20776401460170746\n",
            "epoch: 3 step: 568, loss is 0.3009992241859436\n",
            "epoch: 3 step: 569, loss is 0.039234645664691925\n",
            "epoch: 3 step: 570, loss is 0.10014653205871582\n",
            "epoch: 3 step: 571, loss is 0.11922658979892731\n",
            "epoch: 3 step: 572, loss is 0.049307022243738174\n",
            "epoch: 3 step: 573, loss is 0.021839963272213936\n",
            "epoch: 3 step: 574, loss is 0.013407809659838676\n",
            "epoch: 3 step: 575, loss is 0.1870792806148529\n",
            "epoch: 3 step: 576, loss is 0.09815250337123871\n",
            "epoch: 3 step: 577, loss is 0.1024140790104866\n",
            "epoch: 3 step: 578, loss is 0.29944780468940735\n",
            "epoch: 3 step: 579, loss is 0.06244809553027153\n",
            "epoch: 3 step: 580, loss is 0.17153799533843994\n",
            "epoch: 3 step: 581, loss is 0.03221465274691582\n",
            "epoch: 3 step: 582, loss is 0.017508674412965775\n",
            "epoch: 3 step: 583, loss is 0.15277323126792908\n",
            "epoch: 3 step: 584, loss is 0.2557065486907959\n",
            "epoch: 3 step: 585, loss is 0.020022761076688766\n",
            "epoch: 3 step: 586, loss is 0.08918987214565277\n",
            "epoch: 3 step: 587, loss is 0.058630555868148804\n",
            "epoch: 3 step: 588, loss is 0.12422944605350494\n",
            "epoch: 3 step: 589, loss is 0.05920890346169472\n",
            "epoch: 3 step: 590, loss is 0.025885676965117455\n",
            "epoch: 3 step: 591, loss is 0.02815581113100052\n",
            "epoch: 3 step: 592, loss is 0.10183226317167282\n",
            "epoch: 3 step: 593, loss is 0.054021645337343216\n",
            "epoch: 3 step: 594, loss is 0.12088820338249207\n",
            "epoch: 3 step: 595, loss is 0.050263699144124985\n",
            "epoch: 3 step: 596, loss is 0.033703312277793884\n",
            "epoch: 3 step: 597, loss is 0.09016522765159607\n",
            "epoch: 3 step: 598, loss is 0.02712198719382286\n",
            "epoch: 3 step: 599, loss is 0.019869206473231316\n",
            "epoch: 3 step: 600, loss is 0.01812806725502014\n",
            "epoch: 3 step: 601, loss is 0.025212673470377922\n",
            "epoch: 3 step: 602, loss is 0.030622070655226707\n",
            "epoch: 3 step: 603, loss is 0.23373404145240784\n",
            "epoch: 3 step: 604, loss is 0.20613020658493042\n",
            "epoch: 3 step: 605, loss is 0.37552183866500854\n",
            "epoch: 3 step: 606, loss is 0.10067015141248703\n",
            "epoch: 3 step: 607, loss is 0.1011391282081604\n",
            "epoch: 3 step: 608, loss is 0.02525232546031475\n",
            "epoch: 3 step: 609, loss is 0.010686421766877174\n",
            "epoch: 3 step: 610, loss is 0.2627127170562744\n",
            "epoch: 3 step: 611, loss is 0.2208895981311798\n",
            "epoch: 3 step: 612, loss is 0.05910740792751312\n",
            "epoch: 3 step: 613, loss is 0.059851884841918945\n",
            "epoch: 3 step: 614, loss is 0.31518465280532837\n",
            "epoch: 3 step: 615, loss is 0.06590490788221359\n",
            "epoch: 3 step: 616, loss is 0.15613962709903717\n",
            "epoch: 3 step: 617, loss is 0.04676669463515282\n",
            "epoch: 3 step: 618, loss is 0.026241257786750793\n",
            "epoch: 3 step: 619, loss is 0.0877537801861763\n",
            "epoch: 3 step: 620, loss is 0.060599714517593384\n",
            "epoch: 3 step: 621, loss is 0.10268443822860718\n",
            "epoch: 3 step: 622, loss is 0.09967507421970367\n",
            "epoch: 3 step: 623, loss is 0.05311084911227226\n",
            "epoch: 3 step: 624, loss is 0.0335271842777729\n",
            "epoch: 3 step: 625, loss is 0.028361499309539795\n",
            "epoch: 3 step: 626, loss is 0.02832900919020176\n",
            "epoch: 3 step: 627, loss is 0.02263508550822735\n",
            "epoch: 3 step: 628, loss is 0.042772531509399414\n",
            "epoch: 3 step: 629, loss is 0.029517758637666702\n",
            "epoch: 3 step: 630, loss is 0.11083464324474335\n",
            "epoch: 3 step: 631, loss is 0.23644046485424042\n",
            "epoch: 3 step: 632, loss is 0.028326572850346565\n",
            "epoch: 3 step: 633, loss is 0.020156655460596085\n",
            "epoch: 3 step: 634, loss is 0.059656091034412384\n",
            "epoch: 3 step: 635, loss is 0.033583417534828186\n",
            "epoch: 3 step: 636, loss is 0.2184237688779831\n",
            "epoch: 3 step: 637, loss is 0.1883459836244583\n",
            "epoch: 3 step: 638, loss is 0.05225493758916855\n",
            "epoch: 3 step: 639, loss is 0.2924967408180237\n",
            "epoch: 3 step: 640, loss is 0.10463562607765198\n",
            "epoch: 3 step: 641, loss is 0.04759849235415459\n",
            "epoch: 3 step: 642, loss is 0.10068123787641525\n",
            "epoch: 3 step: 643, loss is 0.2296409010887146\n",
            "epoch: 3 step: 644, loss is 0.04378632828593254\n",
            "epoch: 3 step: 645, loss is 0.034053340554237366\n",
            "epoch: 3 step: 646, loss is 0.035929277539253235\n",
            "epoch: 3 step: 647, loss is 0.035829417407512665\n",
            "epoch: 3 step: 648, loss is 0.055304307490587234\n",
            "epoch: 3 step: 649, loss is 0.07645181566476822\n",
            "epoch: 3 step: 650, loss is 0.03584316372871399\n",
            "epoch: 3 step: 651, loss is 0.0572129562497139\n",
            "epoch: 3 step: 652, loss is 0.02673163264989853\n",
            "epoch: 3 step: 653, loss is 0.14053145051002502\n",
            "epoch: 3 step: 654, loss is 0.21612082421779633\n",
            "epoch: 3 step: 655, loss is 0.059315744787454605\n",
            "epoch: 3 step: 656, loss is 0.31219518184661865\n",
            "epoch: 3 step: 657, loss is 0.046232372522354126\n",
            "epoch: 3 step: 658, loss is 0.04203961417078972\n",
            "epoch: 3 step: 659, loss is 0.06870167702436447\n",
            "epoch: 3 step: 660, loss is 0.021006722003221512\n",
            "epoch: 3 step: 661, loss is 0.34356817603111267\n",
            "epoch: 3 step: 662, loss is 0.06085413321852684\n",
            "epoch: 3 step: 663, loss is 0.06553110480308533\n",
            "epoch: 3 step: 664, loss is 0.024497054517269135\n",
            "epoch: 3 step: 665, loss is 0.027592521160840988\n",
            "epoch: 3 step: 666, loss is 0.02198011986911297\n",
            "epoch: 3 step: 667, loss is 0.22522085905075073\n",
            "epoch: 3 step: 668, loss is 0.02687947265803814\n",
            "epoch: 3 step: 669, loss is 0.034647297114133835\n",
            "epoch: 3 step: 670, loss is 0.16446757316589355\n",
            "epoch: 3 step: 671, loss is 0.019831176847219467\n",
            "epoch: 3 step: 672, loss is 0.01167899090796709\n",
            "epoch: 3 step: 673, loss is 0.02231096662580967\n",
            "epoch: 3 step: 674, loss is 0.02880723774433136\n",
            "epoch: 3 step: 675, loss is 0.13329090178012848\n",
            "epoch: 3 step: 676, loss is 0.05552085116505623\n",
            "epoch: 3 step: 677, loss is 0.03233432397246361\n",
            "epoch: 3 step: 678, loss is 0.03959430381655693\n",
            "epoch: 3 step: 679, loss is 0.050386570394039154\n",
            "epoch: 3 step: 680, loss is 0.02509346790611744\n",
            "epoch: 3 step: 681, loss is 0.019762719050049782\n",
            "epoch: 3 step: 682, loss is 0.03092360869050026\n",
            "epoch: 3 step: 683, loss is 0.15626564621925354\n",
            "epoch: 3 step: 684, loss is 0.059606194496154785\n",
            "epoch: 3 step: 685, loss is 0.0996420830488205\n",
            "epoch: 3 step: 686, loss is 0.03482402488589287\n",
            "epoch: 3 step: 687, loss is 0.035114631056785583\n",
            "epoch: 3 step: 688, loss is 0.031817466020584106\n",
            "epoch: 3 step: 689, loss is 0.03830470144748688\n",
            "epoch: 3 step: 690, loss is 0.019926780834794044\n",
            "epoch: 3 step: 691, loss is 0.021236978471279144\n",
            "epoch: 3 step: 692, loss is 0.07065748423337936\n",
            "epoch: 3 step: 693, loss is 0.03824285417795181\n",
            "epoch: 3 step: 694, loss is 0.02849782630801201\n",
            "epoch: 3 step: 695, loss is 0.03533976897597313\n",
            "epoch: 3 step: 696, loss is 0.024048136547207832\n",
            "epoch: 3 step: 697, loss is 0.051949672400951385\n",
            "epoch: 3 step: 698, loss is 0.09256185591220856\n",
            "epoch: 3 step: 699, loss is 0.025675136595964432\n",
            "epoch: 3 step: 700, loss is 0.01704942062497139\n",
            "epoch: 3 step: 701, loss is 0.013969152234494686\n",
            "epoch: 3 step: 702, loss is 0.3802129626274109\n",
            "epoch: 3 step: 703, loss is 0.013700357638299465\n",
            "epoch: 3 step: 704, loss is 0.07028970122337341\n",
            "epoch: 3 step: 705, loss is 0.028518561273813248\n",
            "epoch: 3 step: 706, loss is 0.04299703985452652\n",
            "epoch: 3 step: 707, loss is 0.01824728213250637\n",
            "epoch: 3 step: 708, loss is 0.04503318667411804\n",
            "epoch: 3 step: 709, loss is 0.02176300808787346\n",
            "epoch: 3 step: 710, loss is 0.11759578436613083\n",
            "epoch: 3 step: 711, loss is 0.014893229119479656\n",
            "epoch: 3 step: 712, loss is 0.04185838624835014\n",
            "epoch: 3 step: 713, loss is 0.021452831104397774\n",
            "epoch: 3 step: 714, loss is 0.1172887533903122\n",
            "epoch: 3 step: 715, loss is 0.026395142078399658\n",
            "epoch: 3 step: 716, loss is 0.1492433398962021\n",
            "epoch: 3 step: 717, loss is 0.009102175012230873\n",
            "epoch: 3 step: 718, loss is 0.08122754096984863\n",
            "epoch: 3 step: 719, loss is 0.14021092653274536\n",
            "epoch: 3 step: 720, loss is 0.027392441406846046\n",
            "epoch: 3 step: 721, loss is 0.017808737233281136\n",
            "epoch: 3 step: 722, loss is 0.22918668389320374\n",
            "epoch: 3 step: 723, loss is 0.12075132876634598\n",
            "epoch: 3 step: 724, loss is 0.199569433927536\n",
            "epoch: 3 step: 725, loss is 0.02570606954395771\n",
            "epoch: 3 step: 726, loss is 0.20514561235904694\n",
            "epoch: 3 step: 727, loss is 0.08486591279506683\n",
            "epoch: 3 step: 728, loss is 0.03459756076335907\n",
            "epoch: 3 step: 729, loss is 0.040580492466688156\n",
            "epoch: 3 step: 730, loss is 0.16013337671756744\n",
            "epoch: 3 step: 731, loss is 0.16820402443408966\n",
            "epoch: 3 step: 732, loss is 0.3487306237220764\n",
            "epoch: 3 step: 733, loss is 0.020469333976507187\n",
            "epoch: 3 step: 734, loss is 0.10543768107891083\n",
            "epoch: 3 step: 735, loss is 0.0552690327167511\n",
            "epoch: 3 step: 736, loss is 0.04797220602631569\n",
            "epoch: 3 step: 737, loss is 0.0631643608212471\n",
            "epoch: 3 step: 738, loss is 0.05649944022297859\n",
            "epoch: 3 step: 739, loss is 0.016592297703027725\n",
            "epoch: 3 step: 740, loss is 0.03390311449766159\n",
            "epoch: 3 step: 741, loss is 0.11501268297433853\n",
            "epoch: 3 step: 742, loss is 0.062436383217573166\n",
            "epoch: 3 step: 743, loss is 0.05599700286984444\n",
            "epoch: 3 step: 744, loss is 0.1326383352279663\n",
            "epoch: 3 step: 745, loss is 0.02644423022866249\n",
            "epoch: 3 step: 746, loss is 0.040209975093603134\n",
            "epoch: 3 step: 747, loss is 0.01682613603770733\n",
            "epoch: 3 step: 748, loss is 0.01948780193924904\n",
            "epoch: 3 step: 749, loss is 0.13752131164073944\n",
            "epoch: 3 step: 750, loss is 0.03587977960705757\n",
            "epoch: 3 step: 751, loss is 0.025778232142329216\n",
            "epoch: 3 step: 752, loss is 0.06171629577875137\n",
            "epoch: 3 step: 753, loss is 0.07046715170145035\n",
            "epoch: 3 step: 754, loss is 0.0434437058866024\n",
            "epoch: 3 step: 755, loss is 0.04170208424329758\n",
            "epoch: 3 step: 756, loss is 0.023951593786478043\n",
            "epoch: 3 step: 757, loss is 0.014888332225382328\n",
            "epoch: 3 step: 758, loss is 0.02809472754597664\n",
            "epoch: 3 step: 759, loss is 0.028167635202407837\n",
            "epoch: 3 step: 760, loss is 0.15466777980327606\n",
            "epoch: 3 step: 761, loss is 0.06329202651977539\n",
            "epoch: 3 step: 762, loss is 0.02748078666627407\n",
            "epoch: 3 step: 763, loss is 0.012966237962245941\n",
            "epoch: 3 step: 764, loss is 0.06910703331232071\n",
            "epoch: 3 step: 765, loss is 0.020037246868014336\n",
            "epoch: 3 step: 766, loss is 0.018649769946932793\n",
            "epoch: 3 step: 767, loss is 0.0172110628336668\n",
            "epoch: 3 step: 768, loss is 0.21158243715763092\n",
            "epoch: 3 step: 769, loss is 0.013114853762090206\n",
            "epoch: 3 step: 770, loss is 0.025085952132940292\n",
            "epoch: 3 step: 771, loss is 0.013602808117866516\n",
            "epoch: 3 step: 772, loss is 0.029893895611166954\n",
            "epoch: 3 step: 773, loss is 0.019959762692451477\n",
            "epoch: 3 step: 774, loss is 0.03372057527303696\n",
            "epoch: 3 step: 775, loss is 0.017588865011930466\n",
            "epoch: 3 step: 776, loss is 0.033620208501815796\n",
            "epoch: 3 step: 777, loss is 0.12271302938461304\n",
            "epoch: 3 step: 778, loss is 0.23243482410907745\n",
            "epoch: 3 step: 779, loss is 0.01037150714546442\n",
            "epoch: 3 step: 780, loss is 0.016666265204548836\n",
            "epoch: 3 step: 781, loss is 0.02562388777732849\n",
            "epoch: 3 step: 782, loss is 0.060499146580696106\n",
            "epoch: 3 step: 783, loss is 0.021993057802319527\n",
            "epoch: 3 step: 784, loss is 0.015406261198222637\n",
            "epoch: 3 step: 785, loss is 0.013637964613735676\n",
            "epoch: 3 step: 786, loss is 0.02244945988059044\n",
            "epoch: 3 step: 787, loss is 0.02067500166594982\n",
            "epoch: 3 step: 788, loss is 0.14408299326896667\n",
            "epoch: 3 step: 789, loss is 0.10067842155694962\n",
            "epoch: 3 step: 790, loss is 0.24954092502593994\n",
            "epoch: 3 step: 791, loss is 0.10361743718385696\n",
            "epoch: 3 step: 792, loss is 0.008438456803560257\n",
            "epoch: 3 step: 793, loss is 0.09035471826791763\n",
            "epoch: 3 step: 794, loss is 0.025372996926307678\n",
            "epoch: 3 step: 795, loss is 0.31330904364585876\n",
            "epoch: 3 step: 796, loss is 0.1896650642156601\n",
            "epoch: 3 step: 797, loss is 0.024468006566166878\n",
            "epoch: 3 step: 798, loss is 0.01740407943725586\n",
            "epoch: 3 step: 799, loss is 0.0454561710357666\n",
            "epoch: 3 step: 800, loss is 0.0339384451508522\n",
            "epoch: 3 step: 801, loss is 0.01842394657433033\n",
            "epoch: 3 step: 802, loss is 0.016257137060165405\n",
            "epoch: 3 step: 803, loss is 0.024951031431555748\n",
            "epoch: 3 step: 804, loss is 0.03110247105360031\n",
            "epoch: 3 step: 805, loss is 0.016194039955735207\n",
            "epoch: 3 step: 806, loss is 0.1276249885559082\n",
            "epoch: 3 step: 807, loss is 0.07732594013214111\n",
            "epoch: 3 step: 808, loss is 0.032472241669893265\n",
            "epoch: 3 step: 809, loss is 0.01717047207057476\n",
            "epoch: 3 step: 810, loss is 0.020076798275113106\n",
            "epoch: 3 step: 811, loss is 0.04698044806718826\n",
            "epoch: 3 step: 812, loss is 0.025248805060982704\n",
            "epoch: 3 step: 813, loss is 0.1027253270149231\n",
            "epoch: 3 step: 814, loss is 0.03599835932254791\n",
            "epoch: 3 step: 815, loss is 0.02551453560590744\n",
            "epoch: 3 step: 816, loss is 0.041433557868003845\n",
            "epoch: 3 step: 817, loss is 0.023673133924603462\n",
            "epoch: 3 step: 818, loss is 0.02646566927433014\n",
            "epoch: 3 step: 819, loss is 0.025294756516814232\n",
            "epoch: 3 step: 820, loss is 0.010778969153761864\n",
            "epoch: 3 step: 821, loss is 0.012114780023694038\n",
            "epoch: 3 step: 822, loss is 0.05622877553105354\n",
            "epoch: 3 step: 823, loss is 0.08621796220541\n",
            "epoch: 3 step: 824, loss is 0.009552886709570885\n",
            "epoch: 3 step: 825, loss is 0.09464907646179199\n",
            "epoch: 3 step: 826, loss is 0.15803852677345276\n",
            "epoch: 3 step: 827, loss is 0.012485407292842865\n",
            "epoch: 3 step: 828, loss is 0.012214762158691883\n",
            "epoch: 3 step: 829, loss is 0.01883689872920513\n",
            "epoch: 3 step: 830, loss is 0.1682537943124771\n",
            "epoch: 3 step: 831, loss is 0.00805748626589775\n",
            "epoch: 3 step: 832, loss is 0.0135263716802001\n",
            "epoch: 3 step: 833, loss is 0.013671173714101315\n",
            "epoch: 3 step: 834, loss is 0.012216854840517044\n",
            "epoch: 3 step: 835, loss is 0.32200056314468384\n",
            "epoch: 3 step: 836, loss is 0.02774922549724579\n",
            "epoch: 3 step: 837, loss is 0.032534584403038025\n",
            "epoch: 3 step: 838, loss is 0.07999585568904877\n",
            "epoch: 3 step: 839, loss is 0.11493833363056183\n",
            "epoch: 3 step: 840, loss is 0.02354818396270275\n",
            "epoch: 3 step: 841, loss is 0.05903192237019539\n",
            "epoch: 3 step: 842, loss is 0.03768796846270561\n",
            "epoch: 3 step: 843, loss is 0.18625985085964203\n",
            "epoch: 3 step: 844, loss is 0.10839961469173431\n",
            "epoch: 3 step: 845, loss is 0.012965965084731579\n",
            "epoch: 3 step: 846, loss is 0.01476279180496931\n",
            "epoch: 3 step: 847, loss is 0.041121918708086014\n",
            "epoch: 3 step: 848, loss is 0.040333449840545654\n",
            "epoch: 3 step: 849, loss is 0.03036400116980076\n",
            "epoch: 3 step: 850, loss is 0.014868827536702156\n",
            "epoch: 3 step: 851, loss is 0.03193384036421776\n",
            "epoch: 3 step: 852, loss is 0.010429080575704575\n",
            "epoch: 3 step: 853, loss is 0.027030406519770622\n",
            "epoch: 3 step: 854, loss is 0.027868757024407387\n",
            "epoch: 3 step: 855, loss is 0.011511566117405891\n",
            "epoch: 3 step: 856, loss is 0.01472483854740858\n",
            "epoch: 3 step: 857, loss is 0.22024591267108917\n",
            "epoch: 3 step: 858, loss is 0.007649723440408707\n",
            "epoch: 3 step: 859, loss is 0.02715172991156578\n",
            "epoch: 3 step: 860, loss is 0.021170079708099365\n",
            "epoch: 3 step: 861, loss is 0.033716846257448196\n",
            "epoch: 3 step: 862, loss is 0.09274406731128693\n",
            "epoch: 3 step: 863, loss is 0.0191776342689991\n",
            "epoch: 3 step: 864, loss is 0.5912829041481018\n",
            "epoch: 3 step: 865, loss is 0.10246240347623825\n",
            "epoch: 3 step: 866, loss is 0.007002821657806635\n",
            "epoch: 3 step: 867, loss is 0.02552768774330616\n",
            "epoch: 3 step: 868, loss is 0.013488538563251495\n",
            "epoch: 3 step: 869, loss is 0.014148801565170288\n",
            "epoch: 3 step: 870, loss is 0.009344365447759628\n",
            "epoch: 3 step: 871, loss is 0.4032697081565857\n",
            "epoch: 3 step: 872, loss is 0.08834750205278397\n",
            "epoch: 3 step: 873, loss is 0.1073014885187149\n",
            "epoch: 3 step: 874, loss is 0.09794558584690094\n",
            "epoch: 3 step: 875, loss is 0.22794228792190552\n",
            "epoch: 3 step: 876, loss is 0.054332464933395386\n",
            "epoch: 3 step: 877, loss is 0.19907240569591522\n",
            "epoch: 3 step: 878, loss is 0.038297444581985474\n",
            "epoch: 3 step: 879, loss is 0.043479789048433304\n",
            "epoch: 3 step: 880, loss is 0.18948476016521454\n",
            "epoch: 3 step: 881, loss is 0.018219860270619392\n",
            "epoch: 3 step: 882, loss is 0.11339417099952698\n",
            "epoch: 3 step: 883, loss is 0.32131677865982056\n",
            "epoch: 3 step: 884, loss is 0.2098812460899353\n",
            "epoch: 3 step: 885, loss is 0.04408164322376251\n",
            "epoch: 3 step: 886, loss is 0.03525872901082039\n",
            "epoch: 3 step: 887, loss is 0.10674067586660385\n",
            "epoch: 3 step: 888, loss is 0.21562500298023224\n",
            "epoch: 3 step: 889, loss is 0.030258843675255775\n",
            "epoch: 3 step: 890, loss is 0.1948566436767578\n",
            "epoch: 3 step: 891, loss is 0.10413926094770432\n",
            "epoch: 3 step: 892, loss is 0.030879708006978035\n",
            "epoch: 3 step: 893, loss is 0.06374196708202362\n",
            "epoch: 3 step: 894, loss is 0.2779930531978607\n",
            "epoch: 3 step: 895, loss is 0.12099418044090271\n",
            "epoch: 3 step: 896, loss is 0.041140951216220856\n",
            "epoch: 3 step: 897, loss is 0.09250741451978683\n",
            "epoch: 3 step: 898, loss is 0.026243343949317932\n",
            "epoch: 3 step: 899, loss is 0.011361679062247276\n",
            "epoch: 3 step: 900, loss is 0.042984459549188614\n",
            "epoch: 3 step: 901, loss is 0.14986461400985718\n",
            "epoch: 3 step: 902, loss is 0.20675772428512573\n",
            "epoch: 3 step: 903, loss is 0.2652110755443573\n",
            "epoch: 3 step: 904, loss is 0.026953451335430145\n",
            "epoch: 3 step: 905, loss is 0.015413643792271614\n",
            "epoch: 3 step: 906, loss is 0.01693532057106495\n",
            "epoch: 3 step: 907, loss is 0.03697001934051514\n",
            "epoch: 3 step: 908, loss is 0.0381612703204155\n",
            "epoch: 3 step: 909, loss is 0.017521439120173454\n",
            "epoch: 3 step: 910, loss is 0.02117776870727539\n",
            "epoch: 3 step: 911, loss is 0.06402214616537094\n",
            "epoch: 3 step: 912, loss is 0.01837228611111641\n",
            "epoch: 3 step: 913, loss is 0.013698514550924301\n",
            "epoch: 3 step: 914, loss is 0.08370558172464371\n",
            "epoch: 3 step: 915, loss is 0.06677408516407013\n",
            "epoch: 3 step: 916, loss is 0.14157909154891968\n",
            "epoch: 3 step: 917, loss is 0.20350240170955658\n",
            "epoch: 3 step: 918, loss is 0.015439017675817013\n",
            "epoch: 3 step: 919, loss is 0.040343888103961945\n",
            "epoch: 3 step: 920, loss is 0.008741160854697227\n",
            "epoch: 3 step: 921, loss is 0.029218560084700584\n",
            "epoch: 3 step: 922, loss is 0.007862946018576622\n",
            "epoch: 3 step: 923, loss is 0.017371511086821556\n",
            "epoch: 3 step: 924, loss is 0.027492063120007515\n",
            "epoch: 3 step: 925, loss is 0.08547145128250122\n",
            "epoch: 3 step: 926, loss is 0.23121412098407745\n",
            "epoch: 3 step: 927, loss is 0.037351105362176895\n",
            "epoch: 3 step: 928, loss is 0.01975817233324051\n",
            "epoch: 3 step: 929, loss is 0.05553249269723892\n",
            "epoch: 3 step: 930, loss is 0.03744684159755707\n",
            "epoch: 3 step: 931, loss is 0.023053718730807304\n",
            "epoch: 3 step: 932, loss is 0.009148238226771355\n",
            "epoch: 3 step: 933, loss is 0.2952788472175598\n",
            "epoch: 3 step: 934, loss is 0.05224741995334625\n",
            "epoch: 3 step: 935, loss is 0.09055846929550171\n",
            "epoch: 3 step: 936, loss is 0.02210397645831108\n",
            "epoch: 3 step: 937, loss is 0.11213705688714981\n",
            "epoch: 3 step: 938, loss is 0.10959263145923615\n",
            "epoch: 3 step: 939, loss is 0.027593068778514862\n",
            "epoch: 3 step: 940, loss is 0.016969673335552216\n",
            "epoch: 3 step: 941, loss is 0.027357740327715874\n",
            "epoch: 3 step: 942, loss is 0.01939093880355358\n",
            "epoch: 3 step: 943, loss is 0.04406753182411194\n",
            "epoch: 3 step: 944, loss is 0.015537993051111698\n",
            "epoch: 3 step: 945, loss is 0.010514226742088795\n",
            "epoch: 3 step: 946, loss is 0.08051168918609619\n",
            "epoch: 3 step: 947, loss is 0.0486583486199379\n",
            "epoch: 3 step: 948, loss is 0.043633345514535904\n",
            "epoch: 3 step: 949, loss is 0.016124527901411057\n",
            "epoch: 3 step: 950, loss is 0.05133602395653725\n",
            "epoch: 3 step: 951, loss is 0.03934137150645256\n",
            "epoch: 3 step: 952, loss is 0.2641242742538452\n",
            "epoch: 3 step: 953, loss is 0.2685604393482208\n",
            "epoch: 3 step: 954, loss is 0.046355970203876495\n",
            "epoch: 3 step: 955, loss is 0.02986469864845276\n",
            "epoch: 3 step: 956, loss is 0.01606201007962227\n",
            "epoch: 3 step: 957, loss is 0.020917203277349472\n",
            "epoch: 3 step: 958, loss is 0.02867228537797928\n",
            "epoch: 3 step: 959, loss is 0.0737476572394371\n",
            "epoch: 3 step: 960, loss is 0.03226039931178093\n",
            "epoch: 3 step: 961, loss is 0.024254590272903442\n",
            "epoch: 3 step: 962, loss is 0.03348586708307266\n",
            "epoch: 3 step: 963, loss is 0.14401021599769592\n",
            "epoch: 3 step: 964, loss is 0.1808973103761673\n",
            "epoch: 3 step: 965, loss is 0.023522933945059776\n",
            "epoch: 3 step: 966, loss is 0.056962136179208755\n",
            "epoch: 3 step: 967, loss is 0.018377892673015594\n",
            "epoch: 3 step: 968, loss is 0.08404510468244553\n",
            "epoch: 3 step: 969, loss is 0.025847025215625763\n",
            "epoch: 3 step: 970, loss is 0.0713125541806221\n",
            "epoch: 3 step: 971, loss is 0.03677801787853241\n",
            "epoch: 3 step: 972, loss is 0.12245707213878632\n",
            "epoch: 3 step: 973, loss is 0.03131023421883583\n",
            "epoch: 3 step: 974, loss is 0.03943607211112976\n",
            "epoch: 3 step: 975, loss is 0.048189420253038406\n",
            "epoch: 3 step: 976, loss is 0.029431352391839027\n",
            "epoch: 3 step: 977, loss is 0.03668273240327835\n",
            "epoch: 3 step: 978, loss is 0.017515145242214203\n",
            "epoch: 3 step: 979, loss is 0.02487228810787201\n",
            "epoch: 3 step: 980, loss is 0.0733749121427536\n",
            "epoch: 3 step: 981, loss is 0.018724696710705757\n",
            "epoch: 3 step: 982, loss is 0.044689349830150604\n",
            "epoch: 3 step: 983, loss is 0.012086210772395134\n",
            "epoch: 3 step: 984, loss is 0.04255499690771103\n",
            "epoch: 3 step: 985, loss is 0.016160964965820312\n",
            "epoch: 3 step: 986, loss is 0.046862173825502396\n",
            "epoch: 3 step: 987, loss is 0.020934851840138435\n",
            "epoch: 3 step: 988, loss is 0.07238580286502838\n",
            "epoch: 3 step: 989, loss is 0.04425480589270592\n",
            "epoch: 3 step: 990, loss is 0.13376197218894958\n",
            "epoch: 3 step: 991, loss is 0.026402398943901062\n",
            "epoch: 3 step: 992, loss is 0.018372876569628716\n",
            "epoch: 3 step: 993, loss is 0.051825881004333496\n",
            "epoch: 3 step: 994, loss is 0.06380271911621094\n",
            "epoch: 3 step: 995, loss is 0.022782286629080772\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WARNING] ME(1671:136248148721664,MainProcess):2025-04-19-15:40:58.831.000 [mindspore/train/callback/_early_stop.py:221] Early stopping is conditioned on accuracy, which is not available. Available choices are: {'loss'}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 4 step: 1, loss is 0.14326030015945435\n",
            "epoch: 4 step: 2, loss is 0.06556695699691772\n",
            "epoch: 4 step: 3, loss is 0.02545430138707161\n",
            "epoch: 4 step: 4, loss is 0.034942466765642166\n",
            "epoch: 4 step: 5, loss is 0.030570806935429573\n",
            "epoch: 4 step: 6, loss is 0.1527153104543686\n",
            "epoch: 4 step: 7, loss is 0.06749889999628067\n",
            "epoch: 4 step: 8, loss is 0.03846844658255577\n",
            "epoch: 4 step: 9, loss is 0.008204332552850246\n",
            "epoch: 4 step: 10, loss is 0.008146343752741814\n",
            "epoch: 4 step: 11, loss is 0.03130723536014557\n",
            "epoch: 4 step: 12, loss is 0.03214151784777641\n",
            "epoch: 4 step: 13, loss is 0.05656413361430168\n",
            "epoch: 4 step: 14, loss is 0.19077695906162262\n",
            "epoch: 4 step: 15, loss is 0.18567226827144623\n",
            "epoch: 4 step: 16, loss is 0.1653951108455658\n",
            "epoch: 4 step: 17, loss is 0.04689908027648926\n",
            "epoch: 4 step: 18, loss is 0.01780650019645691\n",
            "epoch: 4 step: 19, loss is 0.03310846909880638\n",
            "epoch: 4 step: 20, loss is 0.014331369660794735\n",
            "epoch: 4 step: 21, loss is 0.07002156227827072\n",
            "epoch: 4 step: 22, loss is 0.015790225937962532\n",
            "epoch: 4 step: 23, loss is 0.054814111441373825\n",
            "epoch: 4 step: 24, loss is 0.013749941252171993\n",
            "epoch: 4 step: 25, loss is 0.014211568981409073\n",
            "epoch: 4 step: 26, loss is 0.02452280931174755\n",
            "epoch: 4 step: 27, loss is 0.11747374385595322\n",
            "epoch: 4 step: 28, loss is 0.03342930227518082\n",
            "epoch: 4 step: 29, loss is 0.011499098502099514\n",
            "epoch: 4 step: 30, loss is 0.058361589908599854\n",
            "epoch: 4 step: 31, loss is 0.02100883610546589\n",
            "epoch: 4 step: 32, loss is 0.21582750976085663\n",
            "epoch: 4 step: 33, loss is 0.1877308338880539\n",
            "epoch: 4 step: 34, loss is 0.25254106521606445\n",
            "epoch: 4 step: 35, loss is 0.04701429232954979\n",
            "epoch: 4 step: 36, loss is 0.24612224102020264\n",
            "epoch: 4 step: 37, loss is 0.04072590544819832\n",
            "epoch: 4 step: 38, loss is 0.07763654738664627\n",
            "epoch: 4 step: 39, loss is 0.09129392355680466\n",
            "epoch: 4 step: 40, loss is 0.028218531981110573\n",
            "epoch: 4 step: 41, loss is 0.022001249715685844\n",
            "epoch: 4 step: 42, loss is 0.03004407323896885\n",
            "epoch: 4 step: 43, loss is 0.03915296867489815\n",
            "epoch: 4 step: 44, loss is 0.04405928775668144\n",
            "epoch: 4 step: 45, loss is 0.04879816621541977\n",
            "epoch: 4 step: 46, loss is 0.035429105162620544\n",
            "epoch: 4 step: 47, loss is 0.015882989391684532\n",
            "epoch: 4 step: 48, loss is 0.020476913079619408\n",
            "epoch: 4 step: 49, loss is 0.02992517128586769\n",
            "epoch: 4 step: 50, loss is 0.11445974558591843\n",
            "epoch: 4 step: 51, loss is 0.06142131984233856\n",
            "epoch: 4 step: 52, loss is 0.06628011167049408\n",
            "epoch: 4 step: 53, loss is 0.07534079253673553\n",
            "epoch: 4 step: 54, loss is 0.07016279548406601\n",
            "epoch: 4 step: 55, loss is 0.16752883791923523\n",
            "epoch: 4 step: 56, loss is 0.024766458198428154\n",
            "epoch: 4 step: 57, loss is 0.0578351765871048\n",
            "epoch: 4 step: 58, loss is 0.06643521040678024\n",
            "epoch: 4 step: 59, loss is 0.025189517065882683\n",
            "epoch: 4 step: 60, loss is 0.013879358768463135\n",
            "epoch: 4 step: 61, loss is 0.10428662598133087\n",
            "epoch: 4 step: 62, loss is 0.06581352651119232\n",
            "epoch: 4 step: 63, loss is 0.02285483106970787\n",
            "epoch: 4 step: 64, loss is 0.022721437737345695\n",
            "epoch: 4 step: 65, loss is 0.012081735767424107\n",
            "epoch: 4 step: 66, loss is 0.04094593971967697\n",
            "epoch: 4 step: 67, loss is 0.06327693164348602\n",
            "epoch: 4 step: 68, loss is 0.17025333642959595\n",
            "epoch: 4 step: 69, loss is 0.05329789221286774\n",
            "epoch: 4 step: 70, loss is 0.09992609173059464\n",
            "epoch: 4 step: 71, loss is 0.06694246828556061\n",
            "epoch: 4 step: 72, loss is 0.12344501167535782\n",
            "epoch: 4 step: 73, loss is 0.03452884405851364\n",
            "epoch: 4 step: 74, loss is 0.06610559672117233\n",
            "epoch: 4 step: 75, loss is 0.01985916495323181\n",
            "epoch: 4 step: 76, loss is 0.03427577018737793\n",
            "epoch: 4 step: 77, loss is 0.15544401109218597\n",
            "epoch: 4 step: 78, loss is 0.016325095668435097\n",
            "epoch: 4 step: 79, loss is 0.04538052901625633\n",
            "epoch: 4 step: 80, loss is 0.046003859490156174\n",
            "epoch: 4 step: 81, loss is 0.0344339981675148\n",
            "epoch: 4 step: 82, loss is 0.02072395570576191\n",
            "epoch: 4 step: 83, loss is 0.23464538156986237\n",
            "epoch: 4 step: 84, loss is 0.017544295638799667\n",
            "epoch: 4 step: 85, loss is 0.0905805304646492\n",
            "epoch: 4 step: 86, loss is 0.12900084257125854\n",
            "epoch: 4 step: 87, loss is 0.033018652349710464\n",
            "epoch: 4 step: 88, loss is 0.19899290800094604\n",
            "epoch: 4 step: 89, loss is 0.01757289655506611\n",
            "epoch: 4 step: 90, loss is 0.018506567925214767\n",
            "epoch: 4 step: 91, loss is 0.1646043211221695\n",
            "epoch: 4 step: 92, loss is 0.021255923435091972\n",
            "epoch: 4 step: 93, loss is 0.05077100172638893\n",
            "epoch: 4 step: 94, loss is 0.20959612727165222\n",
            "epoch: 4 step: 95, loss is 0.008551317267119884\n",
            "epoch: 4 step: 96, loss is 0.01952570490539074\n",
            "epoch: 4 step: 97, loss is 0.10044407099485397\n",
            "epoch: 4 step: 98, loss is 0.05891568958759308\n",
            "epoch: 4 step: 99, loss is 0.21578216552734375\n",
            "epoch: 4 step: 100, loss is 0.016881845891475677\n",
            "epoch: 4 step: 101, loss is 0.015638256445527077\n",
            "epoch: 4 step: 102, loss is 0.018789920955896378\n",
            "epoch: 4 step: 103, loss is 0.029994234442710876\n",
            "epoch: 4 step: 104, loss is 0.02893630787730217\n",
            "epoch: 4 step: 105, loss is 0.02594875916838646\n",
            "epoch: 4 step: 106, loss is 0.23876281082630157\n",
            "epoch: 4 step: 107, loss is 0.012580453418195248\n",
            "epoch: 4 step: 108, loss is 0.03895635902881622\n",
            "epoch: 4 step: 109, loss is 0.1169145330786705\n",
            "epoch: 4 step: 110, loss is 0.009167342446744442\n",
            "epoch: 4 step: 111, loss is 0.012401103042066097\n",
            "epoch: 4 step: 112, loss is 0.0323600247502327\n",
            "epoch: 4 step: 113, loss is 0.023289654403924942\n",
            "epoch: 4 step: 114, loss is 0.19102458655834198\n",
            "epoch: 4 step: 115, loss is 0.029126321896910667\n",
            "epoch: 4 step: 116, loss is 0.03220951184630394\n",
            "epoch: 4 step: 117, loss is 0.1471167802810669\n",
            "epoch: 4 step: 118, loss is 0.07563954591751099\n",
            "epoch: 4 step: 119, loss is 0.016141630709171295\n",
            "epoch: 4 step: 120, loss is 0.08255750685930252\n",
            "epoch: 4 step: 121, loss is 0.01772361248731613\n",
            "epoch: 4 step: 122, loss is 0.029107972979545593\n",
            "epoch: 4 step: 123, loss is 0.018246077001094818\n",
            "epoch: 4 step: 124, loss is 0.13312022387981415\n",
            "epoch: 4 step: 125, loss is 0.03924735262989998\n",
            "epoch: 4 step: 126, loss is 0.014635736122727394\n",
            "epoch: 4 step: 127, loss is 0.014716459438204765\n",
            "epoch: 4 step: 128, loss is 0.06627438962459564\n",
            "epoch: 4 step: 129, loss is 0.058668479323387146\n",
            "epoch: 4 step: 130, loss is 0.07516369223594666\n",
            "epoch: 4 step: 131, loss is 0.07917555421590805\n",
            "epoch: 4 step: 132, loss is 0.03825680539011955\n",
            "epoch: 4 step: 133, loss is 0.014902839437127113\n",
            "epoch: 4 step: 134, loss is 0.03862850368022919\n",
            "epoch: 4 step: 135, loss is 0.013190800324082375\n",
            "epoch: 4 step: 136, loss is 0.012256409041583538\n",
            "epoch: 4 step: 137, loss is 0.023400215432047844\n",
            "epoch: 4 step: 138, loss is 0.02445092797279358\n",
            "epoch: 4 step: 139, loss is 0.24374748766422272\n",
            "epoch: 4 step: 140, loss is 0.09521955996751785\n",
            "epoch: 4 step: 141, loss is 0.2422737330198288\n",
            "epoch: 4 step: 142, loss is 0.10255394130945206\n",
            "epoch: 4 step: 143, loss is 0.06723404675722122\n",
            "epoch: 4 step: 144, loss is 0.012766236439347267\n",
            "epoch: 4 step: 145, loss is 0.013151703402400017\n",
            "epoch: 4 step: 146, loss is 0.2891532778739929\n",
            "epoch: 4 step: 147, loss is 0.019478637725114822\n",
            "epoch: 4 step: 148, loss is 0.019296759739518166\n",
            "epoch: 4 step: 149, loss is 0.007890217937529087\n",
            "epoch: 4 step: 150, loss is 0.058552805334329605\n",
            "epoch: 4 step: 151, loss is 0.011266738176345825\n",
            "epoch: 4 step: 152, loss is 0.008964727632701397\n",
            "epoch: 4 step: 153, loss is 0.021329496055841446\n",
            "epoch: 4 step: 154, loss is 0.004996137693524361\n",
            "epoch: 4 step: 155, loss is 0.11367478966712952\n",
            "epoch: 4 step: 156, loss is 0.0281013585627079\n",
            "epoch: 4 step: 157, loss is 0.01397187914699316\n",
            "epoch: 4 step: 158, loss is 0.015301046893000603\n",
            "epoch: 4 step: 159, loss is 0.03897839039564133\n",
            "epoch: 4 step: 160, loss is 0.024876581504940987\n",
            "epoch: 4 step: 161, loss is 0.01925618201494217\n",
            "epoch: 4 step: 162, loss is 0.1720479428768158\n",
            "epoch: 4 step: 163, loss is 0.017368096858263016\n",
            "epoch: 4 step: 164, loss is 0.016434302553534508\n",
            "epoch: 4 step: 165, loss is 0.010645006783306599\n",
            "epoch: 4 step: 166, loss is 0.24479058384895325\n",
            "epoch: 4 step: 167, loss is 0.009591764770448208\n",
            "epoch: 4 step: 168, loss is 0.04782930389046669\n",
            "epoch: 4 step: 169, loss is 0.018910428509116173\n",
            "epoch: 4 step: 170, loss is 0.05250559002161026\n",
            "epoch: 4 step: 171, loss is 0.009023960679769516\n",
            "epoch: 4 step: 172, loss is 0.1265835464000702\n",
            "epoch: 4 step: 173, loss is 0.023880841210484505\n",
            "epoch: 4 step: 174, loss is 0.16453643143177032\n",
            "epoch: 4 step: 175, loss is 0.03159863129258156\n",
            "epoch: 4 step: 176, loss is 0.049426622688770294\n",
            "epoch: 4 step: 177, loss is 0.01696154847741127\n",
            "epoch: 4 step: 178, loss is 0.018199589103460312\n",
            "epoch: 4 step: 179, loss is 0.022412480786442757\n",
            "epoch: 4 step: 180, loss is 0.01734321564435959\n",
            "epoch: 4 step: 181, loss is 0.03155228868126869\n",
            "epoch: 4 step: 182, loss is 0.02542739547789097\n",
            "epoch: 4 step: 183, loss is 0.02157456986606121\n",
            "epoch: 4 step: 184, loss is 0.012691491283476353\n",
            "epoch: 4 step: 185, loss is 0.0293037798255682\n",
            "epoch: 4 step: 186, loss is 0.26040634512901306\n",
            "epoch: 4 step: 187, loss is 0.024834899231791496\n",
            "epoch: 4 step: 188, loss is 0.01002867054194212\n",
            "epoch: 4 step: 189, loss is 0.2328444868326187\n",
            "epoch: 4 step: 190, loss is 0.007184564135968685\n",
            "epoch: 4 step: 191, loss is 0.01768594980239868\n",
            "epoch: 4 step: 192, loss is 0.04138379171490669\n",
            "epoch: 4 step: 193, loss is 0.014952180907130241\n",
            "epoch: 4 step: 194, loss is 0.06364347785711288\n",
            "epoch: 4 step: 195, loss is 0.020692843943834305\n",
            "epoch: 4 step: 196, loss is 0.025987938046455383\n",
            "epoch: 4 step: 197, loss is 0.012542294338345528\n",
            "epoch: 4 step: 198, loss is 0.041137438267469406\n",
            "epoch: 4 step: 199, loss is 0.04628425091505051\n",
            "epoch: 4 step: 200, loss is 0.16213586926460266\n",
            "epoch: 4 step: 201, loss is 0.07154405117034912\n",
            "epoch: 4 step: 202, loss is 0.02791883610188961\n",
            "epoch: 4 step: 203, loss is 0.007649876642972231\n",
            "epoch: 4 step: 204, loss is 0.012572371400892735\n",
            "epoch: 4 step: 205, loss is 0.0920196920633316\n",
            "epoch: 4 step: 206, loss is 0.0726495236158371\n",
            "epoch: 4 step: 207, loss is 0.07473944127559662\n",
            "epoch: 4 step: 208, loss is 0.015527259558439255\n",
            "epoch: 4 step: 209, loss is 0.01235121488571167\n",
            "epoch: 4 step: 210, loss is 0.010246971622109413\n",
            "epoch: 4 step: 211, loss is 0.032477833330631256\n",
            "epoch: 4 step: 212, loss is 0.014854216016829014\n",
            "epoch: 4 step: 213, loss is 0.3805047869682312\n",
            "epoch: 4 step: 214, loss is 0.1818387508392334\n",
            "epoch: 4 step: 215, loss is 0.015558125451207161\n",
            "epoch: 4 step: 216, loss is 0.006446395069360733\n",
            "epoch: 4 step: 217, loss is 0.008946653455495834\n",
            "epoch: 4 step: 218, loss is 0.021243583410978317\n",
            "epoch: 4 step: 219, loss is 0.02199637144804001\n",
            "epoch: 4 step: 220, loss is 0.08554707467556\n",
            "epoch: 4 step: 221, loss is 0.013243084773421288\n",
            "epoch: 4 step: 222, loss is 0.023848768323659897\n",
            "epoch: 4 step: 223, loss is 0.022679749876260757\n",
            "epoch: 4 step: 224, loss is 0.012454601936042309\n",
            "epoch: 4 step: 225, loss is 0.05796307697892189\n",
            "epoch: 4 step: 226, loss is 0.4881029427051544\n",
            "epoch: 4 step: 227, loss is 0.013590555638074875\n",
            "epoch: 4 step: 228, loss is 0.037065308541059494\n",
            "epoch: 4 step: 229, loss is 0.02439829520881176\n",
            "epoch: 4 step: 230, loss is 0.005212840158492327\n",
            "epoch: 4 step: 231, loss is 0.09477298706769943\n",
            "epoch: 4 step: 232, loss is 0.01952802576124668\n",
            "epoch: 4 step: 233, loss is 0.014499649405479431\n",
            "epoch: 4 step: 234, loss is 0.098020538687706\n",
            "epoch: 4 step: 235, loss is 0.023498525843024254\n",
            "epoch: 4 step: 236, loss is 0.012926207855343819\n",
            "epoch: 4 step: 237, loss is 0.014852574095129967\n",
            "epoch: 4 step: 238, loss is 0.04200657457113266\n",
            "epoch: 4 step: 239, loss is 0.0941411480307579\n",
            "epoch: 4 step: 240, loss is 0.05782675743103027\n",
            "epoch: 4 step: 241, loss is 0.014348655939102173\n",
            "epoch: 4 step: 242, loss is 0.012983610853552818\n",
            "epoch: 4 step: 243, loss is 0.028152819722890854\n",
            "epoch: 4 step: 244, loss is 0.07763369381427765\n",
            "epoch: 4 step: 245, loss is 0.04365541785955429\n",
            "epoch: 4 step: 246, loss is 0.021539080888032913\n",
            "epoch: 4 step: 247, loss is 0.013288636691868305\n",
            "epoch: 4 step: 248, loss is 0.020622700452804565\n",
            "epoch: 4 step: 249, loss is 0.06003280356526375\n",
            "epoch: 4 step: 250, loss is 0.02336588315665722\n",
            "epoch: 4 step: 251, loss is 0.04533690586686134\n",
            "epoch: 4 step: 252, loss is 0.01734469085931778\n",
            "epoch: 4 step: 253, loss is 0.1282731592655182\n",
            "epoch: 4 step: 254, loss is 0.01665017381310463\n",
            "epoch: 4 step: 255, loss is 0.019251158460974693\n",
            "epoch: 4 step: 256, loss is 0.1591608226299286\n",
            "epoch: 4 step: 257, loss is 0.05770460143685341\n",
            "epoch: 4 step: 258, loss is 0.01258857548236847\n",
            "epoch: 4 step: 259, loss is 0.02666807733476162\n",
            "epoch: 4 step: 260, loss is 0.02095545269548893\n",
            "epoch: 4 step: 261, loss is 0.007687828037887812\n",
            "epoch: 4 step: 262, loss is 0.014342334121465683\n",
            "epoch: 4 step: 263, loss is 0.027439821511507034\n",
            "epoch: 4 step: 264, loss is 0.05175888538360596\n",
            "epoch: 4 step: 265, loss is 0.014153281226754189\n",
            "epoch: 4 step: 266, loss is 0.014322499744594097\n",
            "epoch: 4 step: 267, loss is 0.010463541373610497\n",
            "epoch: 4 step: 268, loss is 0.07072684168815613\n",
            "epoch: 4 step: 269, loss is 0.03598668426275253\n",
            "epoch: 4 step: 270, loss is 0.01808362454175949\n",
            "epoch: 4 step: 271, loss is 0.037838295102119446\n",
            "epoch: 4 step: 272, loss is 0.030158646404743195\n",
            "epoch: 4 step: 273, loss is 0.018471162766218185\n",
            "epoch: 4 step: 274, loss is 0.013142136856913567\n",
            "epoch: 4 step: 275, loss is 0.12431052327156067\n",
            "epoch: 4 step: 276, loss is 0.015487810596823692\n",
            "epoch: 4 step: 277, loss is 0.022207828238606453\n",
            "epoch: 4 step: 278, loss is 0.015434757806360722\n",
            "epoch: 4 step: 279, loss is 0.11830627173185349\n",
            "epoch: 4 step: 280, loss is 0.04757476970553398\n",
            "epoch: 4 step: 281, loss is 0.008834571577608585\n",
            "epoch: 4 step: 282, loss is 0.007586129009723663\n",
            "epoch: 4 step: 283, loss is 0.10012798011302948\n",
            "epoch: 4 step: 284, loss is 0.01813746802508831\n",
            "epoch: 4 step: 285, loss is 0.07021728157997131\n",
            "epoch: 4 step: 286, loss is 0.125463604927063\n",
            "epoch: 4 step: 287, loss is 0.03776151314377785\n",
            "epoch: 4 step: 288, loss is 0.03352627530694008\n",
            "epoch: 4 step: 289, loss is 0.12086758017539978\n",
            "epoch: 4 step: 290, loss is 0.07489645481109619\n",
            "epoch: 4 step: 291, loss is 0.06911150366067886\n",
            "epoch: 4 step: 292, loss is 0.021825334057211876\n",
            "epoch: 4 step: 293, loss is 0.08849785476922989\n",
            "epoch: 4 step: 294, loss is 0.013616085052490234\n",
            "epoch: 4 step: 295, loss is 0.020917506888508797\n",
            "epoch: 4 step: 296, loss is 0.02175072208046913\n",
            "epoch: 4 step: 297, loss is 0.022713305428624153\n",
            "epoch: 4 step: 298, loss is 0.3398752212524414\n",
            "epoch: 4 step: 299, loss is 0.16269659996032715\n",
            "epoch: 4 step: 300, loss is 0.10528150200843811\n",
            "epoch: 4 step: 301, loss is 0.05043457821011543\n",
            "epoch: 4 step: 302, loss is 0.07050514966249466\n",
            "epoch: 4 step: 303, loss is 0.026698384433984756\n",
            "epoch: 4 step: 304, loss is 0.017568504437804222\n",
            "epoch: 4 step: 305, loss is 0.2154010534286499\n",
            "epoch: 4 step: 306, loss is 0.01959771290421486\n",
            "epoch: 4 step: 307, loss is 0.017972828820347786\n",
            "epoch: 4 step: 308, loss is 0.14182564616203308\n",
            "epoch: 4 step: 309, loss is 0.05115153267979622\n",
            "epoch: 4 step: 310, loss is 0.03002653457224369\n",
            "epoch: 4 step: 311, loss is 0.03561471775174141\n",
            "epoch: 4 step: 312, loss is 0.21484121680259705\n",
            "epoch: 4 step: 313, loss is 0.08496062457561493\n",
            "epoch: 4 step: 314, loss is 0.14536985754966736\n",
            "epoch: 4 step: 315, loss is 0.025905318558216095\n",
            "epoch: 4 step: 316, loss is 0.027470683678984642\n",
            "epoch: 4 step: 317, loss is 0.10802742093801498\n",
            "epoch: 4 step: 318, loss is 0.23630444705486298\n",
            "epoch: 4 step: 319, loss is 0.026605496183037758\n",
            "epoch: 4 step: 320, loss is 0.2716616988182068\n",
            "epoch: 4 step: 321, loss is 0.0893949642777443\n",
            "epoch: 4 step: 322, loss is 0.014594745822250843\n",
            "epoch: 4 step: 323, loss is 0.0787447839975357\n",
            "epoch: 4 step: 324, loss is 0.015177718363702297\n",
            "epoch: 4 step: 325, loss is 0.013937716372311115\n",
            "epoch: 4 step: 326, loss is 0.011382430791854858\n",
            "epoch: 4 step: 327, loss is 0.020485736429691315\n",
            "epoch: 4 step: 328, loss is 0.03433723747730255\n",
            "epoch: 4 step: 329, loss is 0.21917009353637695\n",
            "epoch: 4 step: 330, loss is 0.19009444117546082\n",
            "epoch: 4 step: 331, loss is 0.015090309083461761\n",
            "epoch: 4 step: 332, loss is 0.1216173991560936\n",
            "epoch: 4 step: 333, loss is 0.048616815358400345\n",
            "epoch: 4 step: 334, loss is 0.29581132531166077\n",
            "epoch: 4 step: 335, loss is 0.008959921076893806\n",
            "epoch: 4 step: 336, loss is 0.006785499397665262\n",
            "epoch: 4 step: 337, loss is 0.0071405465714633465\n",
            "epoch: 4 step: 338, loss is 0.016683734953403473\n",
            "epoch: 4 step: 339, loss is 0.011234141886234283\n",
            "epoch: 4 step: 340, loss is 0.01580108515918255\n",
            "epoch: 4 step: 341, loss is 0.010566611774265766\n",
            "epoch: 4 step: 342, loss is 0.029740039259195328\n",
            "epoch: 4 step: 343, loss is 0.01665445975959301\n",
            "epoch: 4 step: 344, loss is 0.019383711740374565\n",
            "epoch: 4 step: 345, loss is 0.010650336742401123\n",
            "epoch: 4 step: 346, loss is 0.1437814086675644\n",
            "epoch: 4 step: 347, loss is 0.021951405331492424\n",
            "epoch: 4 step: 348, loss is 0.1107841432094574\n",
            "epoch: 4 step: 349, loss is 0.018647460266947746\n",
            "epoch: 4 step: 350, loss is 0.25591376423835754\n",
            "epoch: 4 step: 351, loss is 0.017592765390872955\n",
            "epoch: 4 step: 352, loss is 0.0412285216152668\n",
            "epoch: 4 step: 353, loss is 0.011250589974224567\n",
            "epoch: 4 step: 354, loss is 0.07564852386713028\n",
            "epoch: 4 step: 355, loss is 0.01720011606812477\n",
            "epoch: 4 step: 356, loss is 0.15757064521312714\n",
            "epoch: 4 step: 357, loss is 0.106438048183918\n",
            "epoch: 4 step: 358, loss is 0.20667590200901031\n",
            "epoch: 4 step: 359, loss is 0.07714293152093887\n",
            "epoch: 4 step: 360, loss is 0.03341953456401825\n",
            "epoch: 4 step: 361, loss is 0.01396118849515915\n",
            "epoch: 4 step: 362, loss is 0.03766586631536484\n",
            "epoch: 4 step: 363, loss is 0.010250553488731384\n",
            "epoch: 4 step: 364, loss is 0.05766281858086586\n",
            "epoch: 4 step: 365, loss is 0.0245042834430933\n",
            "epoch: 4 step: 366, loss is 0.034728504717350006\n",
            "epoch: 4 step: 367, loss is 0.04193548485636711\n",
            "epoch: 4 step: 368, loss is 0.01679074577987194\n",
            "epoch: 4 step: 369, loss is 0.027157774195075035\n",
            "epoch: 4 step: 370, loss is 0.024005843326449394\n",
            "epoch: 4 step: 371, loss is 0.029507262632250786\n",
            "epoch: 4 step: 372, loss is 0.14556215703487396\n",
            "epoch: 4 step: 373, loss is 0.06648598611354828\n",
            "epoch: 4 step: 374, loss is 0.014566577039659023\n",
            "epoch: 4 step: 375, loss is 0.0914510190486908\n",
            "epoch: 4 step: 376, loss is 0.14743736386299133\n",
            "epoch: 4 step: 377, loss is 0.04843461513519287\n",
            "epoch: 4 step: 378, loss is 0.020466171205043793\n",
            "epoch: 4 step: 379, loss is 0.054997291415929794\n",
            "epoch: 4 step: 380, loss is 0.07077857106924057\n",
            "epoch: 4 step: 381, loss is 0.0824911966919899\n",
            "epoch: 4 step: 382, loss is 0.1421901285648346\n",
            "epoch: 4 step: 383, loss is 0.11004655063152313\n",
            "epoch: 4 step: 384, loss is 0.01614268496632576\n",
            "epoch: 4 step: 385, loss is 0.044121574610471725\n",
            "epoch: 4 step: 386, loss is 0.11341694742441177\n",
            "epoch: 4 step: 387, loss is 0.2748047709465027\n",
            "epoch: 4 step: 388, loss is 0.013691938482224941\n",
            "epoch: 4 step: 389, loss is 0.0126990657299757\n",
            "epoch: 4 step: 390, loss is 0.013183302246034145\n",
            "epoch: 4 step: 391, loss is 0.030384249985218048\n",
            "epoch: 4 step: 392, loss is 0.009738330729305744\n",
            "epoch: 4 step: 393, loss is 0.04612420126795769\n",
            "epoch: 4 step: 394, loss is 0.03146009147167206\n",
            "epoch: 4 step: 395, loss is 0.022111693397164345\n",
            "epoch: 4 step: 396, loss is 0.008298154920339584\n",
            "epoch: 4 step: 397, loss is 0.027931731194257736\n",
            "epoch: 4 step: 398, loss is 0.0605149045586586\n",
            "epoch: 4 step: 399, loss is 0.06318162381649017\n",
            "epoch: 4 step: 400, loss is 0.08200351893901825\n",
            "epoch: 4 step: 401, loss is 0.008468138985335827\n",
            "epoch: 4 step: 402, loss is 0.011883864179253578\n",
            "epoch: 4 step: 403, loss is 0.01264816801995039\n",
            "epoch: 4 step: 404, loss is 0.010090270079672337\n",
            "epoch: 4 step: 405, loss is 0.0820910856127739\n",
            "epoch: 4 step: 406, loss is 0.007188174407929182\n",
            "epoch: 4 step: 407, loss is 0.008249394595623016\n",
            "epoch: 4 step: 408, loss is 0.04151557758450508\n",
            "epoch: 4 step: 409, loss is 0.05290504917502403\n",
            "epoch: 4 step: 410, loss is 0.014878001064062119\n",
            "epoch: 4 step: 411, loss is 0.15832388401031494\n",
            "epoch: 4 step: 412, loss is 0.01515880785882473\n",
            "epoch: 4 step: 413, loss is 0.013979877345263958\n",
            "epoch: 4 step: 414, loss is 0.00806796457618475\n",
            "epoch: 4 step: 415, loss is 0.011055326089262962\n",
            "epoch: 4 step: 416, loss is 0.03892042860388756\n",
            "epoch: 4 step: 417, loss is 0.3976482152938843\n",
            "epoch: 4 step: 418, loss is 0.007284646388143301\n",
            "epoch: 4 step: 419, loss is 0.014955015853047371\n",
            "epoch: 4 step: 420, loss is 0.1361505687236786\n",
            "epoch: 4 step: 421, loss is 0.005246973596513271\n",
            "epoch: 4 step: 422, loss is 0.030558519065380096\n",
            "epoch: 4 step: 423, loss is 0.2117980569601059\n",
            "epoch: 4 step: 424, loss is 0.12836883962154388\n",
            "epoch: 4 step: 425, loss is 0.10391011834144592\n",
            "epoch: 4 step: 426, loss is 0.05493740364909172\n",
            "epoch: 4 step: 427, loss is 0.021165279671549797\n",
            "epoch: 4 step: 428, loss is 0.021111682057380676\n",
            "epoch: 4 step: 429, loss is 0.07401944696903229\n",
            "epoch: 4 step: 430, loss is 0.013285115361213684\n",
            "epoch: 4 step: 431, loss is 0.01624327152967453\n",
            "epoch: 4 step: 432, loss is 0.024225346744060516\n",
            "epoch: 4 step: 433, loss is 0.012020843103528023\n",
            "epoch: 4 step: 434, loss is 0.009939638897776604\n",
            "epoch: 4 step: 435, loss is 0.04296491667628288\n",
            "epoch: 4 step: 436, loss is 0.04745056480169296\n",
            "epoch: 4 step: 437, loss is 0.026554526761174202\n",
            "epoch: 4 step: 438, loss is 0.047809164971113205\n",
            "epoch: 4 step: 439, loss is 0.012844075448811054\n",
            "epoch: 4 step: 440, loss is 0.02440464496612549\n",
            "epoch: 4 step: 441, loss is 0.03772985562682152\n",
            "epoch: 4 step: 442, loss is 0.01592828705906868\n",
            "epoch: 4 step: 443, loss is 0.013549738563597202\n",
            "epoch: 4 step: 444, loss is 0.018973782658576965\n",
            "epoch: 4 step: 445, loss is 0.014802263118326664\n",
            "epoch: 4 step: 446, loss is 0.008037140592932701\n",
            "epoch: 4 step: 447, loss is 0.03318959102034569\n",
            "epoch: 4 step: 448, loss is 0.00916794128715992\n",
            "epoch: 4 step: 449, loss is 0.030699709430336952\n",
            "epoch: 4 step: 450, loss is 0.061014629900455475\n",
            "epoch: 4 step: 451, loss is 0.03311910852789879\n",
            "epoch: 4 step: 452, loss is 0.009140240959823132\n",
            "epoch: 4 step: 453, loss is 0.017435383051633835\n",
            "epoch: 4 step: 454, loss is 0.014993324875831604\n",
            "epoch: 4 step: 455, loss is 0.03293626010417938\n",
            "epoch: 4 step: 456, loss is 0.01245872862637043\n",
            "epoch: 4 step: 457, loss is 0.16408242285251617\n",
            "epoch: 4 step: 458, loss is 0.019187081605196\n",
            "epoch: 4 step: 459, loss is 0.011812328360974789\n",
            "epoch: 4 step: 460, loss is 0.15263697504997253\n",
            "epoch: 4 step: 461, loss is 0.011719395406544209\n",
            "epoch: 4 step: 462, loss is 0.06946045905351639\n",
            "epoch: 4 step: 463, loss is 0.029594415798783302\n",
            "epoch: 4 step: 464, loss is 0.033140845596790314\n",
            "epoch: 4 step: 465, loss is 0.016777170822024345\n",
            "epoch: 4 step: 466, loss is 0.028269067406654358\n",
            "epoch: 4 step: 467, loss is 0.09338298439979553\n",
            "epoch: 4 step: 468, loss is 0.025608699768781662\n",
            "epoch: 4 step: 469, loss is 0.04712681099772453\n",
            "epoch: 4 step: 470, loss is 0.03623521700501442\n",
            "epoch: 4 step: 471, loss is 0.01934158243238926\n",
            "epoch: 4 step: 472, loss is 0.08441461622714996\n",
            "epoch: 4 step: 473, loss is 0.01476503349840641\n",
            "epoch: 4 step: 474, loss is 0.02094322443008423\n",
            "epoch: 4 step: 475, loss is 0.009060077369213104\n",
            "epoch: 4 step: 476, loss is 0.01766962558031082\n",
            "epoch: 4 step: 477, loss is 0.0299157053232193\n",
            "epoch: 4 step: 478, loss is 0.017938833683729172\n",
            "epoch: 4 step: 479, loss is 0.033870548009872437\n",
            "epoch: 4 step: 480, loss is 0.03349918872117996\n",
            "epoch: 4 step: 481, loss is 0.08288872241973877\n",
            "epoch: 4 step: 482, loss is 0.04070050269365311\n",
            "epoch: 4 step: 483, loss is 0.009606140665709972\n",
            "epoch: 4 step: 484, loss is 0.015630679205060005\n",
            "epoch: 4 step: 485, loss is 0.022941799834370613\n",
            "epoch: 4 step: 486, loss is 0.02055673487484455\n",
            "epoch: 4 step: 487, loss is 0.00995856337249279\n",
            "epoch: 4 step: 488, loss is 0.013396133668720722\n",
            "epoch: 4 step: 489, loss is 0.008144660852849483\n",
            "epoch: 4 step: 490, loss is 0.0064463852904737\n",
            "epoch: 4 step: 491, loss is 0.02676025964319706\n",
            "epoch: 4 step: 492, loss is 0.006486314348876476\n",
            "epoch: 4 step: 493, loss is 0.22766025364398956\n",
            "epoch: 4 step: 494, loss is 0.20367367565631866\n",
            "epoch: 4 step: 495, loss is 0.29044991731643677\n",
            "epoch: 4 step: 496, loss is 0.003243484301492572\n",
            "epoch: 4 step: 497, loss is 0.01777413673698902\n",
            "epoch: 4 step: 498, loss is 0.039664238691329956\n",
            "epoch: 4 step: 499, loss is 0.10427949577569962\n",
            "epoch: 4 step: 500, loss is 0.39690086245536804\n",
            "epoch: 4 step: 501, loss is 0.008823391981422901\n",
            "epoch: 4 step: 502, loss is 0.011343879625201225\n",
            "epoch: 4 step: 503, loss is 0.025561759248375893\n",
            "epoch: 4 step: 504, loss is 0.024348126724362373\n",
            "epoch: 4 step: 505, loss is 0.028599128127098083\n",
            "epoch: 4 step: 506, loss is 0.11155550926923752\n",
            "epoch: 4 step: 507, loss is 0.015872487798333168\n",
            "epoch: 4 step: 508, loss is 0.15214024484157562\n",
            "epoch: 4 step: 509, loss is 0.10797836631536484\n",
            "epoch: 4 step: 510, loss is 0.011952403001487255\n",
            "epoch: 4 step: 511, loss is 0.04179789125919342\n",
            "epoch: 4 step: 512, loss is 0.01084685605019331\n",
            "epoch: 4 step: 513, loss is 0.02090488001704216\n",
            "epoch: 4 step: 514, loss is 0.03985048085451126\n",
            "epoch: 4 step: 515, loss is 0.02500985935330391\n",
            "epoch: 4 step: 516, loss is 0.030308498069643974\n",
            "epoch: 4 step: 517, loss is 0.13658089935779572\n",
            "epoch: 4 step: 518, loss is 0.0805719718337059\n",
            "epoch: 4 step: 519, loss is 0.054588574916124344\n",
            "epoch: 4 step: 520, loss is 0.030894581228494644\n",
            "epoch: 4 step: 521, loss is 0.02951802685856819\n",
            "epoch: 4 step: 522, loss is 0.021831098943948746\n",
            "epoch: 4 step: 523, loss is 0.005903826095163822\n",
            "epoch: 4 step: 524, loss is 0.029452437534928322\n",
            "epoch: 4 step: 525, loss is 0.020398100838065147\n",
            "epoch: 4 step: 526, loss is 0.011447299271821976\n",
            "epoch: 4 step: 527, loss is 0.01742233708500862\n",
            "epoch: 4 step: 528, loss is 0.02794782817363739\n",
            "epoch: 4 step: 529, loss is 0.015749406069517136\n",
            "epoch: 4 step: 530, loss is 0.09503292292356491\n",
            "epoch: 4 step: 531, loss is 0.021909698843955994\n",
            "epoch: 4 step: 532, loss is 0.010830300860106945\n",
            "epoch: 4 step: 533, loss is 0.055702414363622665\n",
            "epoch: 4 step: 534, loss is 0.010272341780364513\n",
            "epoch: 4 step: 535, loss is 0.020989006385207176\n",
            "epoch: 4 step: 536, loss is 0.006739059928804636\n",
            "epoch: 4 step: 537, loss is 0.010870974510908127\n",
            "epoch: 4 step: 538, loss is 0.0059959725476801395\n",
            "epoch: 4 step: 539, loss is 0.09130126982927322\n",
            "epoch: 4 step: 540, loss is 0.014540400356054306\n",
            "epoch: 4 step: 541, loss is 0.011434383690357208\n",
            "epoch: 4 step: 542, loss is 0.14958585798740387\n",
            "epoch: 4 step: 543, loss is 0.26295122504234314\n",
            "epoch: 4 step: 544, loss is 0.043953265994787216\n",
            "epoch: 4 step: 545, loss is 0.01715140976011753\n",
            "epoch: 4 step: 546, loss is 0.008653216063976288\n",
            "epoch: 4 step: 547, loss is 0.06586046516895294\n",
            "epoch: 4 step: 548, loss is 0.10300597548484802\n",
            "epoch: 4 step: 549, loss is 0.05768660455942154\n",
            "epoch: 4 step: 550, loss is 0.009707644581794739\n",
            "epoch: 4 step: 551, loss is 0.015231205150485039\n",
            "epoch: 4 step: 552, loss is 0.009013308212161064\n",
            "epoch: 4 step: 553, loss is 0.07408253848552704\n",
            "epoch: 4 step: 554, loss is 0.05097903683781624\n",
            "epoch: 4 step: 555, loss is 0.006412227172404528\n",
            "epoch: 4 step: 556, loss is 0.04112355411052704\n",
            "epoch: 4 step: 557, loss is 0.03372326120734215\n",
            "epoch: 4 step: 558, loss is 0.024195285513997078\n",
            "epoch: 4 step: 559, loss is 0.038597654551267624\n",
            "epoch: 4 step: 560, loss is 0.017450738698244095\n",
            "epoch: 4 step: 561, loss is 0.12007059901952744\n",
            "epoch: 4 step: 562, loss is 0.012301155366003513\n",
            "epoch: 4 step: 563, loss is 0.009188899770379066\n",
            "epoch: 4 step: 564, loss is 0.11772793531417847\n",
            "epoch: 4 step: 565, loss is 0.13712576031684875\n",
            "epoch: 4 step: 566, loss is 0.00896653812378645\n",
            "epoch: 4 step: 567, loss is 0.037450432777404785\n",
            "epoch: 4 step: 568, loss is 0.05120065063238144\n",
            "epoch: 4 step: 569, loss is 0.00855925865471363\n",
            "epoch: 4 step: 570, loss is 0.013843650929629803\n",
            "epoch: 4 step: 571, loss is 0.006497543770819902\n",
            "epoch: 4 step: 572, loss is 0.03622863441705704\n",
            "epoch: 4 step: 573, loss is 0.029999349266290665\n",
            "epoch: 4 step: 574, loss is 0.010847982950508595\n",
            "epoch: 4 step: 575, loss is 0.03135346248745918\n",
            "epoch: 4 step: 576, loss is 0.1423732042312622\n",
            "epoch: 4 step: 577, loss is 0.012064351700246334\n",
            "epoch: 4 step: 578, loss is 0.01341391820460558\n",
            "epoch: 4 step: 579, loss is 0.013618645258247852\n",
            "epoch: 4 step: 580, loss is 0.034459713846445084\n",
            "epoch: 4 step: 581, loss is 0.09054605662822723\n",
            "epoch: 4 step: 582, loss is 0.01309796329587698\n",
            "epoch: 4 step: 583, loss is 0.01948988251388073\n",
            "epoch: 4 step: 584, loss is 0.02120845392346382\n",
            "epoch: 4 step: 585, loss is 0.007020210847258568\n",
            "epoch: 4 step: 586, loss is 0.024285614490509033\n",
            "epoch: 4 step: 587, loss is 0.01387741882354021\n",
            "epoch: 4 step: 588, loss is 0.10674753785133362\n",
            "epoch: 4 step: 589, loss is 0.14032062888145447\n",
            "epoch: 4 step: 590, loss is 0.015296565368771553\n",
            "epoch: 4 step: 591, loss is 0.16406510770320892\n",
            "epoch: 4 step: 592, loss is 0.30095869302749634\n",
            "epoch: 4 step: 593, loss is 0.010216398164629936\n",
            "epoch: 4 step: 594, loss is 0.013758104294538498\n",
            "epoch: 4 step: 595, loss is 0.010089763440191746\n",
            "epoch: 4 step: 596, loss is 0.05904750898480415\n",
            "epoch: 4 step: 597, loss is 0.01558735966682434\n",
            "epoch: 4 step: 598, loss is 0.2648160457611084\n",
            "epoch: 4 step: 599, loss is 0.013721980154514313\n",
            "epoch: 4 step: 600, loss is 0.041153497993946075\n",
            "epoch: 4 step: 601, loss is 0.10011696070432663\n",
            "epoch: 4 step: 602, loss is 0.028833232820034027\n",
            "epoch: 4 step: 603, loss is 0.016755441203713417\n",
            "epoch: 4 step: 604, loss is 0.009380778297781944\n",
            "epoch: 4 step: 605, loss is 0.05046979710459709\n",
            "epoch: 4 step: 606, loss is 0.064307801425457\n",
            "epoch: 4 step: 607, loss is 0.03580595925450325\n",
            "epoch: 4 step: 608, loss is 0.028032148256897926\n",
            "epoch: 4 step: 609, loss is 0.2648325264453888\n",
            "epoch: 4 step: 610, loss is 0.02848535217344761\n",
            "epoch: 4 step: 611, loss is 0.013651867397129536\n",
            "epoch: 4 step: 612, loss is 0.035033587366342545\n",
            "epoch: 4 step: 613, loss is 0.016713852062821388\n",
            "epoch: 4 step: 614, loss is 0.1331658661365509\n",
            "epoch: 4 step: 615, loss is 0.07935905456542969\n",
            "epoch: 4 step: 616, loss is 0.12797103822231293\n",
            "epoch: 4 step: 617, loss is 0.03745003789663315\n",
            "epoch: 4 step: 618, loss is 0.24529287219047546\n",
            "epoch: 4 step: 619, loss is 0.012205460108816624\n",
            "epoch: 4 step: 620, loss is 0.09742461144924164\n",
            "epoch: 4 step: 621, loss is 0.03085981123149395\n",
            "epoch: 4 step: 622, loss is 0.4252626299858093\n",
            "epoch: 4 step: 623, loss is 0.015439257957041264\n",
            "epoch: 4 step: 624, loss is 0.017137769609689713\n",
            "epoch: 4 step: 625, loss is 0.09450913220643997\n",
            "epoch: 4 step: 626, loss is 0.10421062260866165\n",
            "epoch: 4 step: 627, loss is 0.08949238061904907\n",
            "epoch: 4 step: 628, loss is 0.015654273331165314\n",
            "epoch: 4 step: 629, loss is 0.04135431349277496\n",
            "epoch: 4 step: 630, loss is 0.015036497265100479\n",
            "epoch: 4 step: 631, loss is 0.03495422750711441\n",
            "epoch: 4 step: 632, loss is 0.24032118916511536\n",
            "epoch: 4 step: 633, loss is 0.20817509293556213\n",
            "epoch: 4 step: 634, loss is 0.19783857464790344\n",
            "epoch: 4 step: 635, loss is 0.1525116115808487\n",
            "epoch: 4 step: 636, loss is 0.02970714122056961\n",
            "epoch: 4 step: 637, loss is 0.05550559237599373\n",
            "epoch: 4 step: 638, loss is 0.01963508129119873\n",
            "epoch: 4 step: 639, loss is 0.012736840173602104\n",
            "epoch: 4 step: 640, loss is 0.024851610884070396\n",
            "epoch: 4 step: 641, loss is 0.028474945574998856\n",
            "epoch: 4 step: 642, loss is 0.01213029120117426\n",
            "epoch: 4 step: 643, loss is 0.07605364918708801\n",
            "epoch: 4 step: 644, loss is 0.06031293049454689\n",
            "epoch: 4 step: 645, loss is 0.023604044690728188\n",
            "epoch: 4 step: 646, loss is 0.05479501187801361\n",
            "epoch: 4 step: 647, loss is 0.023630034178495407\n",
            "epoch: 4 step: 648, loss is 0.20011182129383087\n",
            "epoch: 4 step: 649, loss is 0.36951085925102234\n",
            "epoch: 4 step: 650, loss is 0.010440324433147907\n",
            "epoch: 4 step: 651, loss is 0.02360714040696621\n",
            "epoch: 4 step: 652, loss is 0.011741344816982746\n",
            "epoch: 4 step: 653, loss is 0.03815096244215965\n",
            "epoch: 4 step: 654, loss is 0.020691554993391037\n",
            "epoch: 4 step: 655, loss is 0.020250236615538597\n",
            "epoch: 4 step: 656, loss is 0.03599550575017929\n",
            "epoch: 4 step: 657, loss is 0.11420565843582153\n",
            "epoch: 4 step: 658, loss is 0.014077482745051384\n",
            "epoch: 4 step: 659, loss is 0.020115721970796585\n",
            "epoch: 4 step: 660, loss is 0.03932037949562073\n",
            "epoch: 4 step: 661, loss is 0.041435789316892624\n",
            "epoch: 4 step: 662, loss is 0.029735678806900978\n",
            "epoch: 4 step: 663, loss is 0.17810511589050293\n",
            "epoch: 4 step: 664, loss is 0.028991958126425743\n",
            "epoch: 4 step: 665, loss is 0.012367982417345047\n",
            "epoch: 4 step: 666, loss is 0.01993204466998577\n",
            "epoch: 4 step: 667, loss is 0.09168236702680588\n",
            "epoch: 4 step: 668, loss is 0.06614994257688522\n",
            "epoch: 4 step: 669, loss is 0.023041464388370514\n",
            "epoch: 4 step: 670, loss is 0.039121534675359726\n",
            "epoch: 4 step: 671, loss is 0.02412535436451435\n",
            "epoch: 4 step: 672, loss is 0.015676163136959076\n",
            "epoch: 4 step: 673, loss is 0.10498670488595963\n",
            "epoch: 4 step: 674, loss is 0.19845014810562134\n",
            "epoch: 4 step: 675, loss is 0.005358974449336529\n",
            "epoch: 4 step: 676, loss is 0.14988720417022705\n",
            "epoch: 4 step: 677, loss is 0.023557502776384354\n",
            "epoch: 4 step: 678, loss is 0.009946806356310844\n",
            "epoch: 4 step: 679, loss is 0.05161038041114807\n",
            "epoch: 4 step: 680, loss is 0.022899387404322624\n",
            "epoch: 4 step: 681, loss is 0.024437012150883675\n",
            "epoch: 4 step: 682, loss is 0.02929060347378254\n",
            "epoch: 4 step: 683, loss is 0.16301195323467255\n",
            "epoch: 4 step: 684, loss is 0.19170750677585602\n",
            "epoch: 4 step: 685, loss is 0.29387664794921875\n",
            "epoch: 4 step: 686, loss is 0.02478864975273609\n",
            "epoch: 4 step: 687, loss is 0.06948786973953247\n",
            "epoch: 4 step: 688, loss is 0.018114963546395302\n",
            "epoch: 4 step: 689, loss is 0.018440943211317062\n",
            "epoch: 4 step: 690, loss is 0.4296945333480835\n",
            "epoch: 4 step: 691, loss is 0.11887969821691513\n",
            "epoch: 4 step: 692, loss is 0.01731030084192753\n",
            "epoch: 4 step: 693, loss is 0.03160225972533226\n",
            "epoch: 4 step: 694, loss is 0.03449806571006775\n",
            "epoch: 4 step: 695, loss is 0.02833412028849125\n",
            "epoch: 4 step: 696, loss is 0.012840473093092442\n",
            "epoch: 4 step: 697, loss is 0.025027146562933922\n",
            "epoch: 4 step: 698, loss is 0.02509921044111252\n",
            "epoch: 4 step: 699, loss is 0.02868325263261795\n",
            "epoch: 4 step: 700, loss is 0.03527214750647545\n",
            "epoch: 4 step: 701, loss is 0.21024052798748016\n",
            "epoch: 4 step: 702, loss is 0.07939337193965912\n",
            "epoch: 4 step: 703, loss is 0.0225937832146883\n",
            "epoch: 4 step: 704, loss is 0.04319571703672409\n",
            "epoch: 4 step: 705, loss is 0.23863957822322845\n",
            "epoch: 4 step: 706, loss is 0.031903862953186035\n",
            "epoch: 4 step: 707, loss is 0.0654831975698471\n",
            "epoch: 4 step: 708, loss is 0.028517017140984535\n",
            "epoch: 4 step: 709, loss is 0.030690571293234825\n",
            "epoch: 4 step: 710, loss is 0.00918226595968008\n",
            "epoch: 4 step: 711, loss is 0.06349309533834457\n",
            "epoch: 4 step: 712, loss is 0.3466933071613312\n",
            "epoch: 4 step: 713, loss is 0.020222073420882225\n",
            "epoch: 4 step: 714, loss is 0.007248117122799158\n",
            "epoch: 4 step: 715, loss is 0.011044545099139214\n",
            "epoch: 4 step: 716, loss is 0.030390731990337372\n",
            "epoch: 4 step: 717, loss is 0.012799697928130627\n",
            "epoch: 4 step: 718, loss is 0.026484711095690727\n",
            "epoch: 4 step: 719, loss is 0.027267461642622948\n",
            "epoch: 4 step: 720, loss is 0.010251258499920368\n",
            "epoch: 4 step: 721, loss is 0.022782202810049057\n",
            "epoch: 4 step: 722, loss is 0.006849181372672319\n",
            "epoch: 4 step: 723, loss is 0.14471325278282166\n",
            "epoch: 4 step: 724, loss is 0.10949821770191193\n",
            "epoch: 4 step: 725, loss is 0.02836454100906849\n",
            "epoch: 4 step: 726, loss is 0.006754779722541571\n",
            "epoch: 4 step: 727, loss is 0.03097659721970558\n",
            "epoch: 4 step: 728, loss is 0.03711370751261711\n",
            "epoch: 4 step: 729, loss is 0.009836253710091114\n",
            "epoch: 4 step: 730, loss is 0.009144973009824753\n",
            "epoch: 4 step: 731, loss is 0.015223592519760132\n",
            "epoch: 4 step: 732, loss is 0.022742103785276413\n",
            "epoch: 4 step: 733, loss is 0.022897329181432724\n",
            "epoch: 4 step: 734, loss is 0.0042780013754963875\n",
            "epoch: 4 step: 735, loss is 0.003800461068749428\n",
            "epoch: 4 step: 736, loss is 0.03260120004415512\n",
            "epoch: 4 step: 737, loss is 0.010378715582191944\n",
            "epoch: 4 step: 738, loss is 0.011232580989599228\n",
            "epoch: 4 step: 739, loss is 0.016874535009264946\n",
            "epoch: 4 step: 740, loss is 0.03148410841822624\n",
            "epoch: 4 step: 741, loss is 0.016681252047419548\n",
            "epoch: 4 step: 742, loss is 0.010047134011983871\n",
            "epoch: 4 step: 743, loss is 0.028271488845348358\n",
            "epoch: 4 step: 744, loss is 0.42106321454048157\n",
            "epoch: 4 step: 745, loss is 0.005661035422235727\n",
            "epoch: 4 step: 746, loss is 0.01251577865332365\n",
            "epoch: 4 step: 747, loss is 0.00669017992913723\n",
            "epoch: 4 step: 748, loss is 0.013082696124911308\n",
            "epoch: 4 step: 749, loss is 0.0880594477057457\n",
            "epoch: 4 step: 750, loss is 0.0132164740934968\n",
            "epoch: 4 step: 751, loss is 0.01887480914592743\n",
            "epoch: 4 step: 752, loss is 0.00660105561837554\n",
            "epoch: 4 step: 753, loss is 0.010879982262849808\n",
            "epoch: 4 step: 754, loss is 0.2832266092300415\n",
            "epoch: 4 step: 755, loss is 0.18581640720367432\n",
            "epoch: 4 step: 756, loss is 0.008098261430859566\n",
            "epoch: 4 step: 757, loss is 0.29509031772613525\n",
            "epoch: 4 step: 758, loss is 0.05397610366344452\n",
            "epoch: 4 step: 759, loss is 0.05878289043903351\n",
            "epoch: 4 step: 760, loss is 0.013872058130800724\n",
            "epoch: 4 step: 761, loss is 0.18276366591453552\n",
            "epoch: 4 step: 762, loss is 0.10876785218715668\n",
            "epoch: 4 step: 763, loss is 0.01587834767997265\n",
            "epoch: 4 step: 764, loss is 0.010831231251358986\n",
            "epoch: 4 step: 765, loss is 0.009983989410102367\n",
            "epoch: 4 step: 766, loss is 0.027961615473031998\n",
            "epoch: 4 step: 767, loss is 0.13315750658512115\n",
            "epoch: 4 step: 768, loss is 0.01259184442460537\n",
            "epoch: 4 step: 769, loss is 0.012237760238349438\n",
            "epoch: 4 step: 770, loss is 0.008562094531953335\n",
            "epoch: 4 step: 771, loss is 0.010311360470950603\n",
            "epoch: 4 step: 772, loss is 0.053699932992458344\n",
            "epoch: 4 step: 773, loss is 0.017504354938864708\n",
            "epoch: 4 step: 774, loss is 0.04366820678114891\n",
            "epoch: 4 step: 775, loss is 0.13652360439300537\n",
            "epoch: 4 step: 776, loss is 0.1294790655374527\n",
            "epoch: 4 step: 777, loss is 0.07860099524259567\n",
            "epoch: 4 step: 778, loss is 0.007384303957223892\n",
            "epoch: 4 step: 779, loss is 0.15862397849559784\n",
            "epoch: 4 step: 780, loss is 0.3992833197116852\n",
            "epoch: 4 step: 781, loss is 0.2652679681777954\n",
            "epoch: 4 step: 782, loss is 0.02478298731148243\n",
            "epoch: 4 step: 783, loss is 0.01971656084060669\n",
            "epoch: 4 step: 784, loss is 0.010259446687996387\n",
            "epoch: 4 step: 785, loss is 0.04639660567045212\n",
            "epoch: 4 step: 786, loss is 0.00721783796325326\n",
            "epoch: 4 step: 787, loss is 0.013906039297580719\n",
            "epoch: 4 step: 788, loss is 0.02629680000245571\n",
            "epoch: 4 step: 789, loss is 0.022503424435853958\n",
            "epoch: 4 step: 790, loss is 0.01279355213046074\n",
            "epoch: 4 step: 791, loss is 0.014598718844354153\n",
            "epoch: 4 step: 792, loss is 0.0174665879458189\n",
            "epoch: 4 step: 793, loss is 0.010922889225184917\n",
            "epoch: 4 step: 794, loss is 0.01162582729011774\n",
            "epoch: 4 step: 795, loss is 0.12017156183719635\n",
            "epoch: 4 step: 796, loss is 0.04187779873609543\n",
            "epoch: 4 step: 797, loss is 0.020969705656170845\n",
            "epoch: 4 step: 798, loss is 0.016671447083353996\n",
            "epoch: 4 step: 799, loss is 0.022027092054486275\n",
            "epoch: 4 step: 800, loss is 0.11698205769062042\n",
            "epoch: 4 step: 801, loss is 0.0926990881562233\n",
            "epoch: 4 step: 802, loss is 0.05858917161822319\n",
            "epoch: 4 step: 803, loss is 0.09148330986499786\n",
            "epoch: 4 step: 804, loss is 0.027161424979567528\n",
            "epoch: 4 step: 805, loss is 0.007383785676211119\n",
            "epoch: 4 step: 806, loss is 0.05223146080970764\n",
            "epoch: 4 step: 807, loss is 0.017740435898303986\n",
            "epoch: 4 step: 808, loss is 0.01118715200573206\n",
            "epoch: 4 step: 809, loss is 0.06503430008888245\n",
            "epoch: 4 step: 810, loss is 0.011639486066997051\n",
            "epoch: 4 step: 811, loss is 0.012367546558380127\n",
            "epoch: 4 step: 812, loss is 0.019556906074285507\n",
            "epoch: 4 step: 813, loss is 0.026461796835064888\n",
            "epoch: 4 step: 814, loss is 0.03136317804455757\n",
            "epoch: 4 step: 815, loss is 0.15264315903186798\n",
            "epoch: 4 step: 816, loss is 0.01021018996834755\n",
            "epoch: 4 step: 817, loss is 0.025516942143440247\n",
            "epoch: 4 step: 818, loss is 0.040007006376981735\n",
            "epoch: 4 step: 819, loss is 0.01419565174728632\n",
            "epoch: 4 step: 820, loss is 0.14171837270259857\n",
            "epoch: 4 step: 821, loss is 0.0028836231213063\n",
            "epoch: 4 step: 822, loss is 0.01774507388472557\n",
            "epoch: 4 step: 823, loss is 0.00412031402811408\n",
            "epoch: 4 step: 824, loss is 0.013231933116912842\n",
            "epoch: 4 step: 825, loss is 0.168728768825531\n",
            "epoch: 4 step: 826, loss is 0.02914242446422577\n",
            "epoch: 4 step: 827, loss is 0.03391386568546295\n",
            "epoch: 4 step: 828, loss is 0.014115065336227417\n",
            "epoch: 4 step: 829, loss is 0.01329235639423132\n",
            "epoch: 4 step: 830, loss is 0.022908702492713928\n",
            "epoch: 4 step: 831, loss is 0.014804080128669739\n",
            "epoch: 4 step: 832, loss is 0.009303681552410126\n",
            "epoch: 4 step: 833, loss is 0.01164709497243166\n",
            "epoch: 4 step: 834, loss is 0.029258370399475098\n",
            "epoch: 4 step: 835, loss is 0.010089441202580929\n",
            "epoch: 4 step: 836, loss is 0.011004270054399967\n",
            "epoch: 4 step: 837, loss is 0.014570105820894241\n",
            "epoch: 4 step: 838, loss is 0.014831244014203548\n",
            "epoch: 4 step: 839, loss is 0.056589867919683456\n",
            "epoch: 4 step: 840, loss is 0.017341572791337967\n",
            "epoch: 4 step: 841, loss is 0.01100958976894617\n",
            "epoch: 4 step: 842, loss is 0.01793459616601467\n",
            "epoch: 4 step: 843, loss is 0.19015906751155853\n",
            "epoch: 4 step: 844, loss is 0.0073056407272815704\n",
            "epoch: 4 step: 845, loss is 0.004028483759611845\n",
            "epoch: 4 step: 846, loss is 0.023712970316410065\n",
            "epoch: 4 step: 847, loss is 0.0080311493948102\n",
            "epoch: 4 step: 848, loss is 0.04056462273001671\n",
            "epoch: 4 step: 849, loss is 0.03658550977706909\n",
            "epoch: 4 step: 850, loss is 0.0182195995002985\n",
            "epoch: 4 step: 851, loss is 0.0982334241271019\n",
            "epoch: 4 step: 852, loss is 0.29490432143211365\n",
            "epoch: 4 step: 853, loss is 0.21987205743789673\n",
            "epoch: 4 step: 854, loss is 0.030973870307207108\n",
            "epoch: 4 step: 855, loss is 0.01375890988856554\n",
            "epoch: 4 step: 856, loss is 0.07053323090076447\n",
            "epoch: 4 step: 857, loss is 0.012897966429591179\n",
            "epoch: 4 step: 858, loss is 0.01893623359501362\n",
            "epoch: 4 step: 859, loss is 0.10879082977771759\n",
            "epoch: 4 step: 860, loss is 0.0071525354869663715\n",
            "epoch: 4 step: 861, loss is 0.03534040227532387\n",
            "epoch: 4 step: 862, loss is 0.00860708300024271\n",
            "epoch: 4 step: 863, loss is 0.25840288400650024\n",
            "epoch: 4 step: 864, loss is 0.07430288195610046\n",
            "epoch: 4 step: 865, loss is 0.07745468616485596\n",
            "epoch: 4 step: 866, loss is 0.08477882295846939\n",
            "epoch: 4 step: 867, loss is 0.017781870439648628\n",
            "epoch: 4 step: 868, loss is 0.01355917938053608\n",
            "epoch: 4 step: 869, loss is 0.11233176290988922\n",
            "epoch: 4 step: 870, loss is 0.024825792759656906\n",
            "epoch: 4 step: 871, loss is 0.06862794607877731\n",
            "epoch: 4 step: 872, loss is 0.017225008457899094\n",
            "epoch: 4 step: 873, loss is 0.012479220516979694\n",
            "epoch: 4 step: 874, loss is 0.00819101370871067\n",
            "epoch: 4 step: 875, loss is 0.1094459667801857\n",
            "epoch: 4 step: 876, loss is 0.01522460300475359\n",
            "epoch: 4 step: 877, loss is 0.18394820392131805\n",
            "epoch: 4 step: 878, loss is 0.008176581002771854\n",
            "epoch: 4 step: 879, loss is 0.013428449630737305\n",
            "epoch: 4 step: 880, loss is 0.027452925220131874\n",
            "epoch: 4 step: 881, loss is 0.1342843919992447\n",
            "epoch: 4 step: 882, loss is 0.013109186664223671\n",
            "epoch: 4 step: 883, loss is 0.09991654008626938\n",
            "epoch: 4 step: 884, loss is 0.008602338843047619\n",
            "epoch: 4 step: 885, loss is 0.011126302182674408\n",
            "epoch: 4 step: 886, loss is 0.020143376663327217\n",
            "epoch: 4 step: 887, loss is 0.01838061958551407\n",
            "epoch: 4 step: 888, loss is 0.027409790083765984\n",
            "epoch: 4 step: 889, loss is 0.01051066629588604\n",
            "epoch: 4 step: 890, loss is 0.015389170497655869\n",
            "epoch: 4 step: 891, loss is 0.010704632848501205\n",
            "epoch: 4 step: 892, loss is 0.02569405920803547\n",
            "epoch: 4 step: 893, loss is 0.029137022793293\n",
            "epoch: 4 step: 894, loss is 0.03375036269426346\n",
            "epoch: 4 step: 895, loss is 0.009472248144447803\n",
            "epoch: 4 step: 896, loss is 0.02596103399991989\n",
            "epoch: 4 step: 897, loss is 0.03244023397564888\n",
            "epoch: 4 step: 898, loss is 0.005722964648157358\n",
            "epoch: 4 step: 899, loss is 0.011512667872011662\n",
            "epoch: 4 step: 900, loss is 0.008858944289386272\n",
            "epoch: 4 step: 901, loss is 0.018914857879281044\n",
            "epoch: 4 step: 902, loss is 0.039894793182611465\n",
            "epoch: 4 step: 903, loss is 0.03593459352850914\n",
            "epoch: 4 step: 904, loss is 0.0182985570281744\n",
            "epoch: 4 step: 905, loss is 0.057182639837265015\n",
            "epoch: 4 step: 906, loss is 0.020733291283249855\n",
            "epoch: 4 step: 907, loss is 0.008174640126526356\n",
            "epoch: 4 step: 908, loss is 0.01365983858704567\n",
            "epoch: 4 step: 909, loss is 0.32609057426452637\n",
            "epoch: 4 step: 910, loss is 0.01019798219203949\n",
            "epoch: 4 step: 911, loss is 0.010157905519008636\n",
            "epoch: 4 step: 912, loss is 0.055344436317682266\n",
            "epoch: 4 step: 913, loss is 0.12068696320056915\n",
            "epoch: 4 step: 914, loss is 0.03941422328352928\n",
            "epoch: 4 step: 915, loss is 0.01409967988729477\n",
            "epoch: 4 step: 916, loss is 0.009498804807662964\n",
            "epoch: 4 step: 917, loss is 0.043918829411268234\n",
            "epoch: 4 step: 918, loss is 0.031715381890535355\n",
            "epoch: 4 step: 919, loss is 0.007647580932825804\n",
            "epoch: 4 step: 920, loss is 0.030084023252129555\n",
            "epoch: 4 step: 921, loss is 0.006879052612930536\n",
            "epoch: 4 step: 922, loss is 0.008130217902362347\n",
            "epoch: 4 step: 923, loss is 0.016432175412774086\n",
            "epoch: 4 step: 924, loss is 0.020583048462867737\n",
            "epoch: 4 step: 925, loss is 0.006903443019837141\n",
            "epoch: 4 step: 926, loss is 0.01823846623301506\n",
            "epoch: 4 step: 927, loss is 0.06589756160974503\n",
            "epoch: 4 step: 928, loss is 0.04170064628124237\n",
            "epoch: 4 step: 929, loss is 0.016378212720155716\n",
            "epoch: 4 step: 930, loss is 0.00933206919580698\n",
            "epoch: 4 step: 931, loss is 0.15357883274555206\n",
            "epoch: 4 step: 932, loss is 0.010953348129987717\n",
            "epoch: 4 step: 933, loss is 0.005055374000221491\n",
            "epoch: 4 step: 934, loss is 0.019840680062770844\n",
            "epoch: 4 step: 935, loss is 0.011376986280083656\n",
            "epoch: 4 step: 936, loss is 0.02043837308883667\n",
            "epoch: 4 step: 937, loss is 0.05704374611377716\n",
            "epoch: 4 step: 938, loss is 0.005538354627788067\n",
            "epoch: 4 step: 939, loss is 0.01566472090780735\n",
            "epoch: 4 step: 940, loss is 0.06706181913614273\n",
            "epoch: 4 step: 941, loss is 0.14552952349185944\n",
            "epoch: 4 step: 942, loss is 0.024114251136779785\n",
            "epoch: 4 step: 943, loss is 0.008473040536046028\n",
            "epoch: 4 step: 944, loss is 0.01213974691927433\n",
            "epoch: 4 step: 945, loss is 0.11414219439029694\n",
            "epoch: 4 step: 946, loss is 0.012274006381630898\n",
            "epoch: 4 step: 947, loss is 0.0804007425904274\n",
            "epoch: 4 step: 948, loss is 0.028193606063723564\n",
            "epoch: 4 step: 949, loss is 0.033333830535411835\n",
            "epoch: 4 step: 950, loss is 0.04407054930925369\n",
            "epoch: 4 step: 951, loss is 0.1558343768119812\n",
            "epoch: 4 step: 952, loss is 0.013860825449228287\n",
            "epoch: 4 step: 953, loss is 0.017964312806725502\n",
            "epoch: 4 step: 954, loss is 0.041244663298130035\n",
            "epoch: 4 step: 955, loss is 0.013029692694544792\n",
            "epoch: 4 step: 956, loss is 0.023012110963463783\n",
            "epoch: 4 step: 957, loss is 0.007798161823302507\n",
            "epoch: 4 step: 958, loss is 0.010120613500475883\n",
            "epoch: 4 step: 959, loss is 0.025017252191901207\n",
            "epoch: 4 step: 960, loss is 0.1994909793138504\n",
            "epoch: 4 step: 961, loss is 0.009411663748323917\n",
            "epoch: 4 step: 962, loss is 0.015278630889952183\n",
            "epoch: 4 step: 963, loss is 0.011716723442077637\n",
            "epoch: 4 step: 964, loss is 0.008887620642781258\n",
            "epoch: 4 step: 965, loss is 0.012495327740907669\n",
            "epoch: 4 step: 966, loss is 0.007610253524035215\n",
            "epoch: 4 step: 967, loss is 0.06166612729430199\n",
            "epoch: 4 step: 968, loss is 0.032866887748241425\n",
            "epoch: 4 step: 969, loss is 0.13413138687610626\n",
            "epoch: 4 step: 970, loss is 0.014608597382903099\n",
            "epoch: 4 step: 971, loss is 0.051597293466329575\n",
            "epoch: 4 step: 972, loss is 0.013167252764105797\n",
            "epoch: 4 step: 973, loss is 0.007039925083518028\n",
            "epoch: 4 step: 974, loss is 0.05713535472750664\n",
            "epoch: 4 step: 975, loss is 0.013011275790631771\n",
            "epoch: 4 step: 976, loss is 0.009414156898856163\n",
            "epoch: 4 step: 977, loss is 0.10514181852340698\n",
            "epoch: 4 step: 978, loss is 0.005614747758954763\n",
            "epoch: 4 step: 979, loss is 0.006281173788011074\n",
            "epoch: 4 step: 980, loss is 0.0284504946321249\n",
            "epoch: 4 step: 981, loss is 0.019364802166819572\n",
            "epoch: 4 step: 982, loss is 0.18427810072898865\n",
            "epoch: 4 step: 983, loss is 0.025150123983621597\n",
            "epoch: 4 step: 984, loss is 0.026813970878720284\n",
            "epoch: 4 step: 985, loss is 0.011367976665496826\n",
            "epoch: 4 step: 986, loss is 0.006839242298156023\n",
            "epoch: 4 step: 987, loss is 0.008644862100481987\n",
            "epoch: 4 step: 988, loss is 0.01568695902824402\n",
            "epoch: 4 step: 989, loss is 0.015428184531629086\n",
            "epoch: 4 step: 990, loss is 0.0051233223639428616\n",
            "epoch: 4 step: 991, loss is 0.0056813061237335205\n",
            "epoch: 4 step: 992, loss is 0.10746334493160248\n",
            "epoch: 4 step: 993, loss is 0.007545706816017628\n",
            "epoch: 4 step: 994, loss is 0.0025871717371046543\n",
            "epoch: 4 step: 995, loss is 0.007062616292387247\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WARNING] ME(1671:136248148721664,MainProcess):2025-04-19-16:22:04.560.00 [mindspore/train/callback/_early_stop.py:221] Early stopping is conditioned on accuracy, which is not available. Available choices are: {'loss'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 5 step: 1, loss is 0.25834622979164124\n",
            "epoch: 5 step: 2, loss is 0.020815763622522354\n",
            "epoch: 5 step: 3, loss is 0.009243713691830635\n",
            "epoch: 5 step: 4, loss is 0.02028408646583557\n",
            "epoch: 5 step: 5, loss is 0.02922840788960457\n",
            "epoch: 5 step: 6, loss is 0.018047291785478592\n",
            "epoch: 5 step: 7, loss is 0.10778110474348068\n",
            "epoch: 5 step: 8, loss is 0.009630241431295872\n",
            "epoch: 5 step: 9, loss is 0.027388131245970726\n",
            "epoch: 5 step: 10, loss is 0.0066976845264434814\n",
            "epoch: 5 step: 11, loss is 0.01645222306251526\n",
            "epoch: 5 step: 12, loss is 0.05894847959280014\n",
            "epoch: 5 step: 13, loss is 0.07420646399259567\n",
            "epoch: 5 step: 14, loss is 0.024023529142141342\n",
            "epoch: 5 step: 15, loss is 0.11088667809963226\n",
            "epoch: 5 step: 16, loss is 0.014012335799634457\n",
            "epoch: 5 step: 17, loss is 0.009357435628771782\n",
            "epoch: 5 step: 18, loss is 0.03905756026506424\n",
            "epoch: 5 step: 19, loss is 0.059538017958402634\n",
            "epoch: 5 step: 20, loss is 0.020197885110974312\n",
            "epoch: 5 step: 21, loss is 0.1266661286354065\n",
            "epoch: 5 step: 22, loss is 0.0035341198090463877\n",
            "epoch: 5 step: 23, loss is 0.07691407948732376\n",
            "epoch: 5 step: 24, loss is 0.01944967918097973\n",
            "epoch: 5 step: 25, loss is 0.008124229498207569\n",
            "epoch: 5 step: 26, loss is 0.00681078527122736\n",
            "epoch: 5 step: 27, loss is 0.11475224047899246\n",
            "epoch: 5 step: 28, loss is 0.16747233271598816\n",
            "epoch: 5 step: 29, loss is 0.0979551300406456\n",
            "epoch: 5 step: 30, loss is 0.008421809412539005\n",
            "epoch: 5 step: 31, loss is 0.003691504942253232\n",
            "epoch: 5 step: 32, loss is 0.010684728622436523\n",
            "epoch: 5 step: 33, loss is 0.17450565099716187\n",
            "epoch: 5 step: 34, loss is 0.006206574849784374\n",
            "epoch: 5 step: 35, loss is 0.004073243588209152\n",
            "epoch: 5 step: 36, loss is 0.01179781835526228\n",
            "epoch: 5 step: 37, loss is 0.03418338671326637\n",
            "epoch: 5 step: 38, loss is 0.01716015860438347\n",
            "epoch: 5 step: 39, loss is 0.014566056430339813\n",
            "epoch: 5 step: 40, loss is 0.0063548944890499115\n",
            "epoch: 5 step: 41, loss is 0.0045005083084106445\n",
            "epoch: 5 step: 42, loss is 0.4177479147911072\n",
            "epoch: 5 step: 43, loss is 0.006358648184686899\n",
            "epoch: 5 step: 44, loss is 0.02193591557443142\n",
            "epoch: 5 step: 45, loss is 0.005840009078383446\n",
            "epoch: 5 step: 46, loss is 0.008651177398860455\n",
            "epoch: 5 step: 47, loss is 0.04219170659780502\n",
            "epoch: 5 step: 48, loss is 0.005119733512401581\n",
            "epoch: 5 step: 49, loss is 0.005486923269927502\n",
            "epoch: 5 step: 50, loss is 0.011279439553618431\n",
            "epoch: 5 step: 51, loss is 0.028704438358545303\n",
            "epoch: 5 step: 52, loss is 0.18040311336517334\n",
            "epoch: 5 step: 53, loss is 0.029757851734757423\n",
            "epoch: 5 step: 54, loss is 0.20197325944900513\n",
            "epoch: 5 step: 55, loss is 0.03015230782330036\n",
            "epoch: 5 step: 56, loss is 0.007524054031819105\n",
            "epoch: 5 step: 57, loss is 0.04247122257947922\n",
            "epoch: 5 step: 58, loss is 0.07376660406589508\n",
            "epoch: 5 step: 59, loss is 0.004289495758712292\n",
            "epoch: 5 step: 60, loss is 0.11675248295068741\n",
            "epoch: 5 step: 61, loss is 0.019638095051050186\n",
            "epoch: 5 step: 62, loss is 0.13927023112773895\n",
            "epoch: 5 step: 63, loss is 0.007457011844962835\n",
            "epoch: 5 step: 64, loss is 0.006234514061361551\n",
            "epoch: 5 step: 65, loss is 0.24880200624465942\n",
            "epoch: 5 step: 66, loss is 0.003125067101791501\n",
            "epoch: 5 step: 67, loss is 0.024113904684782028\n",
            "epoch: 5 step: 68, loss is 0.0035922396928071976\n",
            "epoch: 5 step: 69, loss is 0.01770211197435856\n",
            "epoch: 5 step: 70, loss is 0.011494826525449753\n",
            "epoch: 5 step: 71, loss is 0.004285061731934547\n",
            "epoch: 5 step: 72, loss is 0.0032058570068329573\n",
            "epoch: 5 step: 73, loss is 0.05798673257231712\n",
            "epoch: 5 step: 74, loss is 0.021134981885552406\n",
            "epoch: 5 step: 75, loss is 0.03662649169564247\n",
            "epoch: 5 step: 76, loss is 0.009518986567854881\n",
            "epoch: 5 step: 77, loss is 0.03372945263981819\n",
            "epoch: 5 step: 78, loss is 0.07665669173002243\n",
            "epoch: 5 step: 79, loss is 0.09820709377527237\n",
            "epoch: 5 step: 80, loss is 0.046648018062114716\n",
            "epoch: 5 step: 81, loss is 0.08346165716648102\n",
            "epoch: 5 step: 82, loss is 0.12240297347307205\n",
            "epoch: 5 step: 83, loss is 0.11733498424291611\n",
            "epoch: 5 step: 84, loss is 0.17526116967201233\n",
            "epoch: 5 step: 85, loss is 0.01628846675157547\n",
            "epoch: 5 step: 86, loss is 0.008767838589847088\n",
            "epoch: 5 step: 87, loss is 0.011486739851534367\n",
            "epoch: 5 step: 88, loss is 0.015638649463653564\n",
            "epoch: 5 step: 89, loss is 0.015561867505311966\n",
            "epoch: 5 step: 90, loss is 0.011073848232626915\n",
            "epoch: 5 step: 91, loss is 0.16390712559223175\n",
            "epoch: 5 step: 92, loss is 0.01933777891099453\n",
            "epoch: 5 step: 93, loss is 0.031890179961919785\n",
            "epoch: 5 step: 94, loss is 0.011992814019322395\n",
            "epoch: 5 step: 95, loss is 0.10775712132453918\n",
            "epoch: 5 step: 96, loss is 0.015287735499441624\n",
            "epoch: 5 step: 97, loss is 0.005895786453038454\n",
            "epoch: 5 step: 98, loss is 0.009464995004236698\n",
            "epoch: 5 step: 99, loss is 0.13354593515396118\n",
            "epoch: 5 step: 100, loss is 0.01035296730697155\n",
            "epoch: 5 step: 101, loss is 0.03600683808326721\n",
            "epoch: 5 step: 102, loss is 0.009265919215977192\n",
            "epoch: 5 step: 103, loss is 0.013133681379258633\n",
            "epoch: 5 step: 104, loss is 0.005774342454969883\n",
            "epoch: 5 step: 105, loss is 0.0734376311302185\n",
            "epoch: 5 step: 106, loss is 0.024128645658493042\n",
            "epoch: 5 step: 107, loss is 0.008549095131456852\n",
            "epoch: 5 step: 108, loss is 0.3040778636932373\n",
            "epoch: 5 step: 109, loss is 0.11238455772399902\n",
            "epoch: 5 step: 110, loss is 0.003990077413618565\n",
            "epoch: 5 step: 111, loss is 0.007107811514288187\n",
            "epoch: 5 step: 112, loss is 0.011386508122086525\n",
            "epoch: 5 step: 113, loss is 0.01777546852827072\n",
            "epoch: 5 step: 114, loss is 0.013779882341623306\n",
            "epoch: 5 step: 115, loss is 0.022172553464770317\n",
            "epoch: 5 step: 116, loss is 0.24327203631401062\n",
            "epoch: 5 step: 117, loss is 0.017565544694662094\n",
            "epoch: 5 step: 118, loss is 0.013202809728682041\n",
            "epoch: 5 step: 119, loss is 0.004902539774775505\n",
            "epoch: 5 step: 120, loss is 0.14466308057308197\n",
            "epoch: 5 step: 121, loss is 0.005277965683490038\n",
            "epoch: 5 step: 122, loss is 0.010411606170237064\n",
            "epoch: 5 step: 123, loss is 0.07051538676023483\n",
            "epoch: 5 step: 124, loss is 0.014887074939906597\n",
            "epoch: 5 step: 125, loss is 0.015189520083367825\n",
            "epoch: 5 step: 126, loss is 0.015456781722605228\n",
            "epoch: 5 step: 127, loss is 0.017086954787373543\n",
            "epoch: 5 step: 128, loss is 0.04450874775648117\n",
            "epoch: 5 step: 129, loss is 0.01708795875310898\n",
            "epoch: 5 step: 130, loss is 0.013703832402825356\n",
            "epoch: 5 step: 131, loss is 0.035005051642656326\n",
            "epoch: 5 step: 132, loss is 0.3287259340286255\n",
            "epoch: 5 step: 133, loss is 0.263079434633255\n",
            "epoch: 5 step: 134, loss is 0.1287859082221985\n",
            "epoch: 5 step: 135, loss is 0.0048929923214018345\n",
            "epoch: 5 step: 136, loss is 0.3892746865749359\n",
            "epoch: 5 step: 137, loss is 0.058106910437345505\n",
            "epoch: 5 step: 138, loss is 0.011406784877181053\n",
            "epoch: 5 step: 139, loss is 0.0100734643638134\n",
            "epoch: 5 step: 140, loss is 0.03063266910612583\n",
            "epoch: 5 step: 141, loss is 0.03607630729675293\n",
            "epoch: 5 step: 142, loss is 0.06614663451910019\n",
            "epoch: 5 step: 143, loss is 0.08725946396589279\n",
            "epoch: 5 step: 144, loss is 0.030338136479258537\n",
            "epoch: 5 step: 145, loss is 0.031181633472442627\n",
            "epoch: 5 step: 146, loss is 0.020233070477843285\n",
            "epoch: 5 step: 147, loss is 0.05485667288303375\n",
            "epoch: 5 step: 148, loss is 0.027145713567733765\n",
            "epoch: 5 step: 149, loss is 0.021232515573501587\n",
            "epoch: 5 step: 150, loss is 0.005892428569495678\n",
            "epoch: 5 step: 151, loss is 0.03365559130907059\n",
            "epoch: 5 step: 152, loss is 0.010689222253859043\n",
            "epoch: 5 step: 153, loss is 0.012716145254671574\n",
            "epoch: 5 step: 154, loss is 0.037590473890304565\n",
            "epoch: 5 step: 155, loss is 0.013546915724873543\n",
            "epoch: 5 step: 156, loss is 0.006862170994281769\n",
            "epoch: 5 step: 157, loss is 0.00827370211482048\n",
            "epoch: 5 step: 158, loss is 0.0063448213040828705\n",
            "epoch: 5 step: 159, loss is 0.0037865242920815945\n",
            "epoch: 5 step: 160, loss is 0.06967823952436447\n",
            "epoch: 5 step: 161, loss is 0.006128110457211733\n",
            "epoch: 5 step: 162, loss is 0.13230688869953156\n",
            "epoch: 5 step: 163, loss is 0.007504791021347046\n",
            "epoch: 5 step: 164, loss is 0.024478979408740997\n",
            "epoch: 5 step: 165, loss is 0.04485408961772919\n",
            "epoch: 5 step: 166, loss is 0.023510977625846863\n",
            "epoch: 5 step: 167, loss is 0.07249880582094193\n",
            "epoch: 5 step: 168, loss is 0.012528937309980392\n",
            "epoch: 5 step: 169, loss is 0.14938417077064514\n",
            "epoch: 5 step: 170, loss is 0.14540359377861023\n",
            "epoch: 5 step: 171, loss is 0.002257547341287136\n",
            "epoch: 5 step: 172, loss is 0.01390383206307888\n",
            "epoch: 5 step: 173, loss is 0.006505414377897978\n",
            "epoch: 5 step: 174, loss is 0.018302207812666893\n",
            "epoch: 5 step: 175, loss is 0.01171298697590828\n",
            "epoch: 5 step: 176, loss is 0.010380063205957413\n",
            "epoch: 5 step: 177, loss is 0.004981266800314188\n",
            "epoch: 5 step: 178, loss is 0.012283055111765862\n",
            "epoch: 5 step: 179, loss is 0.008410744369029999\n",
            "epoch: 5 step: 180, loss is 0.008904914371669292\n",
            "epoch: 5 step: 181, loss is 0.007342292927205563\n",
            "epoch: 5 step: 182, loss is 0.007726150564849377\n",
            "epoch: 5 step: 183, loss is 0.013734049163758755\n",
            "epoch: 5 step: 184, loss is 0.010878589935600758\n",
            "epoch: 5 step: 185, loss is 0.004242865834385157\n",
            "epoch: 5 step: 186, loss is 0.5853554606437683\n",
            "epoch: 5 step: 187, loss is 0.09039339423179626\n",
            "epoch: 5 step: 188, loss is 0.07287751138210297\n",
            "epoch: 5 step: 189, loss is 0.026096368208527565\n",
            "epoch: 5 step: 190, loss is 0.027618970721960068\n",
            "epoch: 5 step: 191, loss is 0.007094317115843296\n",
            "epoch: 5 step: 192, loss is 0.008837072178721428\n",
            "epoch: 5 step: 193, loss is 0.009820274077355862\n",
            "epoch: 5 step: 194, loss is 0.02638009563088417\n",
            "epoch: 5 step: 195, loss is 0.03271229565143585\n",
            "epoch: 5 step: 196, loss is 0.007963592186570168\n",
            "epoch: 5 step: 197, loss is 0.006085611879825592\n",
            "epoch: 5 step: 198, loss is 0.013747145421802998\n",
            "epoch: 5 step: 199, loss is 0.003577551105991006\n",
            "epoch: 5 step: 200, loss is 0.017111357301473618\n",
            "epoch: 5 step: 201, loss is 0.00982743501663208\n",
            "epoch: 5 step: 202, loss is 0.014390307478606701\n",
            "epoch: 5 step: 203, loss is 0.008897481486201286\n",
            "epoch: 5 step: 204, loss is 0.003739777719601989\n",
            "epoch: 5 step: 205, loss is 0.01908089965581894\n",
            "epoch: 5 step: 206, loss is 0.02859068103134632\n",
            "epoch: 5 step: 207, loss is 0.01042947918176651\n",
            "epoch: 5 step: 208, loss is 0.25731149315834045\n",
            "epoch: 5 step: 209, loss is 0.15693333745002747\n",
            "epoch: 5 step: 210, loss is 0.00741968397051096\n",
            "epoch: 5 step: 211, loss is 0.01154273934662342\n",
            "epoch: 5 step: 212, loss is 0.08640287816524506\n",
            "epoch: 5 step: 213, loss is 0.015873383730649948\n",
            "epoch: 5 step: 214, loss is 0.06226802244782448\n",
            "epoch: 5 step: 215, loss is 0.05298781394958496\n",
            "epoch: 5 step: 216, loss is 0.044886354357004166\n",
            "epoch: 5 step: 217, loss is 0.029453404247760773\n",
            "epoch: 5 step: 218, loss is 0.016695134341716766\n",
            "epoch: 5 step: 219, loss is 0.008827448822557926\n",
            "epoch: 5 step: 220, loss is 0.016288956627249718\n",
            "epoch: 5 step: 221, loss is 0.012841303832828999\n",
            "epoch: 5 step: 222, loss is 0.00982677936553955\n",
            "epoch: 5 step: 223, loss is 0.008862948976457119\n",
            "epoch: 5 step: 224, loss is 0.025807872414588928\n",
            "epoch: 5 step: 225, loss is 0.008171788416802883\n",
            "epoch: 5 step: 226, loss is 0.010294020175933838\n",
            "epoch: 5 step: 227, loss is 0.006417802069336176\n",
            "epoch: 5 step: 228, loss is 0.00679101562127471\n",
            "epoch: 5 step: 229, loss is 0.005397220607846975\n",
            "epoch: 5 step: 230, loss is 0.20491738617420197\n",
            "epoch: 5 step: 231, loss is 0.020215416327118874\n",
            "epoch: 5 step: 232, loss is 0.004493548534810543\n",
            "epoch: 5 step: 233, loss is 0.024138784036040306\n",
            "epoch: 5 step: 234, loss is 0.02369075082242489\n",
            "epoch: 5 step: 235, loss is 0.12176794558763504\n",
            "epoch: 5 step: 236, loss is 0.030508561059832573\n",
            "epoch: 5 step: 237, loss is 0.1653885394334793\n",
            "epoch: 5 step: 238, loss is 0.01738445833325386\n",
            "epoch: 5 step: 239, loss is 0.059535056352615356\n",
            "epoch: 5 step: 240, loss is 0.06059606745839119\n",
            "epoch: 5 step: 241, loss is 0.006032097153365612\n",
            "epoch: 5 step: 242, loss is 0.026774320751428604\n",
            "epoch: 5 step: 243, loss is 0.021089376881718636\n",
            "epoch: 5 step: 244, loss is 0.03031974472105503\n",
            "epoch: 5 step: 245, loss is 0.15566807985305786\n",
            "epoch: 5 step: 246, loss is 0.012355796992778778\n",
            "epoch: 5 step: 247, loss is 0.20426030457019806\n",
            "epoch: 5 step: 248, loss is 0.02923637069761753\n",
            "epoch: 5 step: 249, loss is 0.011680183932185173\n",
            "epoch: 5 step: 250, loss is 0.05339048430323601\n",
            "epoch: 5 step: 251, loss is 0.1458289623260498\n",
            "epoch: 5 step: 252, loss is 0.040280912071466446\n",
            "epoch: 5 step: 253, loss is 0.02767392247915268\n",
            "epoch: 5 step: 254, loss is 0.048811279237270355\n",
            "epoch: 5 step: 255, loss is 0.195365309715271\n",
            "epoch: 5 step: 256, loss is 0.03751668334007263\n",
            "epoch: 5 step: 257, loss is 0.011423316784203053\n",
            "epoch: 5 step: 258, loss is 0.012864399701356888\n",
            "epoch: 5 step: 259, loss is 0.015061757527291775\n",
            "epoch: 5 step: 260, loss is 0.05199139937758446\n",
            "epoch: 5 step: 261, loss is 0.01213917601853609\n",
            "epoch: 5 step: 262, loss is 0.008977713994681835\n",
            "epoch: 5 step: 263, loss is 0.10676475614309311\n",
            "epoch: 5 step: 264, loss is 0.14840345084667206\n",
            "epoch: 5 step: 265, loss is 0.026358947157859802\n",
            "epoch: 5 step: 266, loss is 0.011802498251199722\n",
            "epoch: 5 step: 267, loss is 0.04129326716065407\n",
            "epoch: 5 step: 268, loss is 0.05619390681385994\n",
            "epoch: 5 step: 269, loss is 0.02220684289932251\n",
            "epoch: 5 step: 270, loss is 0.010316108353435993\n",
            "epoch: 5 step: 271, loss is 0.014450158923864365\n",
            "epoch: 5 step: 272, loss is 0.009137123823165894\n",
            "epoch: 5 step: 273, loss is 0.008664422668516636\n",
            "epoch: 5 step: 274, loss is 0.09190207719802856\n",
            "epoch: 5 step: 275, loss is 0.035477638244628906\n",
            "epoch: 5 step: 276, loss is 0.02111121453344822\n",
            "epoch: 5 step: 277, loss is 0.007833171635866165\n",
            "epoch: 5 step: 278, loss is 0.04843819513916969\n",
            "epoch: 5 step: 279, loss is 0.0061331856995821\n",
            "epoch: 5 step: 280, loss is 0.008284651674330235\n",
            "epoch: 5 step: 281, loss is 0.0071707312017679214\n",
            "epoch: 5 step: 282, loss is 0.005847788881510496\n",
            "epoch: 5 step: 283, loss is 0.0063935453072190285\n",
            "epoch: 5 step: 284, loss is 0.0209173783659935\n",
            "epoch: 5 step: 285, loss is 0.008111631497740746\n",
            "epoch: 5 step: 286, loss is 0.002818393986672163\n",
            "epoch: 5 step: 287, loss is 0.13386474549770355\n",
            "epoch: 5 step: 288, loss is 0.004881543107330799\n",
            "epoch: 5 step: 289, loss is 0.04451751708984375\n",
            "epoch: 5 step: 290, loss is 0.029883449897170067\n",
            "epoch: 5 step: 291, loss is 0.13000871241092682\n",
            "epoch: 5 step: 292, loss is 0.1423957645893097\n",
            "epoch: 5 step: 293, loss is 0.012446438893675804\n",
            "epoch: 5 step: 294, loss is 0.012960640713572502\n",
            "epoch: 5 step: 295, loss is 0.019812658429145813\n",
            "epoch: 5 step: 296, loss is 0.028784077614545822\n",
            "epoch: 5 step: 297, loss is 0.012509118765592575\n",
            "epoch: 5 step: 298, loss is 0.007193209603428841\n",
            "epoch: 5 step: 299, loss is 0.028656188398599625\n",
            "epoch: 5 step: 300, loss is 0.008651508949697018\n",
            "epoch: 5 step: 301, loss is 0.010121193714439869\n",
            "epoch: 5 step: 302, loss is 0.0321025475859642\n",
            "epoch: 5 step: 303, loss is 0.1953430026769638\n",
            "epoch: 5 step: 304, loss is 0.0024941125884652138\n",
            "epoch: 5 step: 305, loss is 0.031622886657714844\n",
            "epoch: 5 step: 306, loss is 0.023621512576937675\n",
            "epoch: 5 step: 307, loss is 0.027641385793685913\n",
            "epoch: 5 step: 308, loss is 0.006325491704046726\n",
            "epoch: 5 step: 309, loss is 0.009773261845111847\n",
            "epoch: 5 step: 310, loss is 0.07902281731367111\n",
            "epoch: 5 step: 311, loss is 0.006337538827210665\n",
            "epoch: 5 step: 312, loss is 0.08180047571659088\n",
            "epoch: 5 step: 313, loss is 0.013009353540837765\n",
            "epoch: 5 step: 314, loss is 0.01031711045652628\n",
            "epoch: 5 step: 315, loss is 0.01844848319888115\n",
            "epoch: 5 step: 316, loss is 0.04619014263153076\n",
            "epoch: 5 step: 317, loss is 0.03205393627285957\n",
            "epoch: 5 step: 318, loss is 0.12299279123544693\n",
            "epoch: 5 step: 319, loss is 0.0059336828999221325\n",
            "epoch: 5 step: 320, loss is 0.2123543918132782\n",
            "epoch: 5 step: 321, loss is 0.04362475499510765\n",
            "epoch: 5 step: 322, loss is 0.01359618455171585\n",
            "epoch: 5 step: 323, loss is 0.011107435449957848\n",
            "epoch: 5 step: 324, loss is 0.0055880481377244\n",
            "epoch: 5 step: 325, loss is 0.006784916389733553\n",
            "epoch: 5 step: 326, loss is 0.010603051632642746\n",
            "epoch: 5 step: 327, loss is 0.006183343008160591\n",
            "epoch: 5 step: 328, loss is 0.014500273391604424\n",
            "epoch: 5 step: 329, loss is 0.03583633527159691\n",
            "epoch: 5 step: 330, loss is 0.024860041216015816\n",
            "epoch: 5 step: 331, loss is 0.00920289009809494\n",
            "epoch: 5 step: 332, loss is 0.01623798906803131\n",
            "epoch: 5 step: 333, loss is 0.005648307967931032\n",
            "epoch: 5 step: 334, loss is 0.010042965412139893\n",
            "epoch: 5 step: 335, loss is 0.01703295111656189\n",
            "epoch: 5 step: 336, loss is 0.021468589082360268\n",
            "epoch: 5 step: 337, loss is 0.11317922174930573\n",
            "epoch: 5 step: 338, loss is 0.00665977131575346\n",
            "epoch: 5 step: 339, loss is 0.006034770514816046\n",
            "epoch: 5 step: 340, loss is 0.004723489750176668\n",
            "epoch: 5 step: 341, loss is 0.017181091010570526\n",
            "epoch: 5 step: 342, loss is 0.008726270869374275\n",
            "epoch: 5 step: 343, loss is 0.11444120854139328\n",
            "epoch: 5 step: 344, loss is 0.04640068858861923\n",
            "epoch: 5 step: 345, loss is 0.015616249293088913\n",
            "epoch: 5 step: 346, loss is 0.01178581453859806\n",
            "epoch: 5 step: 347, loss is 0.006045343820005655\n",
            "epoch: 5 step: 348, loss is 0.05944470688700676\n",
            "epoch: 5 step: 349, loss is 0.027380870655179024\n",
            "epoch: 5 step: 350, loss is 0.12616856396198273\n",
            "epoch: 5 step: 351, loss is 0.04664038121700287\n",
            "epoch: 5 step: 352, loss is 0.013076426461338997\n",
            "epoch: 5 step: 353, loss is 0.014202018268406391\n",
            "epoch: 5 step: 354, loss is 0.07777089625597\n",
            "epoch: 5 step: 355, loss is 0.018000489100813866\n",
            "epoch: 5 step: 356, loss is 0.02185225859284401\n",
            "epoch: 5 step: 357, loss is 0.014917748048901558\n",
            "epoch: 5 step: 358, loss is 0.009440870955586433\n",
            "epoch: 5 step: 359, loss is 0.030384108424186707\n",
            "epoch: 5 step: 360, loss is 0.01049621682614088\n",
            "epoch: 5 step: 361, loss is 0.1570209413766861\n",
            "epoch: 5 step: 362, loss is 0.0763443261384964\n",
            "epoch: 5 step: 363, loss is 0.011932522989809513\n",
            "epoch: 5 step: 364, loss is 0.2901233732700348\n",
            "epoch: 5 step: 365, loss is 0.09074822068214417\n",
            "epoch: 5 step: 366, loss is 0.0030337166972458363\n",
            "epoch: 5 step: 367, loss is 0.027605975046753883\n",
            "epoch: 5 step: 368, loss is 0.06860167533159256\n",
            "epoch: 5 step: 369, loss is 0.052944812923669815\n",
            "epoch: 5 step: 370, loss is 0.010333724319934845\n",
            "epoch: 5 step: 371, loss is 0.008743698708713055\n",
            "epoch: 5 step: 372, loss is 0.04179345816373825\n",
            "epoch: 5 step: 373, loss is 0.20725353062152863\n",
            "epoch: 5 step: 374, loss is 0.26269492506980896\n",
            "epoch: 5 step: 375, loss is 0.017546923831105232\n",
            "epoch: 5 step: 376, loss is 0.005683097988367081\n",
            "epoch: 5 step: 377, loss is 0.12134139984846115\n",
            "epoch: 5 step: 378, loss is 0.19509610533714294\n",
            "epoch: 5 step: 379, loss is 0.01476883701980114\n",
            "epoch: 5 step: 380, loss is 0.01373333390802145\n",
            "epoch: 5 step: 381, loss is 0.005505131557583809\n",
            "epoch: 5 step: 382, loss is 0.08287236094474792\n",
            "epoch: 5 step: 383, loss is 0.02249428629875183\n",
            "epoch: 5 step: 384, loss is 0.006875293795019388\n",
            "epoch: 5 step: 385, loss is 0.008971486240625381\n",
            "epoch: 5 step: 386, loss is 0.029807550832629204\n",
            "epoch: 5 step: 387, loss is 0.043329451233148575\n",
            "epoch: 5 step: 388, loss is 0.0077570960856974125\n",
            "epoch: 5 step: 389, loss is 0.01224564015865326\n",
            "epoch: 5 step: 390, loss is 0.02082526870071888\n",
            "epoch: 5 step: 391, loss is 0.00639078626409173\n",
            "epoch: 5 step: 392, loss is 0.028047844767570496\n",
            "epoch: 5 step: 393, loss is 0.011701056733727455\n",
            "epoch: 5 step: 394, loss is 0.014841708354651928\n",
            "epoch: 5 step: 395, loss is 0.011544954031705856\n",
            "epoch: 5 step: 396, loss is 0.16850315034389496\n",
            "epoch: 5 step: 397, loss is 0.07621430605649948\n",
            "epoch: 5 step: 398, loss is 0.007666869554668665\n",
            "epoch: 5 step: 399, loss is 0.13222695887088776\n",
            "epoch: 5 step: 400, loss is 0.048679646104574203\n",
            "epoch: 5 step: 401, loss is 0.10685507953166962\n",
            "epoch: 5 step: 402, loss is 0.017753463238477707\n",
            "epoch: 5 step: 403, loss is 0.010201714932918549\n",
            "epoch: 5 step: 404, loss is 0.1034732386469841\n",
            "epoch: 5 step: 405, loss is 0.014886284247040749\n",
            "epoch: 5 step: 406, loss is 0.04822274670004845\n",
            "epoch: 5 step: 407, loss is 0.01647443138062954\n",
            "epoch: 5 step: 408, loss is 0.021257309243083\n",
            "epoch: 5 step: 409, loss is 0.01801866851747036\n",
            "epoch: 5 step: 410, loss is 0.0879349559545517\n",
            "epoch: 5 step: 411, loss is 0.02553059533238411\n",
            "epoch: 5 step: 412, loss is 0.01450490951538086\n",
            "epoch: 5 step: 413, loss is 0.043611977249383926\n",
            "epoch: 5 step: 414, loss is 0.009737934917211533\n",
            "epoch: 5 step: 415, loss is 0.0073469593189656734\n",
            "epoch: 5 step: 416, loss is 0.003341207979246974\n",
            "epoch: 5 step: 417, loss is 0.010703017003834248\n",
            "epoch: 5 step: 418, loss is 0.0672081932425499\n",
            "epoch: 5 step: 419, loss is 0.0063460697419941425\n",
            "epoch: 5 step: 420, loss is 0.02103084698319435\n",
            "epoch: 5 step: 421, loss is 0.004747254308313131\n",
            "epoch: 5 step: 422, loss is 0.046526823192834854\n",
            "epoch: 5 step: 423, loss is 0.023456478491425514\n",
            "epoch: 5 step: 424, loss is 0.008451608009636402\n",
            "epoch: 5 step: 425, loss is 0.006642807740718126\n",
            "epoch: 5 step: 426, loss is 0.012598426081240177\n",
            "epoch: 5 step: 427, loss is 0.08339501917362213\n",
            "epoch: 5 step: 428, loss is 0.025921041145920753\n",
            "epoch: 5 step: 429, loss is 0.0026874756440520287\n",
            "epoch: 5 step: 430, loss is 0.013324611820280552\n",
            "epoch: 5 step: 431, loss is 0.010722651146352291\n",
            "epoch: 5 step: 432, loss is 0.009842432104051113\n",
            "epoch: 5 step: 433, loss is 0.07554873824119568\n",
            "epoch: 5 step: 434, loss is 0.015386530198156834\n",
            "epoch: 5 step: 435, loss is 0.006817720830440521\n",
            "epoch: 5 step: 436, loss is 0.0062069459818303585\n",
            "epoch: 5 step: 437, loss is 0.006426759995520115\n",
            "epoch: 5 step: 438, loss is 0.005967525765299797\n",
            "epoch: 5 step: 439, loss is 0.004045347683131695\n",
            "epoch: 5 step: 440, loss is 0.0038443277589976788\n",
            "epoch: 5 step: 441, loss is 0.04254086688160896\n",
            "epoch: 5 step: 442, loss is 0.12877267599105835\n",
            "epoch: 5 step: 443, loss is 0.014833066612482071\n",
            "epoch: 5 step: 444, loss is 0.007112237624824047\n",
            "epoch: 5 step: 445, loss is 0.0083700530230999\n",
            "epoch: 5 step: 446, loss is 0.01079461257904768\n",
            "epoch: 5 step: 447, loss is 0.007630740292370319\n",
            "epoch: 5 step: 448, loss is 0.06463314592838287\n",
            "epoch: 5 step: 449, loss is 0.005803145933896303\n",
            "epoch: 5 step: 450, loss is 0.19523228704929352\n",
            "epoch: 5 step: 451, loss is 0.008970681577920914\n",
            "epoch: 5 step: 452, loss is 0.00959770381450653\n",
            "epoch: 5 step: 453, loss is 0.007398085203021765\n",
            "epoch: 5 step: 454, loss is 0.013789908029139042\n",
            "epoch: 5 step: 455, loss is 0.22621016204357147\n",
            "epoch: 5 step: 456, loss is 0.026129266247153282\n",
            "epoch: 5 step: 457, loss is 0.025450335815548897\n",
            "epoch: 5 step: 458, loss is 0.0038364992942661047\n",
            "epoch: 5 step: 459, loss is 0.006538949906826019\n",
            "epoch: 5 step: 460, loss is 0.015176810324192047\n",
            "epoch: 5 step: 461, loss is 0.004473618697375059\n",
            "epoch: 5 step: 462, loss is 0.09586688876152039\n",
            "epoch: 5 step: 463, loss is 0.0060656629502773285\n",
            "epoch: 5 step: 464, loss is 0.006655556615442038\n",
            "epoch: 5 step: 465, loss is 0.012806407175958157\n",
            "epoch: 5 step: 466, loss is 0.02559572272002697\n",
            "epoch: 5 step: 467, loss is 0.26262030005455017\n",
            "epoch: 5 step: 468, loss is 0.30989059805870056\n",
            "epoch: 5 step: 469, loss is 0.012971100397408009\n",
            "epoch: 5 step: 470, loss is 0.020102644339203835\n",
            "epoch: 5 step: 471, loss is 0.013671757653355598\n",
            "epoch: 5 step: 472, loss is 0.058802489191293716\n",
            "epoch: 5 step: 473, loss is 0.015006360597908497\n",
            "epoch: 5 step: 474, loss is 0.17816399037837982\n",
            "epoch: 5 step: 475, loss is 0.01608586125075817\n",
            "epoch: 5 step: 476, loss is 0.017104754224419594\n",
            "epoch: 5 step: 477, loss is 0.09498422592878342\n",
            "epoch: 5 step: 478, loss is 0.007630759850144386\n",
            "epoch: 5 step: 479, loss is 0.02298634871840477\n",
            "epoch: 5 step: 480, loss is 0.11239945143461227\n",
            "epoch: 5 step: 481, loss is 0.008650419302284718\n",
            "epoch: 5 step: 482, loss is 0.27494296431541443\n",
            "epoch: 5 step: 483, loss is 0.022203737869858742\n",
            "epoch: 5 step: 484, loss is 0.023741841316223145\n",
            "epoch: 5 step: 485, loss is 0.009171037003397942\n",
            "epoch: 5 step: 486, loss is 0.033660419285297394\n",
            "epoch: 5 step: 487, loss is 0.010626383125782013\n",
            "epoch: 5 step: 488, loss is 0.013324987143278122\n",
            "epoch: 5 step: 489, loss is 0.02191295102238655\n",
            "epoch: 5 step: 490, loss is 0.13380879163742065\n",
            "epoch: 5 step: 491, loss is 0.003931205254048109\n",
            "epoch: 5 step: 492, loss is 0.007734634447842836\n",
            "epoch: 5 step: 493, loss is 0.017634132876992226\n",
            "epoch: 5 step: 494, loss is 0.014340936206281185\n",
            "epoch: 5 step: 495, loss is 0.013523731380701065\n",
            "epoch: 5 step: 496, loss is 0.007730637677013874\n",
            "epoch: 5 step: 497, loss is 0.01729779690504074\n",
            "epoch: 5 step: 498, loss is 0.17460674047470093\n",
            "epoch: 5 step: 499, loss is 0.004765896126627922\n",
            "epoch: 5 step: 500, loss is 0.02316710166633129\n",
            "epoch: 5 step: 501, loss is 0.014891194179654121\n",
            "epoch: 5 step: 502, loss is 0.008502488024532795\n",
            "epoch: 5 step: 503, loss is 0.1076320931315422\n",
            "epoch: 5 step: 504, loss is 0.023010218515992165\n",
            "epoch: 5 step: 505, loss is 0.01674520969390869\n",
            "epoch: 5 step: 506, loss is 0.02687114104628563\n",
            "epoch: 5 step: 507, loss is 0.010631075128912926\n",
            "epoch: 5 step: 508, loss is 0.08231600373983383\n",
            "epoch: 5 step: 509, loss is 0.025762977078557014\n",
            "epoch: 5 step: 510, loss is 0.07375103235244751\n",
            "epoch: 5 step: 511, loss is 0.010333843529224396\n",
            "epoch: 5 step: 512, loss is 0.009384547360241413\n",
            "epoch: 5 step: 513, loss is 0.019854895770549774\n",
            "epoch: 5 step: 514, loss is 0.028878789395093918\n",
            "epoch: 5 step: 515, loss is 0.00657721096649766\n",
            "epoch: 5 step: 516, loss is 0.009530959650874138\n",
            "epoch: 5 step: 517, loss is 0.0033207149244844913\n",
            "epoch: 5 step: 518, loss is 0.006643327884376049\n",
            "epoch: 5 step: 519, loss is 0.010061616078019142\n",
            "epoch: 5 step: 520, loss is 0.012626320123672485\n",
            "epoch: 5 step: 521, loss is 0.16582027077674866\n",
            "epoch: 5 step: 522, loss is 0.029124869033694267\n",
            "epoch: 5 step: 523, loss is 0.0043923743069171906\n",
            "epoch: 5 step: 524, loss is 0.011541550979018211\n",
            "epoch: 5 step: 525, loss is 0.012736013159155846\n",
            "epoch: 5 step: 526, loss is 0.22294925153255463\n",
            "epoch: 5 step: 527, loss is 0.0059302556328475475\n",
            "epoch: 5 step: 528, loss is 0.01169507671147585\n",
            "epoch: 5 step: 529, loss is 0.009362787008285522\n",
            "epoch: 5 step: 530, loss is 0.01613744907081127\n",
            "epoch: 5 step: 531, loss is 0.03323386237025261\n",
            "epoch: 5 step: 532, loss is 0.006635856349021196\n",
            "epoch: 5 step: 533, loss is 0.010504668578505516\n",
            "epoch: 5 step: 534, loss is 0.014898674562573433\n",
            "epoch: 5 step: 535, loss is 0.2483436018228531\n",
            "epoch: 5 step: 536, loss is 0.06466463208198547\n",
            "epoch: 5 step: 537, loss is 0.013728739693760872\n",
            "epoch: 5 step: 538, loss is 0.01395962294191122\n",
            "epoch: 5 step: 539, loss is 0.01428692415356636\n",
            "epoch: 5 step: 540, loss is 0.009502808563411236\n",
            "epoch: 5 step: 541, loss is 0.01998734474182129\n",
            "epoch: 5 step: 542, loss is 0.010216180235147476\n",
            "epoch: 5 step: 543, loss is 0.060918401926755905\n",
            "epoch: 5 step: 544, loss is 0.012868192046880722\n",
            "epoch: 5 step: 545, loss is 0.004959389567375183\n",
            "epoch: 5 step: 546, loss is 0.012206095270812511\n",
            "epoch: 5 step: 547, loss is 0.1726122796535492\n",
            "epoch: 5 step: 548, loss is 0.008123176172375679\n",
            "epoch: 5 step: 549, loss is 0.007099673617631197\n",
            "epoch: 5 step: 550, loss is 0.016718696802854538\n",
            "epoch: 5 step: 551, loss is 0.006967484485358\n",
            "epoch: 5 step: 552, loss is 0.009635277092456818\n",
            "epoch: 5 step: 553, loss is 0.6719876527786255\n",
            "epoch: 5 step: 554, loss is 0.0024936124682426453\n",
            "epoch: 5 step: 555, loss is 0.03365207836031914\n",
            "epoch: 5 step: 556, loss is 0.014868268743157387\n",
            "epoch: 5 step: 557, loss is 0.006775563582777977\n",
            "epoch: 5 step: 558, loss is 0.0032405701931566\n",
            "epoch: 5 step: 559, loss is 0.019522154703736305\n",
            "epoch: 5 step: 560, loss is 0.016398044303059578\n",
            "epoch: 5 step: 561, loss is 0.07437968254089355\n",
            "epoch: 5 step: 562, loss is 0.006929341703653336\n",
            "epoch: 5 step: 563, loss is 0.2014114111661911\n",
            "epoch: 5 step: 564, loss is 0.006882315501570702\n",
            "epoch: 5 step: 565, loss is 0.09831437468528748\n",
            "epoch: 5 step: 566, loss is 0.00357010867446661\n",
            "epoch: 5 step: 567, loss is 0.00979149341583252\n",
            "epoch: 5 step: 568, loss is 0.13679905235767365\n",
            "epoch: 5 step: 569, loss is 0.2591639459133148\n",
            "epoch: 5 step: 570, loss is 0.0019112668232992291\n",
            "epoch: 5 step: 571, loss is 0.008351319469511509\n",
            "epoch: 5 step: 572, loss is 0.054255321621894836\n",
            "epoch: 5 step: 573, loss is 0.01870678924024105\n",
            "epoch: 5 step: 574, loss is 0.004137824289500713\n",
            "epoch: 5 step: 575, loss is 0.010829949751496315\n",
            "epoch: 5 step: 576, loss is 0.10437769442796707\n",
            "epoch: 5 step: 577, loss is 0.022771932184696198\n",
            "epoch: 5 step: 578, loss is 0.0056073409505188465\n",
            "epoch: 5 step: 579, loss is 0.008836873807013035\n",
            "epoch: 5 step: 580, loss is 0.0041932458989322186\n",
            "epoch: 5 step: 581, loss is 0.004902055952697992\n",
            "epoch: 5 step: 582, loss is 0.01510615274310112\n",
            "epoch: 5 step: 583, loss is 0.04595835879445076\n",
            "epoch: 5 step: 584, loss is 0.011636175215244293\n",
            "epoch: 5 step: 585, loss is 0.00831742212176323\n",
            "epoch: 5 step: 586, loss is 0.0057862610556185246\n",
            "epoch: 5 step: 587, loss is 0.00847641471773386\n",
            "epoch: 5 step: 588, loss is 0.039840441197156906\n",
            "epoch: 5 step: 589, loss is 0.010180311277508736\n",
            "epoch: 5 step: 590, loss is 0.013317769393324852\n",
            "epoch: 5 step: 591, loss is 0.01023170631378889\n",
            "epoch: 5 step: 592, loss is 0.028849594295024872\n",
            "epoch: 5 step: 593, loss is 0.007228069938719273\n",
            "epoch: 5 step: 594, loss is 0.0034441847819834948\n",
            "epoch: 5 step: 595, loss is 0.012059803120791912\n",
            "epoch: 5 step: 596, loss is 0.22479623556137085\n",
            "epoch: 5 step: 597, loss is 0.0100104296579957\n",
            "epoch: 5 step: 598, loss is 0.008675536140799522\n",
            "epoch: 5 step: 599, loss is 0.007583429105579853\n",
            "epoch: 5 step: 600, loss is 0.005111244507133961\n",
            "epoch: 5 step: 601, loss is 0.009660939686000347\n",
            "epoch: 5 step: 602, loss is 0.18208886682987213\n",
            "epoch: 5 step: 603, loss is 0.10358984023332596\n",
            "epoch: 5 step: 604, loss is 0.013375409878790379\n",
            "epoch: 5 step: 605, loss is 0.012672062031924725\n",
            "epoch: 5 step: 606, loss is 0.1269967257976532\n",
            "epoch: 5 step: 607, loss is 0.009130079299211502\n",
            "epoch: 5 step: 608, loss is 0.003885492216795683\n",
            "epoch: 5 step: 609, loss is 0.09861411154270172\n",
            "epoch: 5 step: 610, loss is 0.014371429570019245\n",
            "epoch: 5 step: 611, loss is 0.008743541315197945\n",
            "epoch: 5 step: 612, loss is 0.027536140754818916\n",
            "epoch: 5 step: 613, loss is 0.015464741736650467\n",
            "epoch: 5 step: 614, loss is 0.007725066505372524\n",
            "epoch: 5 step: 615, loss is 0.0474892258644104\n",
            "epoch: 5 step: 616, loss is 0.013355003669857979\n",
            "epoch: 5 step: 617, loss is 0.029219599440693855\n",
            "epoch: 5 step: 618, loss is 0.009021875448524952\n",
            "epoch: 5 step: 619, loss is 0.013959262520074844\n",
            "epoch: 5 step: 620, loss is 0.005816875025629997\n",
            "epoch: 5 step: 621, loss is 0.012811207212507725\n",
            "epoch: 5 step: 622, loss is 0.02481633797287941\n",
            "epoch: 5 step: 623, loss is 0.009766635484993458\n",
            "epoch: 5 step: 624, loss is 0.007737457752227783\n",
            "epoch: 5 step: 625, loss is 0.1748548150062561\n",
            "epoch: 5 step: 626, loss is 0.18597130477428436\n",
            "epoch: 5 step: 627, loss is 0.008444408886134624\n",
            "epoch: 5 step: 628, loss is 0.004916154779493809\n",
            "epoch: 5 step: 629, loss is 0.01519583910703659\n",
            "epoch: 5 step: 630, loss is 0.006522343959659338\n",
            "epoch: 5 step: 631, loss is 0.015869447961449623\n",
            "epoch: 5 step: 632, loss is 0.02288205921649933\n",
            "epoch: 5 step: 633, loss is 0.015417398884892464\n",
            "epoch: 5 step: 634, loss is 0.0022911694832146168\n",
            "epoch: 5 step: 635, loss is 0.008800207637250423\n",
            "epoch: 5 step: 636, loss is 0.00276254303753376\n",
            "epoch: 5 step: 637, loss is 0.05034638196229935\n",
            "epoch: 5 step: 638, loss is 0.008390776813030243\n",
            "epoch: 5 step: 639, loss is 0.05526776611804962\n",
            "epoch: 5 step: 640, loss is 0.007740636356174946\n",
            "epoch: 5 step: 641, loss is 0.013129048980772495\n",
            "epoch: 5 step: 642, loss is 0.01707056164741516\n",
            "epoch: 5 step: 643, loss is 0.008317766711115837\n",
            "epoch: 5 step: 644, loss is 0.11456771194934845\n",
            "epoch: 5 step: 645, loss is 0.008232607506215572\n",
            "epoch: 5 step: 646, loss is 0.013628939166665077\n",
            "epoch: 5 step: 647, loss is 0.19719558954238892\n",
            "epoch: 5 step: 648, loss is 0.12625427544116974\n",
            "epoch: 5 step: 649, loss is 0.15542632341384888\n",
            "epoch: 5 step: 650, loss is 0.004261222667992115\n",
            "epoch: 5 step: 651, loss is 0.0733540803194046\n",
            "epoch: 5 step: 652, loss is 0.008630669675767422\n",
            "epoch: 5 step: 653, loss is 0.0076943663880229\n",
            "epoch: 5 step: 654, loss is 0.01744384691119194\n",
            "epoch: 5 step: 655, loss is 0.30805057287216187\n",
            "epoch: 5 step: 656, loss is 0.030959125608205795\n",
            "epoch: 5 step: 657, loss is 0.0066078584641218185\n",
            "epoch: 5 step: 658, loss is 0.010804569348692894\n",
            "epoch: 5 step: 659, loss is 0.2015560418367386\n",
            "epoch: 5 step: 660, loss is 0.026699377223849297\n",
            "epoch: 5 step: 661, loss is 0.11032392829656601\n",
            "epoch: 5 step: 662, loss is 0.0954783484339714\n",
            "epoch: 5 step: 663, loss is 0.008603140711784363\n",
            "epoch: 5 step: 664, loss is 0.027970856055617332\n",
            "epoch: 5 step: 665, loss is 0.008174700662493706\n",
            "epoch: 5 step: 666, loss is 0.015532704070210457\n",
            "epoch: 5 step: 667, loss is 0.46910879015922546\n",
            "epoch: 5 step: 668, loss is 0.01807604357600212\n",
            "epoch: 5 step: 669, loss is 0.024969762191176414\n",
            "epoch: 5 step: 670, loss is 0.010624386370182037\n",
            "epoch: 5 step: 671, loss is 0.02117142826318741\n",
            "epoch: 5 step: 672, loss is 0.012621845118701458\n",
            "epoch: 5 step: 673, loss is 0.07084960490465164\n",
            "epoch: 5 step: 674, loss is 0.05207313969731331\n",
            "epoch: 5 step: 675, loss is 0.007562453858554363\n",
            "epoch: 5 step: 676, loss is 0.010455048643052578\n",
            "epoch: 5 step: 677, loss is 0.017790667712688446\n",
            "epoch: 5 step: 678, loss is 0.029252365231513977\n",
            "epoch: 5 step: 679, loss is 0.04332587495446205\n",
            "epoch: 5 step: 680, loss is 0.03739515691995621\n",
            "epoch: 5 step: 681, loss is 0.24234852194786072\n",
            "epoch: 5 step: 682, loss is 0.011299666948616505\n",
            "epoch: 5 step: 683, loss is 0.005429879296571016\n",
            "epoch: 5 step: 684, loss is 0.006686649285256863\n",
            "epoch: 5 step: 685, loss is 0.02983756549656391\n",
            "epoch: 5 step: 686, loss is 0.01419546827673912\n",
            "epoch: 5 step: 687, loss is 0.034239355474710464\n",
            "epoch: 5 step: 688, loss is 0.07767084240913391\n",
            "epoch: 5 step: 689, loss is 0.012511570937931538\n",
            "epoch: 5 step: 690, loss is 0.019865529611706734\n",
            "epoch: 5 step: 691, loss is 0.007436995394527912\n",
            "epoch: 5 step: 692, loss is 0.006617323029786348\n",
            "epoch: 5 step: 693, loss is 0.028794975951313972\n",
            "epoch: 5 step: 694, loss is 0.026393769308924675\n",
            "epoch: 5 step: 695, loss is 0.31113308668136597\n",
            "epoch: 5 step: 696, loss is 0.016271261498332024\n",
            "epoch: 5 step: 697, loss is 0.007156773004680872\n",
            "epoch: 5 step: 698, loss is 0.004664841573685408\n",
            "epoch: 5 step: 699, loss is 0.030271008610725403\n",
            "epoch: 5 step: 700, loss is 0.007078935392200947\n",
            "epoch: 5 step: 701, loss is 0.016057660803198814\n",
            "epoch: 5 step: 702, loss is 0.005601493176072836\n",
            "epoch: 5 step: 703, loss is 0.008438389748334885\n",
            "epoch: 5 step: 704, loss is 0.009293300099670887\n",
            "epoch: 5 step: 705, loss is 0.003318630624562502\n",
            "epoch: 5 step: 706, loss is 0.05291074514389038\n",
            "epoch: 5 step: 707, loss is 0.08739645034074783\n",
            "epoch: 5 step: 708, loss is 0.010925047099590302\n",
            "epoch: 5 step: 709, loss is 0.023573655635118484\n",
            "epoch: 5 step: 710, loss is 0.01610950380563736\n",
            "epoch: 5 step: 711, loss is 0.00593008566647768\n",
            "epoch: 5 step: 712, loss is 0.07483822107315063\n",
            "epoch: 5 step: 713, loss is 0.012629342265427113\n",
            "epoch: 5 step: 714, loss is 0.013071763329207897\n",
            "epoch: 5 step: 715, loss is 0.005124241579324007\n",
            "epoch: 5 step: 716, loss is 0.00769571028649807\n",
            "epoch: 5 step: 717, loss is 0.09263305366039276\n",
            "epoch: 5 step: 718, loss is 0.01824747584760189\n",
            "epoch: 5 step: 719, loss is 0.007604393642395735\n",
            "epoch: 5 step: 720, loss is 0.004443556535989046\n",
            "epoch: 5 step: 721, loss is 0.006915252190083265\n",
            "epoch: 5 step: 722, loss is 0.0035911707673221827\n",
            "epoch: 5 step: 723, loss is 0.007414877414703369\n",
            "epoch: 5 step: 724, loss is 0.009942825883626938\n",
            "epoch: 5 step: 725, loss is 0.011041279882192612\n",
            "epoch: 5 step: 726, loss is 0.011999598704278469\n",
            "epoch: 5 step: 727, loss is 0.011120426468551159\n",
            "epoch: 5 step: 728, loss is 0.10324627161026001\n",
            "epoch: 5 step: 729, loss is 0.10905104130506516\n",
            "epoch: 5 step: 730, loss is 0.0068433573469519615\n",
            "epoch: 5 step: 731, loss is 0.019733767956495285\n",
            "epoch: 5 step: 732, loss is 0.07149095088243484\n",
            "epoch: 5 step: 733, loss is 0.013658907264471054\n",
            "epoch: 5 step: 734, loss is 0.013758765533566475\n",
            "epoch: 5 step: 735, loss is 0.16894757747650146\n",
            "epoch: 5 step: 736, loss is 0.01799023523926735\n",
            "epoch: 5 step: 737, loss is 0.0206241924315691\n",
            "epoch: 5 step: 738, loss is 0.005320242140442133\n",
            "epoch: 5 step: 739, loss is 0.012529416009783745\n",
            "epoch: 5 step: 740, loss is 0.004742352291941643\n",
            "epoch: 5 step: 741, loss is 0.01160190999507904\n",
            "epoch: 5 step: 742, loss is 0.11488658934831619\n",
            "epoch: 5 step: 743, loss is 0.05302862450480461\n",
            "epoch: 5 step: 744, loss is 0.013889819383621216\n",
            "epoch: 5 step: 745, loss is 0.024418512359261513\n",
            "epoch: 5 step: 746, loss is 0.04241369292140007\n",
            "epoch: 5 step: 747, loss is 0.03884857892990112\n",
            "epoch: 5 step: 748, loss is 0.010933266952633858\n",
            "epoch: 5 step: 749, loss is 0.008732118643820286\n",
            "epoch: 5 step: 750, loss is 0.009248580783605576\n",
            "epoch: 5 step: 751, loss is 0.014417067170143127\n",
            "epoch: 5 step: 752, loss is 0.03075176663696766\n",
            "epoch: 5 step: 753, loss is 0.0058444105088710785\n",
            "epoch: 5 step: 754, loss is 0.009032306261360645\n",
            "epoch: 5 step: 755, loss is 0.005837564822286367\n",
            "epoch: 5 step: 756, loss is 0.03506554663181305\n",
            "epoch: 5 step: 757, loss is 0.038295067846775055\n",
            "epoch: 5 step: 758, loss is 0.14258818328380585\n",
            "epoch: 5 step: 759, loss is 0.004694483708590269\n",
            "epoch: 5 step: 760, loss is 0.1435256451368332\n",
            "epoch: 5 step: 761, loss is 0.011698910966515541\n",
            "epoch: 5 step: 762, loss is 0.0031586301047354937\n",
            "epoch: 5 step: 763, loss is 0.006753844674676657\n",
            "epoch: 5 step: 764, loss is 0.0021077548153698444\n",
            "epoch: 5 step: 765, loss is 0.013524944894015789\n",
            "epoch: 5 step: 766, loss is 0.050844430923461914\n",
            "epoch: 5 step: 767, loss is 0.011190660297870636\n",
            "epoch: 5 step: 768, loss is 0.04312548041343689\n",
            "epoch: 5 step: 769, loss is 0.6206152439117432\n",
            "epoch: 5 step: 770, loss is 0.004573293495923281\n",
            "epoch: 5 step: 771, loss is 0.2301124930381775\n",
            "epoch: 5 step: 772, loss is 0.06150777265429497\n",
            "epoch: 5 step: 773, loss is 0.009029378183186054\n",
            "epoch: 5 step: 774, loss is 0.004795385990291834\n",
            "epoch: 5 step: 775, loss is 0.030145032331347466\n",
            "epoch: 5 step: 776, loss is 0.013753448612987995\n",
            "epoch: 5 step: 777, loss is 0.03511535003781319\n",
            "epoch: 5 step: 778, loss is 0.022470388561487198\n",
            "epoch: 5 step: 779, loss is 0.012700564227998257\n",
            "epoch: 5 step: 780, loss is 0.24577489495277405\n",
            "epoch: 5 step: 781, loss is 0.17962458729743958\n",
            "epoch: 5 step: 782, loss is 0.009172661229968071\n",
            "epoch: 5 step: 783, loss is 0.016284754499793053\n",
            "epoch: 5 step: 784, loss is 0.0755005031824112\n",
            "epoch: 5 step: 785, loss is 0.005995322018861771\n",
            "epoch: 5 step: 786, loss is 0.01678161509335041\n",
            "epoch: 5 step: 787, loss is 0.00762790534645319\n",
            "epoch: 5 step: 788, loss is 0.013760361820459366\n",
            "epoch: 5 step: 789, loss is 0.015591973438858986\n",
            "epoch: 5 step: 790, loss is 0.023846125230193138\n",
            "epoch: 5 step: 791, loss is 0.31170380115509033\n",
            "epoch: 5 step: 792, loss is 0.005333996843546629\n",
            "epoch: 5 step: 793, loss is 0.009394846856594086\n",
            "epoch: 5 step: 794, loss is 0.00821664184331894\n",
            "epoch: 5 step: 795, loss is 0.021627023816108704\n",
            "epoch: 5 step: 796, loss is 0.007057516369968653\n",
            "epoch: 5 step: 797, loss is 0.005778168328106403\n",
            "epoch: 5 step: 798, loss is 0.03983617201447487\n",
            "epoch: 5 step: 799, loss is 0.0070375725626945496\n",
            "epoch: 5 step: 800, loss is 0.1240094006061554\n",
            "epoch: 5 step: 801, loss is 0.016937509179115295\n",
            "epoch: 5 step: 802, loss is 0.03347310423851013\n",
            "epoch: 5 step: 803, loss is 0.0059469775296747684\n",
            "epoch: 5 step: 804, loss is 0.022485699504613876\n",
            "epoch: 5 step: 805, loss is 0.0102279307320714\n",
            "epoch: 5 step: 806, loss is 0.012976055964827538\n",
            "epoch: 5 step: 807, loss is 0.1261928826570511\n",
            "epoch: 5 step: 808, loss is 0.03976717218756676\n",
            "epoch: 5 step: 809, loss is 0.25540822744369507\n",
            "epoch: 5 step: 810, loss is 0.04308690130710602\n",
            "epoch: 5 step: 811, loss is 0.044179514050483704\n",
            "epoch: 5 step: 812, loss is 0.013166646473109722\n",
            "epoch: 5 step: 813, loss is 0.01968712918460369\n",
            "epoch: 5 step: 814, loss is 0.01098907832056284\n",
            "epoch: 5 step: 815, loss is 0.005730075296014547\n",
            "epoch: 5 step: 816, loss is 0.004488545004278421\n",
            "epoch: 5 step: 817, loss is 0.05025994032621384\n",
            "epoch: 5 step: 818, loss is 0.03635465353727341\n",
            "epoch: 5 step: 819, loss is 0.13923941552639008\n",
            "epoch: 5 step: 820, loss is 0.025013446807861328\n",
            "epoch: 5 step: 821, loss is 0.005478770472109318\n",
            "epoch: 5 step: 822, loss is 0.02576945722103119\n",
            "epoch: 5 step: 823, loss is 0.01363501138985157\n",
            "epoch: 5 step: 824, loss is 0.0035115214996039867\n",
            "epoch: 5 step: 825, loss is 0.11358974874019623\n",
            "epoch: 5 step: 826, loss is 0.017231594771146774\n",
            "epoch: 5 step: 827, loss is 0.005794272758066654\n",
            "epoch: 5 step: 828, loss is 0.05240564048290253\n",
            "epoch: 5 step: 829, loss is 0.02906123735010624\n",
            "epoch: 5 step: 830, loss is 0.00625318568199873\n",
            "epoch: 5 step: 831, loss is 0.030114199966192245\n",
            "epoch: 5 step: 832, loss is 0.005727849435061216\n",
            "epoch: 5 step: 833, loss is 0.0027466972824186087\n",
            "epoch: 5 step: 834, loss is 0.011937730945646763\n",
            "epoch: 5 step: 835, loss is 0.023283150047063828\n",
            "epoch: 5 step: 836, loss is 0.02529819682240486\n",
            "epoch: 5 step: 837, loss is 0.039741598069667816\n",
            "epoch: 5 step: 838, loss is 0.0118867177516222\n",
            "epoch: 5 step: 839, loss is 0.006528965663164854\n",
            "epoch: 5 step: 840, loss is 0.03523698449134827\n",
            "epoch: 5 step: 841, loss is 0.07330910861492157\n",
            "epoch: 5 step: 842, loss is 0.011926176957786083\n",
            "epoch: 5 step: 843, loss is 0.010867659002542496\n",
            "epoch: 5 step: 844, loss is 0.0033265110105276108\n",
            "epoch: 5 step: 845, loss is 0.005099447909742594\n",
            "epoch: 5 step: 846, loss is 0.018877342343330383\n",
            "epoch: 5 step: 847, loss is 0.023585664108395576\n",
            "epoch: 5 step: 848, loss is 0.0037463828921318054\n",
            "epoch: 5 step: 849, loss is 0.007572262082248926\n",
            "epoch: 5 step: 850, loss is 0.01172342337667942\n",
            "epoch: 5 step: 851, loss is 0.005204346030950546\n",
            "epoch: 5 step: 852, loss is 0.09916829317808151\n",
            "epoch: 5 step: 853, loss is 0.011468103155493736\n",
            "epoch: 5 step: 854, loss is 0.010799144394695759\n",
            "epoch: 5 step: 855, loss is 0.004994083195924759\n",
            "epoch: 5 step: 856, loss is 0.06698530912399292\n",
            "epoch: 5 step: 857, loss is 0.017694272100925446\n",
            "epoch: 5 step: 858, loss is 0.01757611520588398\n",
            "epoch: 5 step: 859, loss is 0.006686648353934288\n",
            "epoch: 5 step: 860, loss is 0.010351093485951424\n",
            "epoch: 5 step: 861, loss is 0.00559571199119091\n",
            "epoch: 5 step: 862, loss is 0.009227043017745018\n",
            "epoch: 5 step: 863, loss is 0.006373799871653318\n",
            "epoch: 5 step: 864, loss is 0.004407679196447134\n",
            "epoch: 5 step: 865, loss is 0.014030911028385162\n",
            "epoch: 5 step: 866, loss is 0.01385880634188652\n",
            "epoch: 5 step: 867, loss is 0.11732903867959976\n",
            "epoch: 5 step: 868, loss is 0.007671725004911423\n",
            "epoch: 5 step: 869, loss is 0.006089396309107542\n",
            "epoch: 5 step: 870, loss is 0.006948565598577261\n",
            "epoch: 5 step: 871, loss is 0.11677759140729904\n",
            "epoch: 5 step: 872, loss is 0.007282791193574667\n",
            "epoch: 5 step: 873, loss is 0.00618657236918807\n",
            "epoch: 5 step: 874, loss is 0.018603194504976273\n",
            "epoch: 5 step: 875, loss is 0.021931791678071022\n",
            "epoch: 5 step: 876, loss is 0.02763843536376953\n",
            "epoch: 5 step: 877, loss is 0.014277186244726181\n",
            "epoch: 5 step: 878, loss is 0.010058591142296791\n",
            "epoch: 5 step: 879, loss is 0.04810338839888573\n",
            "epoch: 5 step: 880, loss is 0.017587658017873764\n",
            "epoch: 5 step: 881, loss is 0.01987089216709137\n",
            "epoch: 5 step: 882, loss is 0.0025357797276228666\n",
            "epoch: 5 step: 883, loss is 0.02580111101269722\n",
            "epoch: 5 step: 884, loss is 0.0027392995543777943\n",
            "epoch: 5 step: 885, loss is 0.008582363836467266\n",
            "epoch: 5 step: 886, loss is 0.018552694469690323\n",
            "epoch: 5 step: 887, loss is 0.044977400451898575\n",
            "epoch: 5 step: 888, loss is 0.0034132669679820538\n",
            "epoch: 5 step: 889, loss is 0.0900704562664032\n",
            "epoch: 5 step: 890, loss is 0.003942127339541912\n",
            "epoch: 5 step: 891, loss is 0.006264321506023407\n",
            "epoch: 5 step: 892, loss is 0.17024782299995422\n",
            "epoch: 5 step: 893, loss is 0.10711022466421127\n",
            "epoch: 5 step: 894, loss is 0.013671965338289738\n",
            "epoch: 5 step: 895, loss is 0.00752623938024044\n",
            "epoch: 5 step: 896, loss is 0.0043039084412157536\n",
            "epoch: 5 step: 897, loss is 0.025917910039424896\n",
            "epoch: 5 step: 898, loss is 0.006674138829112053\n",
            "epoch: 5 step: 899, loss is 0.006106219720095396\n",
            "epoch: 5 step: 900, loss is 0.014590084552764893\n",
            "epoch: 5 step: 901, loss is 0.006845386698842049\n",
            "epoch: 5 step: 902, loss is 0.019580552354454994\n",
            "epoch: 5 step: 903, loss is 0.011368014849722385\n",
            "epoch: 5 step: 904, loss is 0.06949424743652344\n",
            "epoch: 5 step: 905, loss is 0.0045320442877709866\n",
            "epoch: 5 step: 906, loss is 0.004814381245523691\n",
            "epoch: 5 step: 907, loss is 0.00740361912176013\n",
            "epoch: 5 step: 908, loss is 0.0673074871301651\n",
            "epoch: 5 step: 909, loss is 0.009285232983529568\n",
            "epoch: 5 step: 910, loss is 0.010134885087609291\n",
            "epoch: 5 step: 911, loss is 0.17160813510417938\n",
            "epoch: 5 step: 912, loss is 0.013129767961800098\n",
            "epoch: 5 step: 913, loss is 0.007645059376955032\n",
            "epoch: 5 step: 914, loss is 0.0150133753195405\n",
            "epoch: 5 step: 915, loss is 0.0018723219400271773\n",
            "epoch: 5 step: 916, loss is 0.005754295736551285\n",
            "epoch: 5 step: 917, loss is 0.008235939778387547\n",
            "epoch: 5 step: 918, loss is 0.004831260535866022\n",
            "epoch: 5 step: 919, loss is 0.0072059109807014465\n",
            "epoch: 5 step: 920, loss is 0.006361548323184252\n",
            "epoch: 5 step: 921, loss is 0.12173844128847122\n",
            "epoch: 5 step: 922, loss is 0.005971293896436691\n",
            "epoch: 5 step: 923, loss is 0.013852094300091267\n",
            "epoch: 5 step: 924, loss is 0.022495295852422714\n",
            "epoch: 5 step: 925, loss is 0.052553970366716385\n",
            "epoch: 5 step: 926, loss is 0.0071696019731462\n",
            "epoch: 5 step: 927, loss is 0.004747902974486351\n",
            "epoch: 5 step: 928, loss is 0.0016821566969156265\n",
            "epoch: 5 step: 929, loss is 0.19995366036891937\n",
            "epoch: 5 step: 930, loss is 0.007869107648730278\n",
            "epoch: 5 step: 931, loss is 0.1692551225423813\n",
            "epoch: 5 step: 932, loss is 0.020580172538757324\n",
            "epoch: 5 step: 933, loss is 0.002915521152317524\n",
            "epoch: 5 step: 934, loss is 0.013911501504480839\n",
            "epoch: 5 step: 935, loss is 0.008460016921162605\n",
            "epoch: 5 step: 936, loss is 0.05984284728765488\n",
            "epoch: 5 step: 937, loss is 0.0023104753345251083\n",
            "epoch: 5 step: 938, loss is 0.02899082377552986\n",
            "epoch: 5 step: 939, loss is 0.0563933290541172\n",
            "epoch: 5 step: 940, loss is 0.009736108593642712\n",
            "epoch: 5 step: 941, loss is 0.008720188401639462\n",
            "epoch: 5 step: 942, loss is 0.030860118567943573\n",
            "epoch: 5 step: 943, loss is 0.004719455260783434\n",
            "epoch: 5 step: 944, loss is 0.009507508017122746\n",
            "epoch: 5 step: 945, loss is 0.003239512676373124\n",
            "epoch: 5 step: 946, loss is 0.011039936915040016\n",
            "epoch: 5 step: 947, loss is 0.008890504948794842\n",
            "epoch: 5 step: 948, loss is 0.008667453192174435\n",
            "epoch: 5 step: 949, loss is 0.18140389025211334\n",
            "epoch: 5 step: 950, loss is 0.010897628031671047\n",
            "epoch: 5 step: 951, loss is 0.0051027038134634495\n",
            "epoch: 5 step: 952, loss is 0.1525239199399948\n",
            "epoch: 5 step: 953, loss is 0.05935010313987732\n",
            "epoch: 5 step: 954, loss is 0.01989034004509449\n",
            "epoch: 5 step: 955, loss is 0.08746420592069626\n",
            "epoch: 5 step: 956, loss is 0.016677843406796455\n",
            "epoch: 5 step: 957, loss is 0.0071037872694432735\n",
            "epoch: 5 step: 958, loss is 0.008360747247934341\n",
            "epoch: 5 step: 959, loss is 0.006766430102288723\n",
            "epoch: 5 step: 960, loss is 0.0025849342346191406\n",
            "epoch: 5 step: 961, loss is 0.015009501948952675\n",
            "epoch: 5 step: 962, loss is 0.05311177670955658\n",
            "epoch: 5 step: 963, loss is 0.021476617082953453\n",
            "epoch: 5 step: 964, loss is 0.014465097337961197\n",
            "epoch: 5 step: 965, loss is 0.00942574068903923\n",
            "epoch: 5 step: 966, loss is 0.0035348774399608374\n",
            "epoch: 5 step: 967, loss is 0.005767809227108955\n",
            "epoch: 5 step: 968, loss is 0.009752574376761913\n",
            "epoch: 5 step: 969, loss is 0.010782221332192421\n",
            "epoch: 5 step: 970, loss is 0.006231057457625866\n",
            "epoch: 5 step: 971, loss is 0.02853989414870739\n",
            "epoch: 5 step: 972, loss is 0.00520004378631711\n",
            "epoch: 5 step: 973, loss is 0.006624487228691578\n",
            "epoch: 5 step: 974, loss is 0.014501607045531273\n",
            "epoch: 5 step: 975, loss is 0.005625354126095772\n",
            "epoch: 5 step: 976, loss is 0.006213684566318989\n",
            "epoch: 5 step: 977, loss is 0.005280823912471533\n",
            "epoch: 5 step: 978, loss is 0.005079947877675295\n",
            "epoch: 5 step: 979, loss is 0.0177641361951828\n",
            "epoch: 5 step: 980, loss is 0.005037747789174318\n",
            "epoch: 5 step: 981, loss is 0.014699654653668404\n",
            "epoch: 5 step: 982, loss is 0.03559493646025658\n",
            "epoch: 5 step: 983, loss is 0.021532900631427765\n",
            "epoch: 5 step: 984, loss is 0.007813462056219578\n",
            "epoch: 5 step: 985, loss is 0.008112648501992226\n",
            "epoch: 5 step: 986, loss is 0.0038236514665186405\n",
            "epoch: 5 step: 987, loss is 0.01595848612487316\n",
            "epoch: 5 step: 988, loss is 0.05517025291919708\n",
            "epoch: 5 step: 989, loss is 0.00908814836293459\n",
            "epoch: 5 step: 990, loss is 0.003135810839012265\n",
            "epoch: 5 step: 991, loss is 0.00340017001144588\n",
            "epoch: 5 step: 992, loss is 0.008771183900535107\n",
            "epoch: 5 step: 993, loss is 0.010170222260057926\n",
            "epoch: 5 step: 994, loss is 0.022997695952653885\n",
            "epoch: 5 step: 995, loss is 0.0053741540759801865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING] ME(1671:136248148721664,MainProcess):2025-04-19-17:02:32.305.000 [mindspore/train/callback/_early_stop.py:221] Early stopping is conditioned on accuracy, which is not available. Available choices are: {'loss'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 6 step: 1, loss is 0.00695651164278388\n",
            "epoch: 6 step: 2, loss is 0.024641219526529312\n",
            "epoch: 6 step: 3, loss is 0.003481963649392128\n",
            "epoch: 6 step: 4, loss is 0.07406826317310333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mindspore as ms\n",
        "from mindspore import Tensor, load_checkpoint, load_param_into_net, export\n",
        "import pickle\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embed_dim = 50\n",
        "hidden1 = 100\n",
        "hidden2 = 50\n",
        "num_classes = 2\n",
        "\n",
        "def load_model(ckpt_path):\n",
        "    model_net = LSTMClassifier(vocab_size, embed_dim, hidden1, hidden2, num_classes)\n",
        "    params = load_checkpoint(ckpt_path)\n",
        "    load_param_into_net(model_net, params)\n",
        "    model\n",
        "    return model_net\n",
        "\n",
        "ckpt_path = \"/content/best_model-5_920.ckpt\"\n",
        "model_net = load_model(ckpt_path)\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = X.shape[1]\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = X.shape[1]\n",
        "dummy_input = Tensor(np.zeros((batch_size, sequence_length), dtype=np.int32))\n",
        "\n",
        "try:\n",
        "  model_net.set_train(False)\n",
        "\n",
        "  dummy_input = Tensor(ms.ops.zeros((batch_size, sequence_length), ms.int32))\n",
        "  export(\n",
        "    model_net,\n",
        "    dummy_input,\n",
        "    file_name=\"lstm_haram_dynamic_batch\",\n",
        "    file_format=\"ONNX\",\n",
        "    input_names=[\"input_ids\"],\n",
        "    output_names=[\"logits\"],\n",
        "    opset_version=12,\n",
        "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
        "    enable_onnx_checker=True\n",
        ")\n",
        "  print(\"ONNX model saved as lstm_classifier.onnx with dynamic axes\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Error saving ONNX model: {e}\")\n",
        "\n",
        "params = load_checkpoint(ckpt_path)\n",
        "\n",
        "try:\n",
        "  with open('lstm_classifier_params.pkl', 'wb') as f:\n",
        "      pickle.dump(params, f)\n",
        "  print(\"Model parameters pickled to lstm_classifier_params.pkl\")\n",
        "except Exception as e:\n",
        "  print(f\"Error pickling model parameters: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVVX7GbNAONd",
        "outputId": "9eb689a7-ba5e-45f6-8e87-d0f7cdaf5d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:11:12.228.000 [mindspore/nn/layer/basic.py:176] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:11:12.265.000 [mindspore/nn/layer/basic.py:176] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:11:12.480.000 [mindspore/nn/layer/basic.py:202] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:11:12.488.000 [mindspore/nn/layer/basic.py:202] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:11:12.514.000 [mindspore/nn/layer/basic.py:202] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
            "[WARNING] ME(2540:140657341231104,MainProcess):2025-04-24-12:11:12.721.000 [mindspore/nn/layer/basic.py:202] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model saved as lstm_classifier.onnx with dynamic axes\n",
            "Model parameters pickled to lstm_classifier_params.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mindspore.train import Model\n",
        "from mindspore.nn import SoftmaxCrossEntropyWithLogits, Adam\n",
        "\n",
        "# Recreate loss and optimizer (optimizer parameters won’t matter for eval)\n",
        "loss_fn = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "opt     = Adam(model_net.trainable_params(), learning_rate=1e-4)\n",
        "\n",
        "model = Model(model_net, loss_fn, opt, metrics={\"accuracy\"})\n",
        "\n",
        "# Evaluate on your test_dataset\n",
        "res = model.eval(test_dataset, dataset_sink_mode=False)\n",
        "print(f\"Test set accuracy: {res['accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnra5iHpA7yD",
        "outputId": "dbb033f2-62fa-4e7a-e283-ce556944461c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.9894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_l = X.shape[1]"
      ],
      "metadata": {
        "id": "GM_8CvzhHyv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R69FbQTKvUR",
        "outputId": "14fe98b3-ff45-4ccb-da53-fe1a8fbff41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10636"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kT4J_JFI44_",
        "outputId": "46e2b05f-8006-46fc-a2a8-e9fecc389dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.21.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the ONNX model\n",
        "sess = ort.InferenceSession(\"/content/lstm_haram_dynamic_batch.onnx\")\n",
        "\n",
        "# 2. Get input and output names\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "\n",
        "# 3. Prepare data\n",
        "X_input = X_test.astype(np.int32)\n",
        "y_true = y_test\n",
        "\n",
        "# 4. Process data one sample at a time\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(X_input)):\n",
        "    # Get single sample and add batch dimension\n",
        "    single_input = X_input[i:i+1]  # Creates a batch of size 1\n",
        "\n",
        "    # Run inference\n",
        "    output = sess.run([output_name], {input_name: single_input})\n",
        "    pred = np.argmax(output[0], axis=1)[0]\n",
        "    predictions.append(pred)\n",
        "\n",
        "# 5. Convert predictions to numpy array\n",
        "y_pred = np.array(predictions)\n",
        "\n",
        "# 6. Calculate accuracy\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"ONNX model accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y0cz5QVIhuu",
        "outputId": "da8e8cfb-1750-4025-ef53-0e99daec3ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model accuracy: 0.9894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST**"
      ],
      "metadata": {
        "id": "oMM50UmH1IyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_ingredients(json_data):\n",
        "    try:\n",
        "        # 1) Extract the ingredients string manually\n",
        "        m = re.search(\n",
        "            r'\"ingredients\"\\s*:\\s*\"(?P<body>(?:\\\\.|[^\"\\\\])*)\"',\n",
        "            json_data,\n",
        "            flags=re.DOTALL\n",
        "        )\n",
        "        if not m:\n",
        "            return \"Error: No ingredients field found in input\"\n",
        "\n",
        "        # this is the raw contents, still with literal \\n and escapes\n",
        "        raw = m.group('body')\n",
        "\n",
        "        # 2) Turn literal \\n sequences into real newlines\n",
        "        body = raw.replace(r'\\n', '\\n')\n",
        "\n",
        "        # 3) Now run your cleanup on the real multiline string\n",
        "        #    — drop the \"INGREDIENTS:\" header, split off allergens, etc.\n",
        "        #    — then remove punctuation, lowercase, collapse spaces.\n",
        "        #    — exactly as you already have it:\n",
        "        body = re.sub(r'^INGREDIENTS:\\s*', '', body, flags=re.IGNORECASE)\n",
        "        body = body.split(\"ALLERGEN INFORMATION:\")[0]\n",
        "        body = body.replace('\\n', ' ').lower()\n",
        "        body = re.sub(r'[()]', '', body)\n",
        "        body = re.sub(r'[^a-z\\s]', '', body)\n",
        "        body = re.sub(r'\\s+', ' ', body).strip()\n",
        "\n",
        "        return body\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "yVpY94uZ6Zfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = \"\"\"\n",
        "{\n",
        "  \"status\": \"success\",\n",
        "  \"ingredients\": \"INGREDIENTS:SUGAR / GLUCOSE-FRUCTOSE, ENRICHED WHEAT FLOUR, HYDROGENATED VEGETABLE OIL (HYDROGENATED PALM KERNEL AND HYDROGENATED COCONUT OILS), VEGETABLE OIL (MODIFIED PALM AND MODIFIED PALM KERNEL OILS), BROWN SUGAR, LIQUID WHOLE EGGS, WATER, LIQUID EGG WHITES, GLYCERIN, COCOA, MODIFIED MILK INGREDIENTS, SALT, BAKING POWDER, PROPYLENE GLYCOL, CORN STARCH, SORBITAN MONOSTEARATE, MONO AND DIGLYCERIDES, WHEAT STARCH, ICING SUGAR, POTASSIUM SORBATE, POLYSORBATE 60, SODIUM AND/OR CALCIUM AND/OR AMMONIUM ALGINATE, SOY LECITHIN, XANTHAN GUM, SORBIC ACID, SODIUM BISULPHATE, CARRAGEENAN, CELLULOSE GUM, SOY PROTEIN, DEXTROSE, AMYLASE, LIPASE, ARTIFICIAL FLAVOUR, COLOUR. MAY CONTAIN PEANUTS AND/OR NUTS\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "result = preprocess_ingredients(json_data)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "7r-kS4Ke7adm",
        "outputId": "efea7314-9c08-4103-c807-df77f96fb394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sugar glucosefructose enriched wheat flour hydrogenated vegetable oil hydrogenated palm kernel and hydrogenated coconut oils vegetable oil modified palm and modified palm kernel oils brown sugar liquid whole eggs water liquid egg whites glycerin cocoa modified milk ingredients salt baking powder propylene glycol corn starch sorbitan monostearate mono and diglycerides wheat starch icing sugar potassium sorbate polysorbate sodium andor calcium andor ammonium alginate soy lecithin xanthan gum sorbic acid sodium bisulphate carrageenan cellulose gum soy protein dextrose amylase lipase artificial flavour colour may contain peanuts andor nuts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Load the tokenizer\n",
        "with open('tokenizer.pickle', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# 2. Load the ONNX model\n",
        "model_path = \"/content/lstm_haram_dynamic_batch.onnx\"  # Adjust path as needed\n",
        "sess = ort.InferenceSession(model_path)\n",
        "\n",
        "# Get input and output names\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "\n",
        "# 3. Define prediction function\n",
        "def predict_text(texts, max_length=None):\n",
        "\n",
        "    # Handle single text input\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    # Determine sequence length if not provided\n",
        "    if max_length is None:\n",
        "        # Infer from model's input shape or use a default\n",
        "        max_length = sess.get_inputs()[0].shape[1] if len(sess.get_inputs()[0].shape) > 1 else 100\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Convert to the expected data type\n",
        "    X_input = padded_sequences.astype(np.int32)\n",
        "\n",
        "    # Process in batches of 1 (assuming model was trained with batch size 1)\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_input)):\n",
        "        # Get single sample and add batch dimension if needed\n",
        "        single_input = X_input[i:i+1]\n",
        "\n",
        "        # Run inference\n",
        "        output = sess.run([output_name], {input_name: single_input})\n",
        "\n",
        "        # Get predicted class\n",
        "        pred_class = np.argmax(output[0], axis=1)[0]\n",
        "        predictions.append(pred_class)\n",
        "\n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "utc8AE8k7c39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_text(result)\n",
        "print(\"Result:\", result)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "class_labels = {0: \"Halal\", 1: \"Haram\"}\n",
        "for text, pred in zip(result, predictions):\n",
        "    print(f\"Text: '{text}' → Prediction: {class_labels[pred]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MevoRhXx88hx",
        "outputId": "c8e81da5-61ee-4101-d0e2-400dbcf24d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: potatoes bacon pork salt tbhq bht\n",
            "Predictions: [1]\n",
            "Text: 'p' → Prediction: Haram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import onnxruntime as ort\n",
        "\n",
        "# 1) Load your saved tokenizer\n",
        "with open('tokenizer.pickle', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# 2) Initialize ONNX Runtime session\n",
        "sess = ort.InferenceSession(\"/content/lstm_haram_dynamic_batch.onnx\")\n",
        "input_meta  = sess.get_inputs()[0]\n",
        "input_name  = input_meta.name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "\n",
        "# Map indices to labels\n",
        "class_labels = {0: \"Halal\", 1: \"Haram\"}\n",
        "\n",
        "def predict_texts_single(texts,\n",
        "                         tokenizer,\n",
        "                         session,\n",
        "                         input_name,\n",
        "                         output_name,\n",
        "                         maxlen=None):\n",
        "    \"\"\"\n",
        "    Runs ONNX inference one input at a time (batch size = 1) to match a fixed-model batch.\n",
        "    \"\"\"\n",
        "    # Determine padding length\n",
        "    if maxlen is None:\n",
        "        # input_meta.shape is like [1, maxlen]\n",
        "        maxlen = input_meta.shape[1]\n",
        "\n",
        "    preds = []\n",
        "    for txt in texts:\n",
        "        # 1) Tokenize & pad single text\n",
        "        seq = tokenizer.texts_to_sequences([txt])\n",
        "        x = pad_sequences(seq, maxlen=maxlen).astype(np.int32)  # shape (1, maxlen)\n",
        "\n",
        "        # 2) Run inference\n",
        "        out = session.run([output_name], {input_name: x})[0]     # (1, num_classes)\n",
        "        pred = np.argmax(out, axis=1)[0]                        # single int\n",
        "        preds.append(pred)\n",
        "\n",
        "    return np.array(preds)\n",
        "\n",
        "# Example usage\n",
        "texts = [result]\n",
        "y_pred = predict_texts_single(\n",
        "    texts,\n",
        "    tokenizer,\n",
        "    sess,\n",
        "    input_name,\n",
        "    output_name\n",
        ")\n",
        "\n",
        "for text, pred in zip(texts, y_pred):\n",
        "    print(f\"Text: '{text}' → Prediction: {class_labels[pred]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umzWeceq9Lgk",
        "outputId": "d5ffcab7-b48c-4e7b-c1ce-6065c1343b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'sugar glucosefructose enriched wheat flour hydrogenated vegetable oil hydrogenated palm kernel and hydrogenated coconut oils vegetable oil modified palm and modified palm kernel oils brown sugar liquid whole eggs water liquid egg whites glycerin cocoa modified milk ingredients salt baking powder propylene glycol corn starch sorbitan monostearate mono and diglycerides wheat starch icing sugar potassium sorbate polysorbate sodium andor calcium andor ammonium alginate soy lecithin xanthan gum sorbic acid sodium bisulphate carrageenan cellulose gum soy protein dextrose amylase lipase artificial flavour colour may contain peanuts andor nuts' → Prediction: Haram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnJI7fCGAQ8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}